"""MAIN-RAG (Multi-Agent Filtering RAG) Pipeline for AutoRAG-Research.

Implements the MAIN-RAG algorithm from ACL 2025 paper (arxiv:2501.00332).
Uses three LLM agents to collaboratively filter retrieved documents through
adaptive thresholding and probabilistic scoring.
"""

import logging
from dataclasses import dataclass, field
from statistics import mean, stdev
from typing import Any

from langchain_core.language_models import BaseLanguageModel
from sqlalchemy.orm import Session, sessionmaker

from autorag_research.config import BaseGenerationPipelineConfig
from autorag_research.orm.service.generation_pipeline import GenerationResult
from autorag_research.pipelines.generation.base import BaseGenerationPipeline
from autorag_research.pipelines.retrieval.base import BaseRetrievalPipeline
from autorag_research.util import aggregate_token_usage

logger = logging.getLogger("AutoRAG-Research")

# Agent-1 (Predictor) prompts
DEFAULT_PREDICTOR_SYSTEM_PROMPT = """You are an accurate and reliable AI assistant that can answer questions with the help of external documents. You should only provide the correct answer without repeating the question and instruction."""

DEFAULT_PREDICTOR_USER_PROMPT = """Document:
{document}

Question: {query}"""

# Agent-2 (Judge) prompts
DEFAULT_JUDGE_SYSTEM_PROMPT = """You are a noisy document evaluator that can judge if the external document is noisy for the query with unrelated or misleading information. Given a retrieved Document, a Question, and an Answer generated by an LLM (LLM Answer), you should judge whether both the following two conditions are reached: (1) the Document provides specific information for answering the Question; (2) the LLM Answer directly answers the question based on the retrieved Document. Please note that external documents may contain noisy or factually incorrect information. If the information in the document does not contain the answer, you should point it out with evidence. You should answer with "Yes" or "No" with evidence of your judgment, where "No" means one of the conditions (1) and (2) are unreached and indicates it is a noisy document."""

DEFAULT_JUDGE_USER_PROMPT = """Document:
{document}

Question: {query}

Answer: {answer}

Does this document provide relevant information to answer the question correctly? Respond with only "Yes" or "No"."""

# Agent-3 (Final Predictor) prompts
DEFAULT_FINAL_PREDICTOR_SYSTEM_PROMPT = """You are an accurate and reliable AI assistant that can answer questions with the help of external documents. You should only provide the correct answer without repeating the question and instruction."""

DEFAULT_FINAL_PREDICTOR_USER_PROMPT = """Documents:
{documents}

Question: {query}"""


@dataclass(kw_only=True)
class MAINRAGPipelineConfig(BaseGenerationPipelineConfig):
    """Configuration for MAIN-RAG pipeline.

    MAIN-RAG (Multi-Agent Filtering RAG) uses three LLM agents to collaboratively
    filter retrieved documents through adaptive thresholding and probabilistic scoring.

    Attributes:
        name: Unique name for this pipeline instance.
        retrieval_pipeline_name: Name of the retrieval pipeline to use.
        llm: LangChain BaseLanguageModel instance for text generation.
            For optimal results, enable logprobs: llm.bind(logprobs=True, top_logprobs=5)
        std_multiplier: Multiplier for standard deviation in threshold calculation.
            threshold = mean - n * std. Default 0.0 means threshold = mean.
        predictor_system_prompt: System prompt for Agent-1 (Predictor).
        predictor_user_prompt: User prompt template for Agent-1 (must contain {document}, {query}).
        judge_system_prompt: System prompt for Agent-2 (Judge).
        judge_user_prompt: User prompt template for Agent-2 (must contain {document}, {query}, {answer}).
        final_predictor_system_prompt: System prompt for Agent-3 (Final Predictor).
        final_predictor_user_prompt: User prompt template for Agent-3 (must contain {documents}, {query}).
        top_k: Number of chunks to retrieve per query.
        batch_size: Number of queries to process in each batch.

    Example:
        ```python
        from langchain_openai import ChatOpenAI

        config = MAINRAGPipelineConfig(
            name="main_rag_v1",
            retrieval_pipeline_name="bm25_baseline",
            llm=ChatOpenAI(model="gpt-4o-mini").bind(logprobs=True, top_logprobs=5),
            std_multiplier=0.0,
            top_k=20,
        )
        ```
    """

    std_multiplier: float = 0.0
    predictor_system_prompt: str = field(default=DEFAULT_PREDICTOR_SYSTEM_PROMPT)
    predictor_user_prompt: str = field(default=DEFAULT_PREDICTOR_USER_PROMPT)
    judge_system_prompt: str = field(default=DEFAULT_JUDGE_SYSTEM_PROMPT)
    judge_user_prompt: str = field(default=DEFAULT_JUDGE_USER_PROMPT)
    final_predictor_system_prompt: str = field(default=DEFAULT_PREDICTOR_SYSTEM_PROMPT)
    final_predictor_user_prompt: str = field(default=DEFAULT_PREDICTOR_USER_PROMPT)

    def get_pipeline_class(self) -> type["MAINRAGPipeline"]:
        """Return the MAINRAGPipeline class."""
        return MAINRAGPipeline

    def get_pipeline_kwargs(self) -> dict[str, Any]:
        """Return kwargs for MAINRAGPipeline constructor."""
        if self._retrieval_pipeline is None:
            msg = f"Retrieval pipeline '{self.retrieval_pipeline_name}' not injected"
            raise ValueError(msg)

        return {
            "llm": self.llm,
            "retrieval_pipeline": self._retrieval_pipeline,
            "std_multiplier": self.std_multiplier,
            "predictor_system_prompt": self.predictor_system_prompt,
            "predictor_user_prompt": self.predictor_user_prompt,
            "judge_system_prompt": self.judge_system_prompt,
            "judge_user_prompt": self.judge_user_prompt,
            "final_predictor_system_prompt": self.final_predictor_system_prompt,
            "final_predictor_user_prompt": self.final_predictor_user_prompt,
        }


def calculate_binary_logprob_score(
    response: Any,
    positive_token: str = "Yes",  # noqa: S107
    negative_token: str = "No",  # noqa: S107
) -> tuple[float, bool]:
    """Calculate relevance score from Yes/No log probabilities.

    Implements the MAIN-RAG scoring formula: score = log P(Yes) - log P(No)

    Args:
        response: LangChain LLM response object.
        positive_token: Token representing positive judgment (default: "Yes").
        negative_token: Token representing negative judgment (default: "No").

    Returns:
        Tuple of (score, used_logprobs):
        - score: The calculated relevance score
        - used_logprobs: True if real logprobs were used, False if fallback

    Fallback behavior when logprobs unavailable:
        - Response starts with positive_token: score = 1.0
        - Response starts with negative_token: score = -1.0
        - Ambiguous: score = 0.0
    """
    from autorag_research.util import extract_token_logprobs

    # Try to extract logprobs
    logprobs = extract_token_logprobs(response, target_tokens=[positive_token, negative_token])

    if logprobs is not None:
        # Find the tokens (case-insensitive)
        pos_logprob = None
        neg_logprob = None

        for token, logprob in logprobs.items():
            if token.lower() == positive_token.lower():
                pos_logprob = logprob
            elif token.lower() == negative_token.lower():
                neg_logprob = logprob

        if pos_logprob is not None and neg_logprob is not None:
            # MAIN-RAG formula: r_i = log P(Yes) - log P(No)
            score = pos_logprob - neg_logprob
            return score, True

    # Fallback: binary scoring from response text
    response_text = ""
    if hasattr(response, "content"):
        response_text = response.content.strip().lower()
    elif isinstance(response, str):
        response_text = response.strip().lower()

    if response_text.startswith(positive_token.lower()):
        return 1.0, False
    elif response_text.startswith(negative_token.lower()):
        return -1.0, False
    else:
        logger.warning(f"Ambiguous judge response (no logprobs): {response_text[:50]}")
        return 0.0, False


class MAINRAGPipeline(BaseGenerationPipeline):
    """Multi-Agent Filtering Retrieval-Augmented Generation Pipeline.

    MAIN-RAG uses three LLM agents to collaboratively filter retrieved documents:
    - Agent-1 (Predictor): Generates candidate answers per document
    - Agent-2 (Judge): Scores document relevance via log probabilities
    - Agent-3 (Final Predictor): Generates final answer from filtered docs

    Algorithm Flow:
    1. RETRIEVAL: retrieval_pipeline.retrieve(query, top_k) -> Documents
    2. AGENT-1: For each document, generate candidate answer
    3. AGENT-2: For each (doc, query, answer) triplet, compute relevance score
    4. ADAPTIVE FILTERING: threshold = mean - n * std; filter docs by score
    5. RANKING: Sort filtered documents by score (descending)
    6. AGENT-3: Generate final answer using filtered documents

    Example:
        ```python
        from langchain_openai import ChatOpenAI

        from autorag_research.orm.connection import DBConnection
        from autorag_research.pipelines.generation.main_rag import MAINRAGPipeline
        from autorag_research.pipelines.retrieval.bm25 import BM25RetrievalPipeline

        db = DBConnection.from_config()
        session_factory = db.get_session_factory()

        # Create retrieval pipeline
        retrieval_pipeline = BM25RetrievalPipeline(
            session_factory=session_factory,
            name="bm25_baseline",
            index_path="/path/to/index",
        )

        # Create MAIN-RAG pipeline with logprobs enabled
        llm = ChatOpenAI(model="gpt-4o-mini").bind(logprobs=True, top_logprobs=5)
        pipeline = MAINRAGPipeline(
            session_factory=session_factory,
            name="main_rag_v1",
            llm=llm,
            retrieval_pipeline=retrieval_pipeline,
            std_multiplier=0.0,  # Paper's default
        )

        # Run pipeline
        results = pipeline.run(top_k=20)
        ```
    """

    def __init__(
        self,
        session_factory: sessionmaker[Session],
        name: str,
        llm: BaseLanguageModel,
        retrieval_pipeline: BaseRetrievalPipeline,
        std_multiplier: float = 0.0,
        predictor_system_prompt: str = DEFAULT_PREDICTOR_SYSTEM_PROMPT,
        predictor_user_prompt: str = DEFAULT_PREDICTOR_USER_PROMPT,
        judge_system_prompt: str = DEFAULT_JUDGE_SYSTEM_PROMPT,
        judge_user_prompt: str = DEFAULT_JUDGE_USER_PROMPT,
        final_predictor_system_prompt: str = DEFAULT_FINAL_PREDICTOR_SYSTEM_PROMPT,
        final_predictor_user_prompt: str = DEFAULT_FINAL_PREDICTOR_USER_PROMPT,
        schema: Any | None = None,
    ) -> None:
        """Initialize MAIN-RAG pipeline.

        Args:
            session_factory: SQLAlchemy sessionmaker for database connections.
            name: Name for this pipeline.
            llm: LangChain BaseLanguageModel instance for text generation.
                For optimal results, enable logprobs: llm.bind(logprobs=True, top_logprobs=5)
            retrieval_pipeline: Retrieval pipeline for fetching relevant context.
            std_multiplier: Multiplier for standard deviation in threshold calculation.
                threshold = mean - n * std. Default 0.0 means threshold = mean.
                Higher values are more permissive (lower threshold).
                Negative values are more aggressive (higher threshold).
            predictor_system_prompt: System prompt for Agent-1 (Predictor).
            predictor_user_prompt: User prompt template for Agent-1.
                Must contain {document} and {query} placeholders.
            judge_system_prompt: System prompt for Agent-2 (Judge).
            judge_user_prompt: User prompt template for Agent-2.
                Must contain {document}, {query}, and {answer} placeholders.
            final_predictor_system_prompt: System prompt for Agent-3 (Final Predictor).
            final_predictor_user_prompt: User prompt template for Agent-3.
                Must contain {documents} and {query} placeholders.
            schema: Schema namespace from create_schema(). If None, uses default schema.
        """
        # Store custom attributes BEFORE super().__init__()
        # because _get_pipeline_config() is called in parent init
        self._std_multiplier = std_multiplier
        self._predictor_system_prompt = predictor_system_prompt
        self._predictor_user_prompt = predictor_user_prompt
        self._judge_system_prompt = judge_system_prompt
        self._judge_user_prompt = judge_user_prompt
        self._final_predictor_system_prompt = final_predictor_system_prompt
        self._final_predictor_user_prompt = final_predictor_user_prompt

        super().__init__(session_factory, name, llm, retrieval_pipeline, schema)

    def _get_pipeline_config(self) -> dict[str, Any]:
        """Return MAIN-RAG pipeline configuration."""
        return {
            "type": "main_rag",
            "std_multiplier": self._std_multiplier,
            "predictor_system_prompt": self._predictor_system_prompt,
            "predictor_user_prompt": self._predictor_user_prompt,
            "judge_system_prompt": self._judge_system_prompt,
            "judge_user_prompt": self._judge_user_prompt,
            "final_predictor_system_prompt": self._final_predictor_system_prompt,
            "final_predictor_user_prompt": self._final_predictor_user_prompt,
        }

    def _get_response_text(self, response: Any) -> str:
        """Extract text content from LLM response.

        Args:
            response: LangChain LLM response object.

        Returns:
            Text content of the response.
        """
        return response.content if hasattr(response, "content") else str(response)

    # ==================== Async Methods for Parallel Execution ====================

    async def _ainvoke_llm(self, system_prompt: str, user_prompt: str, **format_kwargs: Any) -> tuple[Any, dict]:
        """Async version of _invoke_llm using LangChain's ainvoke.

        Args:
            system_prompt: System prompt for the LLM.
            user_prompt: User prompt template with format placeholders.
            **format_kwargs: Values to format into the user prompt.

        Returns:
            Tuple of (response, token_usage_dict).
        """
        from langchain_core.messages import HumanMessage, SystemMessage

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt.format(**format_kwargs)),
        ]
        response = await self._llm.ainvoke(messages)
        token_usage = self._extract_token_usage(response)
        return response, token_usage

    async def _aagent_predict(self, query: str, document: str) -> tuple[str, dict]:
        """Async version of Agent-1 (Predictor).

        Args:
            query: The query text.
            document: The document content.

        Returns:
            Tuple of (answer_text, token_usage_dict).
        """
        response, token_usage = await self._ainvoke_llm(
            self._predictor_system_prompt,
            self._predictor_user_prompt,
            query=query,
            document=document,
        )
        return self._get_response_text(response), token_usage

    async def _aagent_judge(self, query: str, document: str, answer: str) -> tuple[float, dict]:
        """Async version of Agent-2 (Judge).

        Args:
            query: The query text.
            document: The document content.
            answer: The candidate answer from Agent-1.

        Returns:
            Tuple of (relevance_score, token_usage_dict).

        Raises:
            LogprobsNotSupportedError: If LLM does not support logprobs.
        """
        from autorag_research.exceptions import LogprobsNotSupportedError

        response, token_usage = await self._ainvoke_llm(
            self._judge_system_prompt,
            self._judge_user_prompt,
            query=query,
            document=document,
            answer=answer,
        )

        score, used_logprobs = calculate_binary_logprob_score(response)

        if not used_logprobs:
            raise LogprobsNotSupportedError(self.name)

        return score, token_usage

    async def _aagent_final_predict(self, query: str, documents: list[str]) -> tuple[str, dict]:
        """Async version of Agent-3 (Final Predictor).

        Args:
            query: The query text.
            documents: List of filtered document contents.

        Returns:
            Tuple of (answer_text, token_usage_dict).
        """
        formatted_docs = "\n\n".join(f"[Document {i + 1}]\n{doc}" for i, doc in enumerate(documents))
        response, token_usage = await self._ainvoke_llm(
            self._final_predictor_system_prompt,
            self._final_predictor_user_prompt,
            query=query,
            documents=formatted_docs,
        )
        return self._get_response_text(response), token_usage

    @staticmethod
    def _calculate_adaptive_threshold(scores: list[float], std_multiplier: float) -> float:
        """Calculate adaptive filtering threshold (static method for testing).

        Formula: threshold = mean - n * std
        Where:
            mean = average of scores
            std = standard deviation of scores
            n = std_multiplier

        Args:
            scores: List of relevance scores.
            std_multiplier: Multiplier for standard deviation.

        Returns:
            Threshold value.

        Raises:
            ValueError: If scores list is empty.
        """
        if not scores:
            msg = "Cannot calculate threshold from empty scores list"
            raise ValueError(msg)

        score_mean = mean(scores)

        # Handle single score or all same scores (zero std)
        if len(scores) == 1:
            return score_mean

        score_std = stdev(scores)

        # threshold = mean - n * std
        return score_mean - std_multiplier * score_std

    async def _generate(self, query_id: int | str, top_k: int) -> GenerationResult:
        """Generate an answer using the MAIN-RAG algorithm with parallel agent execution.

        Implements the 6-phase MAIN-RAG algorithm with parallel execution:
        1. Retrieval: Get documents using retrieval pipeline
        2. Agent-1 (Predictor): Generate candidate answers per document (PARALLEL)
        3. Agent-2 (Judge): Score document relevance (PARALLEL)
        4. Adaptive Filtering: Filter documents by threshold
        5. Ranking: Sort filtered documents by score
        6. Agent-3 (Final Predictor): Generate final answer

        Args:
            query_id: The query ID to answer.
            top_k: Number of chunks to retrieve.

        Returns:
            GenerationResult containing the generated text and metadata.
        """
        # Get query text from database
        query = self._service.get_query_text(query_id)
        all_token_usages: list[dict] = []  # Each: {"token_usage": {...}, "execution_time": 0}

        # ==================== Phase 1: Retrieval ====================
        retrieved = await self._retrieval_pipeline._retrieve_by_id(query_id, top_k)

        # Edge case: Empty retrieval results
        if not retrieved:
            return GenerationResult(
                text="",
                token_usage=None,
                metadata={
                    "pipeline_type": "main_rag",
                    "error": "No documents retrieved",
                    "std_multiplier": self._std_multiplier,
                    "original_doc_count": 0,
                    "filtered_doc_count": 0,
                },
            )

        # Get chunk IDs and contents
        chunk_ids = [r["doc_id"] for r in retrieved]
        retrieval_scores = [r["score"] for r in retrieved]
        chunk_contents = self._service.get_chunk_contents(chunk_ids)

        # Edge case: Single document - skip filtering
        if len(chunk_contents) == 1:
            logger.debug("Single document retrieved, skipping filtering phase")
            final_answer, final_usage = await self._aagent_final_predict(query, chunk_contents)
            all_token_usages.append({"token_usage": final_usage, "execution_time": 0})

            return GenerationResult(
                text=final_answer,
                token_usage=self._build_token_usage_dict(all_token_usages),
                metadata={
                    "pipeline_type": "main_rag",
                    "std_multiplier": self._std_multiplier,
                    "skipped_filtering": True,
                    "original_doc_count": 1,
                    "filtered_doc_count": 1,
                    "threshold": None,
                    "retrieved_chunk_ids": chunk_ids,
                    "retrieval_scores": retrieval_scores,
                    "relevance_scores": [{"doc_id": chunk_ids[0], "score": None}],
                },
            )

        # ==================== Phase 2: Agent-1 (Predictor) ====================
        candidate_answers: list[str] = []
        for doc in chunk_contents:
            answer, usage = await self._aagent_predict(query, doc)
            candidate_answers.append(answer)
            all_token_usages.append({"token_usage": usage, "execution_time": 0})

        # ==================== Phase 3: Agent-2 (Judge) ====================
        relevance_scores: list[float] = []
        for doc, answer in zip(chunk_contents, candidate_answers, strict=True):
            score, usage = await self._aagent_judge(query, doc, answer)
            relevance_scores.append(score)
            all_token_usages.append({"token_usage": usage, "execution_time": 0})

        # ==================== Phase 4: Adaptive Filtering ====================
        threshold = self._calculate_adaptive_threshold(relevance_scores, self._std_multiplier)

        # Create scored document list: (chunk_id, content, score)
        scored_docs = [
            {"doc_id": cid, "content": content, "score": score}
            for cid, content, score in zip(chunk_ids, chunk_contents, relevance_scores, strict=True)
        ]

        # Filter by threshold
        filtered_docs = [doc for doc in scored_docs if doc["score"] >= threshold]

        # Edge case: All documents filtered - use top-1 by score
        if not filtered_docs:
            logger.warning("All documents filtered out, using top-scoring document as fallback")
            # Sort by score descending and take top-1
            sorted_docs = sorted(scored_docs, key=lambda x: x["score"], reverse=True)
            filtered_docs = [sorted_docs[0]]

        # ==================== Phase 5: Ranking ====================
        # Sort filtered documents by score (descending)
        filtered_docs = sorted(filtered_docs, key=lambda x: x["score"], reverse=True)

        # ==================== Phase 6: Agent-3 (Final Predictor) ====================
        filtered_contents = [doc["content"] for doc in filtered_docs]
        final_answer, final_usage = await self._aagent_final_predict(query, filtered_contents)
        all_token_usages.append({"token_usage": final_usage, "execution_time": 0})

        # Build metadata with filtering statistics
        relevance_scores_metadata = [{"doc_id": doc["doc_id"], "score": doc["score"]} for doc in filtered_docs]

        return GenerationResult(
            text=final_answer,
            token_usage=self._build_token_usage_dict(all_token_usages),
            metadata={
                "pipeline_type": "main_rag",
                "std_multiplier": self._std_multiplier,
                "threshold": threshold,
                "original_doc_count": len(chunk_contents),
                "filtered_doc_count": len(filtered_docs),
                "retrieved_chunk_ids": chunk_ids,
                "retrieval_scores": retrieval_scores,
                "relevance_scores": relevance_scores_metadata,
            },
        )

    def _extract_token_usage(self, response: Any) -> dict[str, int]:
        """Extract token usage from LLM response.

        Args:
            response: LangChain LLM response object.

        Returns:
            Dict with prompt_tokens, completion_tokens, total_tokens.
        """
        token_usage = {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0,
        }

        # Try to get usage from response metadata (LangChain style)
        if hasattr(response, "usage_metadata") and response.usage_metadata:
            usage = response.usage_metadata
            token_usage = {
                "prompt_tokens": usage.get("input_tokens", 0),
                "completion_tokens": usage.get("output_tokens", 0),
                "total_tokens": usage.get("total_tokens", 0),
            }
        elif hasattr(response, "response_metadata"):
            usage = response.response_metadata.get("token_usage", {})
            if usage:
                token_usage = {
                    "prompt_tokens": usage.get("prompt_tokens", 0),
                    "completion_tokens": usage.get("completion_tokens", 0),
                    "total_tokens": usage.get("total_tokens", 0),
                }

        return token_usage

    @staticmethod
    def _build_token_usage_dict(results: list[dict]) -> dict[str, int]:
        """Aggregate token usage from multiple LLM calls using shared utility.

        Args:
            results: List of dicts with 'token_usage' and 'execution_time' keys.

        Returns:
            Aggregated token usage dict.
        """
        prompt, completion, embedding, _ = aggregate_token_usage(results)
        return {
            "prompt_tokens": prompt,
            "completion_tokens": completion,
            "total_tokens": prompt + completion,
            "embedding_tokens": embedding,
        }
