"""MAIN-RAG (Multi-Agent Filtering RAG) Pipeline for AutoRAG-Research.

Implements the MAIN-RAG algorithm from ACL 2025 paper (arxiv:2501.00332).
Uses three LLM agents to collaboratively filter retrieved documents through
adaptive thresholding and probabilistic scoring.
"""

import logging
from dataclasses import dataclass, field
from statistics import mean, stdev
from typing import Any

from langchain_core.language_models import BaseLanguageModel
from sqlalchemy.orm import Session, sessionmaker

from autorag_research.config import BaseGenerationPipelineConfig
from autorag_research.orm.service.generation_pipeline import GenerationResult
from autorag_research.orm.uow.generation_uow import GenerationUnitOfWork
from autorag_research.pipelines.generation.base import BaseGenerationPipeline
from autorag_research.pipelines.retrieval.base import BaseRetrievalPipeline

logger = logging.getLogger("AutoRAG-Research")

# Agent-1 (Predictor) prompts
DEFAULT_PREDICTOR_SYSTEM_PROMPT = """You are an accurate and reliable AI assistant that can answer questions with the help of external documents. You should only provide the correct answer without repeating the question and instruction."""

DEFAULT_PREDICTOR_USER_PROMPT = """Document:
{document}

Question: {query}"""

# Agent-2 (Judge) prompts
DEFAULT_JUDGE_SYSTEM_PROMPT = """You are a noisy document evaluator that can judge if the external document is noisy for the query with unrelated or misleading information. Given a retrieved Document, a Question, and an Answer generated by an LLM (LLM Answer), you should judge whether both the following two conditions are reached: (1) the Document provides specific information for answering the Question; (2) the LLM Answer directly answers the question based on the retrieved Document. Please note that external documents may contain noisy or factually incorrect information. If the information in the document does not contain the answer, you should point it out with evidence. You should answer with "Yes" or "No" with evidence of your judgment, where "No" means one of the conditions (1) and (2) are unreached and indicates it is a noisy document."""

DEFAULT_JUDGE_USER_PROMPT = """Document:
{document}

Question: {query}

Answer: {answer}

Does this document provide relevant information to answer the question correctly? Respond with only "Yes" or "No"."""

# Agent-3 (Final Predictor) prompts
DEFAULT_FINAL_PREDICTOR_SYSTEM_PROMPT = """You are an accurate and reliable AI assistant that can answer questions with the help of external documents. You should only provide the correct answer without repeating the question and instruction."""

DEFAULT_FINAL_PREDICTOR_USER_PROMPT = """Documents:
{documents}

Question: {query}"""


@dataclass(kw_only=True)
class MAINRAGPipelineConfig(BaseGenerationPipelineConfig):
    """Configuration for MAIN-RAG pipeline.

    MAIN-RAG (Multi-Agent Filtering RAG) uses three LLM agents to collaboratively
    filter retrieved documents through adaptive thresholding and probabilistic scoring.

    Attributes:
        name: Unique name for this pipeline instance.
        retrieval_pipeline_name: Name of the retrieval pipeline to use.
        llm: LangChain BaseLanguageModel instance for text generation.
            For optimal results, enable logprobs: llm.bind(logprobs=True, top_logprobs=5)
        std_multiplier: Multiplier for standard deviation in threshold calculation.
            threshold = mean - n * std. Default 0.0 means threshold = mean.
        predictor_system_prompt: System prompt for Agent-1 (Predictor).
        predictor_user_prompt: User prompt template for Agent-1 (must contain {document}, {query}).
        judge_system_prompt: System prompt for Agent-2 (Judge).
        judge_user_prompt: User prompt template for Agent-2 (must contain {document}, {query}, {answer}).
        final_predictor_system_prompt: System prompt for Agent-3 (Final Predictor).
        final_predictor_user_prompt: User prompt template for Agent-3 (must contain {documents}, {query}).
        top_k: Number of chunks to retrieve per query.
        batch_size: Number of queries to process in each batch.

    Example:
        ```python
        from langchain_openai import ChatOpenAI

        config = MAINRAGPipelineConfig(
            name="main_rag_v1",
            retrieval_pipeline_name="bm25_baseline",
            llm=ChatOpenAI(model="gpt-4o-mini").bind(logprobs=True, top_logprobs=5),
            std_multiplier=0.0,
            top_k=20,
        )
        ```
    """

    std_multiplier: float = 0.0
    predictor_system_prompt: str = field(default=DEFAULT_PREDICTOR_SYSTEM_PROMPT)
    predictor_user_prompt: str = field(default=DEFAULT_PREDICTOR_USER_PROMPT)
    judge_system_prompt: str = field(default=DEFAULT_JUDGE_SYSTEM_PROMPT)
    judge_user_prompt: str = field(default=DEFAULT_JUDGE_USER_PROMPT)
    final_predictor_system_prompt: str = field(default=DEFAULT_PREDICTOR_SYSTEM_PROMPT)
    final_predictor_user_prompt: str = field(default=DEFAULT_PREDICTOR_USER_PROMPT)

    def get_pipeline_class(self) -> type["MAINRAGPipeline"]:
        """Return the MAINRAGPipeline class."""
        return MAINRAGPipeline

    def get_pipeline_kwargs(self) -> dict[str, Any]:
        """Return kwargs for MAINRAGPipeline constructor."""
        if self._retrieval_pipeline is None:
            msg = f"Retrieval pipeline '{self.retrieval_pipeline_name}' not injected"
            raise ValueError(msg)

        return {
            "llm": self.llm,
            "retrieval_pipeline": self._retrieval_pipeline,
            "std_multiplier": self.std_multiplier,
            "predictor_system_prompt": self.predictor_system_prompt,
            "predictor_user_prompt": self.predictor_user_prompt,
            "judge_system_prompt": self.judge_system_prompt,
            "judge_user_prompt": self.judge_user_prompt,
            "final_predictor_system_prompt": self.final_predictor_system_prompt,
            "final_predictor_user_prompt": self.final_predictor_user_prompt,
        }


def calculate_binary_logprob_score(
    response: Any,
    positive_token: str = "Yes",  # noqa: S107
    negative_token: str = "No",  # noqa: S107
) -> tuple[float, bool]:
    """Calculate relevance score from Yes/No log probabilities.

    Implements the MAIN-RAG scoring formula: score = log P(Yes) - log P(No)

    Args:
        response: LangChain LLM response object.
        positive_token: Token representing positive judgment (default: "Yes").
        negative_token: Token representing negative judgment (default: "No").

    Returns:
        Tuple of (score, used_logprobs):
        - score: The calculated relevance score
        - used_logprobs: True if real logprobs were used, False if fallback

    Fallback behavior when logprobs unavailable:
        - Response starts with positive_token: score = 1.0
        - Response starts with negative_token: score = -1.0
        - Ambiguous: score = 0.0
    """
    from autorag_research.util import extract_token_logprobs

    # Try to extract logprobs
    logprobs = extract_token_logprobs(response, target_tokens=[positive_token, negative_token])

    if logprobs is not None:
        # Find the tokens (case-insensitive)
        pos_logprob = None
        neg_logprob = None

        for token, logprob in logprobs.items():
            if token.lower() == positive_token.lower():
                pos_logprob = logprob
            elif token.lower() == negative_token.lower():
                neg_logprob = logprob

        if pos_logprob is not None and neg_logprob is not None:
            # MAIN-RAG formula: r_i = log P(Yes) - log P(No)
            score = pos_logprob - neg_logprob
            return score, True

    # Fallback: binary scoring from response text
    response_text = ""
    if hasattr(response, "content"):
        response_text = response.content.strip().lower()
    elif isinstance(response, str):
        response_text = response.strip().lower()

    if response_text.startswith(positive_token.lower()):
        return 1.0, False
    elif response_text.startswith(negative_token.lower()):
        return -1.0, False
    else:
        logger.warning(f"Ambiguous judge response (no logprobs): {response_text[:50]}")
        return 0.0, False


class MAINRAGPipeline(BaseGenerationPipeline):
    """Multi-Agent Filtering Retrieval-Augmented Generation Pipeline.

    MAIN-RAG uses three LLM agents to collaboratively filter retrieved documents:
    - Agent-1 (Predictor): Generates candidate answers per document
    - Agent-2 (Judge): Scores document relevance via log probabilities
    - Agent-3 (Final Predictor): Generates final answer from filtered docs

    Algorithm Flow:
    1. RETRIEVAL: retrieval_pipeline.retrieve(query, top_k) -> Documents
    2. AGENT-1: For each document, generate candidate answer
    3. AGENT-2: For each (doc, query, answer) triplet, compute relevance score
    4. ADAPTIVE FILTERING: threshold = mean - n * std; filter docs by score
    5. RANKING: Sort filtered documents by score (descending)
    6. AGENT-3: Generate final answer using filtered documents

    Example:
        ```python
        from langchain_openai import ChatOpenAI

        from autorag_research.orm.connection import DBConnection
        from autorag_research.pipelines.generation.main_rag import MAINRAGPipeline
        from autorag_research.pipelines.retrieval.bm25 import BM25RetrievalPipeline

        db = DBConnection.from_config()
        session_factory = db.get_session_factory()

        # Create retrieval pipeline
        retrieval_pipeline = BM25RetrievalPipeline(
            session_factory=session_factory,
            name="bm25_baseline",
            index_path="/path/to/index",
        )

        # Create MAIN-RAG pipeline with logprobs enabled
        llm = ChatOpenAI(model="gpt-4o-mini").bind(logprobs=True, top_logprobs=5)
        pipeline = MAINRAGPipeline(
            session_factory=session_factory,
            name="main_rag_v1",
            llm=llm,
            retrieval_pipeline=retrieval_pipeline,
            std_multiplier=0.0,  # Paper's default
        )

        # Run pipeline
        results = pipeline.run(top_k=20)
        ```
    """

    def __init__(
        self,
        session_factory: sessionmaker[Session],
        name: str,
        llm: BaseLanguageModel,
        retrieval_pipeline: BaseRetrievalPipeline,
        std_multiplier: float = 0.0,
        predictor_system_prompt: str = DEFAULT_PREDICTOR_SYSTEM_PROMPT,
        predictor_user_prompt: str = DEFAULT_PREDICTOR_USER_PROMPT,
        judge_system_prompt: str = DEFAULT_JUDGE_SYSTEM_PROMPT,
        judge_user_prompt: str = DEFAULT_JUDGE_USER_PROMPT,
        final_predictor_system_prompt: str = DEFAULT_FINAL_PREDICTOR_SYSTEM_PROMPT,
        final_predictor_user_prompt: str = DEFAULT_FINAL_PREDICTOR_USER_PROMPT,
        schema: Any | None = None,
    ) -> None:
        """Initialize MAIN-RAG pipeline.

        Args:
            session_factory: SQLAlchemy sessionmaker for database connections.
            name: Name for this pipeline.
            llm: LangChain BaseLanguageModel instance for text generation.
                For optimal results, enable logprobs: llm.bind(logprobs=True, top_logprobs=5)
            retrieval_pipeline: Retrieval pipeline for fetching relevant context.
            std_multiplier: Multiplier for standard deviation in threshold calculation.
                threshold = mean - n * std. Default 0.0 means threshold = mean.
                Higher values are more permissive (lower threshold).
                Negative values are more aggressive (higher threshold).
            predictor_system_prompt: System prompt for Agent-1 (Predictor).
            predictor_user_prompt: User prompt template for Agent-1.
                Must contain {document} and {query} placeholders.
            judge_system_prompt: System prompt for Agent-2 (Judge).
            judge_user_prompt: User prompt template for Agent-2.
                Must contain {document}, {query}, and {answer} placeholders.
            final_predictor_system_prompt: System prompt for Agent-3 (Final Predictor).
            final_predictor_user_prompt: User prompt template for Agent-3.
                Must contain {documents} and {query} placeholders.
            schema: Schema namespace from create_schema(). If None, uses default schema.
        """
        # Store custom attributes BEFORE super().__init__()
        # because _get_pipeline_config() is called in parent init
        self._std_multiplier = std_multiplier
        self._predictor_system_prompt = predictor_system_prompt
        self._predictor_user_prompt = predictor_user_prompt
        self._judge_system_prompt = judge_system_prompt
        self._judge_user_prompt = judge_user_prompt
        self._final_predictor_system_prompt = final_predictor_system_prompt
        self._final_predictor_user_prompt = final_predictor_user_prompt

        super().__init__(session_factory, name, llm, retrieval_pipeline, schema)

    def _get_pipeline_config(self) -> dict[str, Any]:
        """Return MAIN-RAG pipeline configuration."""
        return {
            "type": "main_rag",
            "std_multiplier": self._std_multiplier,
            "predictor_system_prompt": self._predictor_system_prompt,
            "predictor_user_prompt": self._predictor_user_prompt,
            "judge_system_prompt": self._judge_system_prompt,
            "judge_user_prompt": self._judge_user_prompt,
            "final_predictor_system_prompt": self._final_predictor_system_prompt,
            "final_predictor_user_prompt": self._final_predictor_user_prompt,
        }

    def _invoke_llm(self, system_prompt: str, user_prompt: str, **format_kwargs: Any) -> tuple[Any, dict]:
        """Invoke LLM with system/user prompts and return response with token usage.

        Args:
            system_prompt: System prompt for the LLM.
            user_prompt: User prompt template with format placeholders.
            **format_kwargs: Values to format into the user prompt.

        Returns:
            Tuple of (response, token_usage_dict).
        """
        from langchain_core.messages import HumanMessage, SystemMessage

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt.format(**format_kwargs)),
        ]
        response = self._llm.invoke(messages)
        token_usage = self._extract_token_usage(response)
        return response, token_usage

    def _get_response_text(self, response: Any) -> str:
        """Extract text content from LLM response.

        Args:
            response: LangChain LLM response object.

        Returns:
            Text content of the response.
        """
        return response.content if hasattr(response, "content") else str(response)

    def _agent_predict(self, query: str, document: str) -> tuple[str, dict]:
        """Agent-1: Generate candidate answer for a single document.

        Args:
            query: The query text.
            document: The document content.

        Returns:
            Tuple of (answer_text, token_usage_dict).
        """
        response, token_usage = self._invoke_llm(
            self._predictor_system_prompt,
            self._predictor_user_prompt,
            query=query,
            document=document,
        )
        return self._get_response_text(response), token_usage

    def _agent_judge(self, query: str, document: str, answer: str) -> tuple[float, dict]:
        """Agent-2: Score document relevance using log probabilities.

        Args:
            query: The query text.
            document: The document content.
            answer: The candidate answer from Agent-1.

        Returns:
            Tuple of (relevance_score, token_usage_dict).

        Raises:
            LogprobsNotSupportedError: If LLM does not support logprobs.
        """
        from autorag_research.exceptions import LogprobsNotSupportedError

        response, token_usage = self._invoke_llm(
            self._judge_system_prompt,
            self._judge_user_prompt,
            query=query,
            document=document,
            answer=answer,
        )

        score, used_logprobs = calculate_binary_logprob_score(response)

        if not used_logprobs:
            raise LogprobsNotSupportedError(self.name)

        return score, token_usage

    def _agent_final_predict(self, query: str, documents: list[str]) -> tuple[str, dict]:
        """Agent-3: Generate final answer from filtered documents.

        Args:
            query: The query text.
            documents: List of filtered document contents.

        Returns:
            Tuple of (answer_text, token_usage_dict).
        """
        formatted_docs = "\n\n".join(f"[Document {i + 1}]\n{doc}" for i, doc in enumerate(documents))
        response, token_usage = self._invoke_llm(
            self._final_predictor_system_prompt,
            self._final_predictor_user_prompt,
            query=query,
            documents=formatted_docs,
        )
        return self._get_response_text(response), token_usage

    # ==================== Async Methods for Parallel Execution ====================

    async def _ainvoke_llm(self, system_prompt: str, user_prompt: str, **format_kwargs: Any) -> tuple[Any, dict]:
        """Async version of _invoke_llm using LangChain's ainvoke.

        Args:
            system_prompt: System prompt for the LLM.
            user_prompt: User prompt template with format placeholders.
            **format_kwargs: Values to format into the user prompt.

        Returns:
            Tuple of (response, token_usage_dict).
        """
        from langchain_core.messages import HumanMessage, SystemMessage

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt.format(**format_kwargs)),
        ]
        response = await self._llm.ainvoke(messages)
        token_usage = self._extract_token_usage(response)
        return response, token_usage

    async def _aagent_predict(self, query: str, document: str) -> tuple[str, dict]:
        """Async version of Agent-1 (Predictor).

        Args:
            query: The query text.
            document: The document content.

        Returns:
            Tuple of (answer_text, token_usage_dict).
        """
        response, token_usage = await self._ainvoke_llm(
            self._predictor_system_prompt,
            self._predictor_user_prompt,
            query=query,
            document=document,
        )
        return self._get_response_text(response), token_usage

    async def _aagent_judge(self, query: str, document: str, answer: str) -> tuple[float, dict]:
        """Async version of Agent-2 (Judge).

        Args:
            query: The query text.
            document: The document content.
            answer: The candidate answer from Agent-1.

        Returns:
            Tuple of (relevance_score, token_usage_dict).

        Raises:
            LogprobsNotSupportedError: If LLM does not support logprobs.
        """
        from autorag_research.exceptions import LogprobsNotSupportedError

        response, token_usage = await self._ainvoke_llm(
            self._judge_system_prompt,
            self._judge_user_prompt,
            query=query,
            document=document,
            answer=answer,
        )

        score, used_logprobs = calculate_binary_logprob_score(response)

        if not used_logprobs:
            raise LogprobsNotSupportedError(self.name)

        return score, token_usage

    async def _aagent_final_predict(self, query: str, documents: list[str]) -> tuple[str, dict]:
        """Async version of Agent-3 (Final Predictor).

        Args:
            query: The query text.
            documents: List of filtered document contents.

        Returns:
            Tuple of (answer_text, token_usage_dict).
        """
        formatted_docs = "\n\n".join(f"[Document {i + 1}]\n{doc}" for i, doc in enumerate(documents))
        response, token_usage = await self._ainvoke_llm(
            self._final_predictor_system_prompt,
            self._final_predictor_user_prompt,
            query=query,
            documents=formatted_docs,
        )
        return self._get_response_text(response), token_usage

    async def _run_predictors_parallel(
        self, query: str, documents: list[str], max_concurrency: int
    ) -> tuple[list[str], list[dict]]:
        """Run Agent-1 (Predictor) in parallel for all documents.

        Args:
            query: The query text.
            documents: List of document contents.
            max_concurrency: Maximum number of concurrent LLM calls.

        Returns:
            Tuple of (answers, token_usages) lists in same order as documents.
        """
        from autorag_research.util import run_with_concurrency_limit

        async def predict_one(doc: str) -> tuple[str, dict]:
            return await self._aagent_predict(query, doc)

        results = await run_with_concurrency_limit(
            documents,
            predict_one,
            max_concurrency=max_concurrency,
            error_message="Agent-1 (Predictor) failed",
        )

        # Unzip results, handle None values from failed calls
        answers: list[str] = []
        usages: list[dict] = []
        for result in results:
            if result is not None:
                answers.append(result[0])
                usages.append(result[1])
            else:
                answers.append("")
                usages.append({})
        return answers, usages

    async def _run_judges_parallel(
        self, query: str, documents: list[str], answers: list[str], max_concurrency: int
    ) -> tuple[list[float], list[dict]]:
        """Run Agent-2 (Judge) in parallel for all (document, answer) pairs.

        Args:
            query: The query text.
            documents: List of document contents.
            answers: List of candidate answers from Agent-1.
            max_concurrency: Maximum number of concurrent LLM calls.

        Returns:
            Tuple of (scores, token_usages) lists in same order as documents.

        Raises:
            LogprobsNotSupportedError: If the LLM does not support logprobs.
                This is a structural error that should fail fast.
        """
        from autorag_research.util import run_with_concurrency_limit

        # First, run a single judge call to check for LogprobsNotSupportedError.
        # This error is structural and should fail-fast rather than be silently caught.
        if documents and answers:
            first_score, first_usage = await self._aagent_judge(query, documents[0], answers[0])

            # If only one document, return immediately
            if len(documents) == 1:
                return [first_score], [first_usage]

            # Run remaining judges in parallel
            async def judge_one(item: tuple[str, str]) -> tuple[float, dict]:
                doc, answer = item
                return await self._aagent_judge(query, doc, answer)

            remaining_items = list(zip(documents[1:], answers[1:], strict=True))
            remaining_results = await run_with_concurrency_limit(
                remaining_items,
                judge_one,
                max_concurrency=max_concurrency,
                error_message="Agent-2 (Judge) failed",
            )

            # Combine first result with remaining results
            scores: list[float] = [first_score]
            usages: list[dict] = [first_usage]
            for result in remaining_results:
                if result is not None:
                    scores.append(result[0])
                    usages.append(result[1])
                else:
                    scores.append(0.0)
                    usages.append({})
            return scores, usages

        return [], []

    @staticmethod
    def _calculate_adaptive_threshold(scores: list[float], std_multiplier: float) -> float:
        """Calculate adaptive filtering threshold (static method for testing).

        Formula: threshold = mean - n * std
        Where:
            mean = average of scores
            std = standard deviation of scores
            n = std_multiplier

        Args:
            scores: List of relevance scores.
            std_multiplier: Multiplier for standard deviation.

        Returns:
            Threshold value.

        Raises:
            ValueError: If scores list is empty.
        """
        if not scores:
            msg = "Cannot calculate threshold from empty scores list"
            raise ValueError(msg)

        score_mean = mean(scores)

        # Handle single score or all same scores (zero std)
        if len(scores) == 1:
            return score_mean

        score_std = stdev(scores)

        # threshold = mean - n * std
        return score_mean - std_multiplier * score_std

    async def _agenerate(self, query: str, top_k: int, max_concurrency: int) -> GenerationResult:
        """Async version of _generate with parallel Agent-1 and Agent-2 execution.

        Implements the 6-phase MAIN-RAG algorithm with parallel execution:
        1. Retrieval: Get documents using retrieval pipeline (sync)
        2. Agent-1 (Predictor): Generate candidate answers per document (PARALLEL)
        3. Agent-2 (Judge): Score document relevance (PARALLEL)
        4. Adaptive Filtering: Filter documents by threshold
        5. Ranking: Sort filtered documents by score
        6. Agent-3 (Final Predictor): Generate final answer

        Args:
            query: The query text to answer.
            top_k: Number of chunks to retrieve.
            max_concurrency: Maximum number of concurrent LLM calls.

        Returns:
            GenerationResult containing the generated text and metadata.
        """
        all_token_usages: list[dict] = []

        # ==================== Phase 1: Retrieval ====================
        retrieved = self._retrieval_pipeline.retrieve(query, top_k)

        # Edge case: Empty retrieval results
        if not retrieved:
            return GenerationResult(
                text="",
                token_usage=None,
                metadata={
                    "pipeline_type": "main_rag",
                    "error": "No documents retrieved",
                    "std_multiplier": self._std_multiplier,
                    "original_doc_count": 0,
                    "filtered_doc_count": 0,
                },
            )

        # Get chunk IDs and contents
        chunk_ids = [r["doc_id"] for r in retrieved]
        retrieval_scores = [r["score"] for r in retrieved]
        chunk_contents = self._get_chunk_contents(chunk_ids)

        # Edge case: Single document - skip filtering
        if len(chunk_contents) == 1:
            logger.debug("Single document retrieved, skipping filtering phase")

            # Still run Agent-1 for candidate answer (maintains algorithm consistency)
            _, predict_usage = await self._aagent_predict(query, chunk_contents[0])
            all_token_usages.append(predict_usage)

            # Skip Agent-2 (Judge) and go directly to Agent-3
            final_answer, final_usage = await self._aagent_final_predict(query, chunk_contents)
            all_token_usages.append(final_usage)

            return GenerationResult(
                text=final_answer,
                token_usage=self._aggregate_token_usage(all_token_usages),
                metadata={
                    "pipeline_type": "main_rag",
                    "std_multiplier": self._std_multiplier,
                    "skipped_filtering": True,
                    "original_doc_count": 1,
                    "filtered_doc_count": 1,
                    "threshold": None,
                    "retrieved_chunk_ids": chunk_ids,
                    "retrieval_scores": retrieval_scores,
                    "relevance_scores": [{"doc_id": chunk_ids[0], "score": None}],
                },
            )

        # ==================== Phase 2: Agent-1 (Predictor) - PARALLEL ====================
        candidate_answers, predict_usages = await self._run_predictors_parallel(query, chunk_contents, max_concurrency)
        all_token_usages.extend(predict_usages)

        # ==================== Phase 3: Agent-2 (Judge) - PARALLEL ====================
        relevance_scores, judge_usages = await self._run_judges_parallel(
            query, chunk_contents, candidate_answers, max_concurrency
        )
        all_token_usages.extend(judge_usages)

        # ==================== Phase 4: Adaptive Filtering ====================
        threshold = self._calculate_adaptive_threshold(relevance_scores, self._std_multiplier)

        # Create scored document list: (chunk_id, content, score)
        scored_docs = [
            {"doc_id": cid, "content": content, "score": score}
            for cid, content, score in zip(chunk_ids, chunk_contents, relevance_scores, strict=True)
        ]

        # Filter by threshold
        filtered_docs = [doc for doc in scored_docs if doc["score"] >= threshold]

        # Edge case: All documents filtered - use top-1 by score
        if not filtered_docs:
            logger.warning("All documents filtered out, using top-scoring document as fallback")
            # Sort by score descending and take top-1
            sorted_docs = sorted(scored_docs, key=lambda x: x["score"], reverse=True)
            filtered_docs = [sorted_docs[0]]

        # ==================== Phase 5: Ranking ====================
        # Sort filtered documents by score (descending)
        filtered_docs = sorted(filtered_docs, key=lambda x: x["score"], reverse=True)

        # ==================== Phase 6: Agent-3 (Final Predictor) ====================
        filtered_contents = [doc["content"] for doc in filtered_docs]
        final_answer, final_usage = await self._aagent_final_predict(query, filtered_contents)
        all_token_usages.append(final_usage)

        # Build metadata with filtering statistics
        relevance_scores_metadata = [{"doc_id": doc["doc_id"], "score": doc["score"]} for doc in filtered_docs]

        return GenerationResult(
            text=final_answer,
            token_usage=self._aggregate_token_usage(all_token_usages),
            metadata={
                "pipeline_type": "main_rag",
                "std_multiplier": self._std_multiplier,
                "threshold": threshold,
                "original_doc_count": len(chunk_contents),
                "filtered_doc_count": len(filtered_docs),
                "retrieved_chunk_ids": chunk_ids,
                "retrieval_scores": retrieval_scores,
                "relevance_scores": relevance_scores_metadata,
            },
        )

    def _generate(self, query: str, top_k: int, max_concurrency: int = 10) -> GenerationResult:
        """Execute the MAIN-RAG generation pipeline with parallel agent execution.

        This method wraps the async _agenerate method for synchronous execution.

        Implements the 6-phase MAIN-RAG algorithm:
        1. Retrieval: Get documents using retrieval pipeline
        2. Agent-1 (Predictor): Generate candidate answers per document (parallel)
        3. Agent-2 (Judge): Score document relevance (parallel)
        4. Adaptive Filtering: Filter documents by threshold
        5. Ranking: Sort filtered documents by score
        6. Agent-3 (Final Predictor): Generate final answer

        Args:
            query: The query text to answer.
            top_k: Number of chunks to retrieve.
            max_concurrency: Maximum number of concurrent LLM calls (default: 10).

        Returns:
            GenerationResult containing the generated text and metadata.
        """
        import asyncio

        return asyncio.run(self._agenerate(query, top_k, max_concurrency))

    def _get_chunk_contents(self, chunk_ids: list[int]) -> list[str]:
        """Get chunk contents by IDs.

        Args:
            chunk_ids: List of chunk IDs to fetch.

        Returns:
            List of chunk content strings in the same order as input IDs.
        """
        with GenerationUnitOfWork(self.session_factory, self._schema) as uow:
            chunks = uow.chunks.get_by_ids(chunk_ids)
            # Create a map for preserving order
            chunk_map = {chunk.id: chunk.contents for chunk in chunks}
            return [chunk_map.get(cid, "") for cid in chunk_ids]

    def _extract_token_usage(self, response: Any) -> dict[str, int]:
        """Extract token usage from LLM response.

        Args:
            response: LangChain LLM response object.

        Returns:
            Dict with prompt_tokens, completion_tokens, total_tokens.
        """
        token_usage = {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0,
        }

        # Try to get usage from response metadata (LangChain style)
        if hasattr(response, "usage_metadata") and response.usage_metadata:
            usage = response.usage_metadata
            token_usage = {
                "prompt_tokens": usage.get("input_tokens", 0),
                "completion_tokens": usage.get("output_tokens", 0),
                "total_tokens": usage.get("total_tokens", 0),
            }
        elif hasattr(response, "response_metadata"):
            usage = response.response_metadata.get("token_usage", {})
            if usage:
                token_usage = {
                    "prompt_tokens": usage.get("prompt_tokens", 0),
                    "completion_tokens": usage.get("completion_tokens", 0),
                    "total_tokens": usage.get("total_tokens", 0),
                }

        return token_usage

    def _aggregate_token_usage(self, usages: list[dict]) -> dict[str, int]:
        """Aggregate token usage from multiple LLM calls.

        Args:
            usages: List of token usage dicts.

        Returns:
            Aggregated token usage dict.
        """
        total = {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0,
        }
        for usage in usages:
            if usage:
                total["prompt_tokens"] += usage.get("prompt_tokens", 0)
                total["completion_tokens"] += usage.get("completion_tokens", 0)
                total["total_tokens"] += usage.get("total_tokens", 0)
        return total

    def run(
        self,
        top_k: int = 10,
        batch_size: int = 10,
    ) -> dict[str, Any]:
        """Run the MAIN-RAG pipeline with parallel agent execution.

        Args:
            top_k: Number of top documents to retrieve per query.
            batch_size: Number of concurrent LLM calls per query (max_concurrency).
                Controls how many Agent-1/Agent-2 calls run in parallel.

        Returns:
            Dictionary with pipeline execution statistics:
            - pipeline_id: The pipeline ID
            - total_queries: Number of queries processed
            - total_tokens: Total tokens used (if available)
            - avg_execution_time_ms: Average execution time per query
        """
        return self._service.run_pipeline(
            generate_func=lambda q, k: self._generate(q, k, max_concurrency=batch_size),
            pipeline_id=self.pipeline_id,
            top_k=top_k,
            batch_size=batch_size,
        )
