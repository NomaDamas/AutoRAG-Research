# MAIN-RAG: Multi-Agent Filtering RAG Pipeline Configuration
#
# Training-free RAG framework using three LLM agents to collaboratively filter
# retrieved documents through adaptive thresholding and probabilistic scoring.
#
# Paper: "MAIN-RAG: Multi-Agent Filtering Retrieval-Augmented Generation"
#        ACL 2025 Long Paper (arxiv:2501.00332)
#
# Algorithm:
#   1. Agent-1 (Predictor): Generates candidate answers per document
#   2. Agent-2 (Judge): Scores document relevance via log probabilities
#   3. Adaptive Filtering: threshold = mean - n * std
#   4. Agent-3 (Final Predictor): Generates answer from filtered documents
#
# Parameters:
#   retrieval_pipeline_name: Name of retrieval pipeline to use
#   llm: LLM model identifier (must support logprobs for best results)
#   std_multiplier: Adaptive threshold parameter n (default: 0.0 = use mean)
#   top_k: Number of documents to retrieve initially (default: 20)
#
_target_: autorag_research.pipelines.generation.main_rag.MAINRAGPipelineConfig
description: "Multi-Agent Filtering RAG with adaptive document selection"
name: main_rag
retrieval_pipeline_name: bm25
llm: gpt-5.2
top_k: 20
batch_size: 10

# Adaptive filtering parameter
# n=0: threshold = mean (default, recommended by paper)
# n>0: lower threshold, more permissive (more documents pass)
# n<0: higher threshold, more aggressive filtering
std_multiplier: 0.0

# Optional: Custom prompt templates (system + user for each agent)
# Uncomment and modify to customize agent prompts
#
# Agent-1 (Predictor) prompts:
# predictor_system_prompt: |
#   You are an accurate and reliable AI assistant that can answer questions with the help of external documents.
#   You should only provide the correct answer without repeating the question and instruction.
#
# predictor_user_prompt: |
#   Document:
#   {document}
#
#   Question: {query}
#
# Agent-2 (Judge) prompts:
# judge_system_prompt: |
#   You are a noisy document evaluator that can judge if the external document is noisy for the query with unrelated or misleading information.
#   Given a retrieved Document, a Question, and an Answer generated by an LLM (LLM Answer), you should judge whether both the following two conditions are reached:
#   (1) the Document provides specific information for answering the Question;
#   (2) the LLM Answer directly answers the question based on the retrieved Document.
#   Please note that external documents may contain noisy or factually incorrect information.
#   If the information in the document does not contain the answer, you should point it out with evidence.
#   You should answer with "Yes" or "No" with evidence of your judgment, where "No" means one of the conditions (1) and (2) are unreached and indicates it is a noisy document.
#
# judge_user_prompt: |
#   Document:
#   {document}
#
#   Question: {query}
#
#   Answer: {answer}
#
#   Does this document provide relevant information to answer the question correctly? Respond with only "Yes" or "No".
#
# Agent-3 (Final Predictor) prompts:
# final_predictor_system_prompt: |
#   You are an accurate and reliable AI assistant that can answer questions with the help of external documents.
#   You should only provide the correct answer without repeating the question and instruction.
#
# final_predictor_user_prompt: |
#   Documents:
#   {documents}
#
#   Question: {query}
