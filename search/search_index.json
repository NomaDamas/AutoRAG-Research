{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"AutoRAG-Research","text":"<p>Automate your RAG research with reproducible benchmarks.</p>"},{"location":"#what-is-autorag-research","title":"What is AutoRAG-Research?","text":"<p>A Python framework for:</p> <ul> <li>Running RAG benchmarks on standard datasets</li> <li>Evaluating retrieval and generation pipelines</li> <li>Comparing algorithms with reproducible metrics</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>pip install autorag-research\ndocker-compose up -d\nautorag-research data restore beir scifact_openai-small\nautorag-research run --db-name=beir_scifact_test_openai_small\n</code></pre>"},{"location":"#choose-your-path","title":"Choose Your Path","text":"I want to... Go to Run text retrieval benchmarks Text Retrieval Tutorial Run full RAG with generation Text RAG Tutorial Work with visual documents Multimodal Tutorial Use my own dataset Custom Dataset Tutorial Test my own pipeline Custom Pipeline Tutorial Create my own metric Custom Metric Tutorial"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Learn - Core concepts and architecture</li> <li>Tutorial - Step-by-step guides</li> <li>Datasets - Available benchmarks</li> <li>Pipelines - Retrieval and generation algorithms</li> <li>Metrics - Evaluation measures</li> <li>CLI Reference - Command-line usage</li> <li>API Reference - Python API</li> </ul>"},{"location":"agent-skill/","title":"Agent Skill: autorag-query","text":"<p>AutoRAG-Research ships with an agent skill that lets AI coding agents query pipeline results and metrics using natural language.</p> <p>The skill follows the Vercel skills standard and works with Claude Code, Codex, Kiro, Cursor, and other compatible agents.</p>"},{"location":"agent-skill/#installation","title":"Installation","text":"<p>The skill is bundled at <code>.agents/skills/autorag-query/</code> in the repository and is auto-detected by agents when you work inside the project.</p> <p>To install globally (available across all projects):</p> <pre><code>npx skills add NomaDamas/AutoRAG-Research --skill autorag-query\n</code></pre>"},{"location":"agent-skill/#how-it-works","title":"How It Works","text":"<p>When you ask a data question, the agent:</p> <ol> <li>Reads the bundled database schema (<code>references/schema.sql</code>)</li> <li>Generates a SELECT-only SQL query</li> <li>Executes it via <code>scripts/query_executor.py</code></li> <li>Returns formatted results (table / JSON / CSV)</li> </ol> <p>Example:</p> <p>You: \"Which pipeline has the best BLEU score?\"</p> <p>Agent reads the schema, generates SQL, runs it, and replies: \"hybrid_search_v2 achieved the highest BLEU score of 0.85.\"</p>"},{"location":"agent-skill/#what-you-can-ask","title":"What You Can Ask","text":"<ul> <li>\"Show me all pipelines and their types\"</li> <li>\"Which retrieval pipeline has the best recall?\"</li> <li>\"Compare token usage across generation pipelines\"</li> <li>\"What are the 5 worst-performing queries for BLEU?\"</li> <li>\"Show retrieval scores for query #42\"</li> </ul>"},{"location":"agent-skill/#query-executor-script","title":"Query Executor Script","text":"<p>The skill includes a standalone script you can also run directly.</p>"},{"location":"agent-skill/#basic-usage","title":"Basic Usage","text":"<pre><code>uv run python .agents/skills/autorag-query/scripts/query_executor.py \\\n  --query \"SELECT name, pipeline_type FROM pipeline LIMIT 5\" \\\n  --config-path configs\n</code></pre>"},{"location":"agent-skill/#parameterized-queries","title":"Parameterized Queries","text":"<p>Use <code>:param_name</code> placeholders with <code>--params</code> for safe value substitution:</p> <pre><code>uv run python .agents/skills/autorag-query/scripts/query_executor.py \\\n  --query \"SELECT p.name, s.metric_result FROM summary s JOIN pipeline p ON s.pipeline_id = p.id JOIN metric m ON s.metric_id = m.id WHERE m.name = :metric_name ORDER BY s.metric_result DESC LIMIT 3\" \\\n  --config-path configs \\\n  --params '{\"metric_name\": \"rouge\"}'\n</code></pre>"},{"location":"agent-skill/#output-formats","title":"Output Formats","text":"<pre><code># JSON output\nuv run python .agents/skills/autorag-query/scripts/query_executor.py \\\n  --query \"SELECT name, metric_type FROM metric\" \\\n  --config-path configs \\\n  --format json\n\n# CSV output\nuv run python .agents/skills/autorag-query/scripts/query_executor.py \\\n  --query \"SELECT name FROM pipeline\" \\\n  --config-path configs \\\n  --format csv\n</code></pre>"},{"location":"agent-skill/#options","title":"Options","text":"Flag Description Default <code>--query</code>, <code>-q</code> SQL query (SELECT only, required) - <code>--format</code>, <code>-f</code> Output format: <code>table</code>, <code>json</code>, <code>csv</code> <code>table</code> <code>--config-path</code>, <code>-c</code> Path to <code>configs/</code> directory containing <code>db.yaml</code> env vars fallback <code>--params</code>, <code>-p</code> JSON parameters for <code>:param</code> placeholders - <code>--timeout</code>, <code>-t</code> Query timeout in seconds <code>10</code> <code>--limit</code>, <code>-l</code> Max rows returned (0 = unlimited) <code>10000</code> <code>--database</code>, <code>-d</code> Database name override from config"},{"location":"agent-skill/#connection","title":"Connection","text":"<p>The script auto-detects the database connection:</p> <ol> <li>Config file (if <code>--config-path</code> is provided): Reads <code>db.yaml</code> from the specified directory</li> <li>Environment variables (fallback): Uses <code>POSTGRES_HOST</code>, <code>POSTGRES_PORT</code>, <code>POSTGRES_USER</code>, <code>POSTGRES_PASSWORD</code>, <code>POSTGRES_DB</code></li> </ol>"},{"location":"agent-skill/#safety","title":"Safety","text":"<ul> <li>Only <code>SELECT</code> statements are allowed (DDL/DML keywords are rejected)</li> <li>Dangerous PostgreSQL functions are blocked (<code>pg_read_file</code>, <code>pg_execute</code>, <code>COPY</code>, etc.)</li> <li>Results are capped at 10,000 rows by default (enforced via subquery wrapper)</li> <li>Query timeout defaults to 10 seconds</li> <li>Engine connections are disposed after each execution</li> </ul>"},{"location":"agent-skill/#query-templates","title":"Query Templates","text":"<p>The skill bundles 20+ query templates in <code>references/common-queries.md</code>, organized by use case:</p> <ul> <li>Pipeline comparison: Top pipelines by metric, multi-metric pivot tables</li> <li>Per-query analysis: Score breakdowns, ground truth comparison, worst-performing queries</li> <li>Retrieval results: Retrieved chunks with scores, recall calculation</li> <li>Token usage: Per-pipeline totals, most expensive queries, usage over time</li> <li>Execution performance: Slowest queries, average execution time by pipeline</li> <li>JSONB extraction: <code>token_usage</code>, <code>config</code>, and <code>result_metadata</code> patterns</li> </ul>"},{"location":"agent-skill/#skill-directory-structure","title":"Skill Directory Structure","text":"<pre><code>.agents/skills/autorag-query/\n\u251c\u2500\u2500 SKILL.md                    # Skill definition (auto-detected by agents)\n\u251c\u2500\u2500 references/\n\u2502   \u251c\u2500\u2500 schema.sql              # Complete database schema\n\u2502   \u2514\u2500\u2500 common-queries.md       # 20+ curated query templates\n\u2514\u2500\u2500 scripts/\n    \u2514\u2500\u2500 query_executor.py       # Safe SQL execution script\n</code></pre>"},{"location":"cli/","title":"CLI Reference","text":"<p>Command-line interface for AutoRAG-Research.</p>"},{"location":"cli/#environment-variables","title":"Environment Variables","text":"Variable Description POSTGRES_HOST PostgreSQL host POSTGRES_PORT PostgreSQL port POSTGRES_USER PostgreSQL user POSTGRES_PASSWORD PostgreSQL password"},{"location":"cli/#commands","title":"Commands","text":""},{"location":"cli/#autorag-research","title":"autorag-research","text":"<p>AutoRAG-Research CLI - RAG research on steroids.</p>"},{"location":"cli/#usage","title":"Usage","text":"<p><code>autorag-research [OPTIONS] COMMAND [ARGS]...</code></p>"},{"location":"cli/#arguments","title":"Arguments","text":"<p>No arguments available</p>"},{"location":"cli/#options","title":"Options","text":"Name Description Required Default <code>-cp, --config-path PATH</code> Path to configuration directory  [env var: AUTORAG_RESEARCH_CONFIG_PATH] No - <code>-V, --version</code> Show version and exit No - <code>--help</code> Show this message and exit. No -"},{"location":"cli/#commands_1","title":"Commands","text":"Name Description <code>health-check</code> Health check a specific model config. <code>show</code> Show available resources. <code>init</code> Download default configuration files to... <code>run</code> Run experiment pipelines with metrics... <code>data</code> Manage PostgreSQL dump files via... <code>drop</code> Drop PostgreSQL databases. <code>ingest</code> Ingest datasets into PostgreSQL. <code>plugin</code> Manage AutoRAG-Research plugins."},{"location":"cli/#subcommands","title":"Subcommands","text":""},{"location":"cli/#autorag-research-health-check","title":"<code>autorag-research health-check</code>","text":"<p>Health check a specific model config.</p> <p>Loads the model config via Hydra and runs a health check to verify the model is functional.</p> <p>Examples:   autorag-research health-check embedding mock   autorag-research health-check llm openai-gpt4   autorag-research health-check reranker cohere</p>"},{"location":"cli/#usage_1","title":"Usage","text":"<p><code>autorag-research health-check [OPTIONS] MODEL_TYPE:{embedding|llm|reranker} NAME</code></p>"},{"location":"cli/#arguments_1","title":"Arguments","text":"Name Description Required <code>MODEL_TYPE:{embedding|llm|reranker}</code> Model type to check: embedding, llm, or reranker Yes <code>NAME</code> Config name (YAML filename without extension) Yes"},{"location":"cli/#options_1","title":"Options","text":"Name Description Required Default <code>--help</code> Show this message and exit. No -"},{"location":"cli/#autorag-research-show","title":"<code>autorag-research show</code>","text":"<p>Show available resources.</p> <p>RESOURCE types:   datasets   - Available dump files (optionally filter by ingestor name)   ingestors  - Available data ingestors with their parameters   pipelines  - Available pipeline configurations   metrics    - Available evaluation metrics   databases  - Available PostgreSQL databases</p> <p>Examples:   autorag-research show datasets   autorag-research show datasets beir   autorag-research show ingestors   autorag-research show pipelines   autorag-research show metrics   autorag-research show databases</p>"},{"location":"cli/#usage_2","title":"Usage","text":"<p><code>autorag-research show [OPTIONS] RESOURCE:{datasets|ingestors|pipelines|metrics|databases} [NAME]</code></p>"},{"location":"cli/#arguments_2","title":"Arguments","text":"Name Description Required <code>RESOURCE:{datasets|ingestors|pipelines|metrics|databases}</code> Resource type: datasets, ingestors, pipelines, metrics, or databases Yes <code>[NAME]</code> Resource name (e.g., ingestor name for 'datasets') No"},{"location":"cli/#options_2","title":"Options","text":"Name Description Required Default <code>--help</code> Show this message and exit. No -"},{"location":"cli/#autorag-research-init","title":"<code>autorag-research init</code>","text":"<p>Download default configuration files to the configured directory.</p> <p>Downloads configuration files from the AutoRAG-Research GitHub repository to your local configs directory. Existing files are not overwritten.</p> <p>Examples:   autorag-research init   autorag-research --config-path=/my/configs init</p>"},{"location":"cli/#usage_3","title":"Usage","text":"<p><code>autorag-research init [OPTIONS]</code></p>"},{"location":"cli/#arguments_3","title":"Arguments","text":"<p>No arguments available</p>"},{"location":"cli/#options_3","title":"Options","text":"Name Description Required Default <code>--help</code> Show this message and exit. No -"},{"location":"cli/#autorag-research-run","title":"<code>autorag-research run</code>","text":"<p>Run experiment pipelines with metrics evaluation.</p> <p>Configuration is loaded from configs/experiment.yaml (or specified --config-name).</p> <p>Examples:   autorag-research run --db-name=beir_scifact_test   autorag-research run --db-name=beir_scifact_test --verbose   autorag-research run --db-name=beir_scifact_test --config-name=my_experiment</p>"},{"location":"cli/#usage_4","title":"Usage","text":"<p><code>autorag-research run [OPTIONS]</code></p>"},{"location":"cli/#arguments_4","title":"Arguments","text":"<p>No arguments available</p>"},{"location":"cli/#options_4","title":"Options","text":"Name Description Required Default <code>-d, --db-name TEXT</code> Database schema name (required) No - <code>-cn, --config-name TEXT</code> Config file name without .yaml extension  [default: experiment] No - <code>-v, --verbose</code> Enable verbose logging No - <code>--help</code> Show this message and exit. No -"},{"location":"cli/#autorag-research-data","title":"<code>autorag-research data</code>","text":"<p>Manage PostgreSQL dump files via HuggingFace Hub.</p>"},{"location":"cli/#usage_5","title":"Usage","text":"<p><code>autorag-research data [OPTIONS] COMMAND [ARGS]...</code></p>"},{"location":"cli/#arguments_5","title":"Arguments","text":"<p>No arguments available</p>"},{"location":"cli/#options_5","title":"Options","text":"Name Description Required Default <code>--help</code> Show this message and exit. No -"},{"location":"cli/#subcommands_1","title":"Subcommands","text":""},{"location":"cli/#autorag-research-data-restore","title":"<code>autorag-research data restore</code>","text":"<p>Download and restore a dump to PostgreSQL.</p> <p>Downloads the dump file from HuggingFace Hub (if not cached) and restores it to a PostgreSQL database. The database will be created if it doesn't exist.</p> <p>Examples:     autorag-research data restore beir scifact_openai-small     autorag-research data restore beir scifact_openai-small --db-name=my_custom_db     autorag-research data restore mteb nfcorpus_bge-small --clean     autorag-research data restore beir scifact_openai-small --clean --yes</p>"},{"location":"cli/#usage_6","title":"Usage","text":"<p><code>autorag-research data restore [OPTIONS] INGESTOR FILENAME</code></p>"},{"location":"cli/#arguments_6","title":"Arguments","text":"Name Description Required <code>INGESTOR</code> Ingestor name (e.g., beir, mteb) Yes <code>FILENAME</code> Dump filename without .dump extension Yes"},{"location":"cli/#options_6","title":"Options","text":"Name Description Required Default <code>--db-name TEXT</code> Target database name (defaults to filename) No - <code>--clean</code> Drop database objects before recreating No - <code>--no-owner / --with-owner</code> Skip restoration of object ownership  [default: no-owner] No - <code>-y, --yes</code> Skip confirmation prompts No - <code>--help</code> Show this message and exit. No -"},{"location":"cli/#autorag-research-data-dump","title":"<code>autorag-research data dump</code>","text":"<p>Export a database to a dump file.</p> <p>Creates a PostgreSQL dump file using pg_dump in custom format, which can be restored with 'autorag-research data restore' or pg_restore.</p> <p>Examples:     autorag-research data dump --db-name=beir_scifact_test     autorag-research data dump --db-name=beir_scifact_test --output=./backup.dump</p>"},{"location":"cli/#usage_7","title":"Usage","text":"<p><code>autorag-research data dump [OPTIONS]</code></p>"},{"location":"cli/#arguments_7","title":"Arguments","text":"<p>No arguments available</p>"},{"location":"cli/#options_7","title":"Options","text":"Name Description Required Default <code>--db-name TEXT</code> Database name to dump Yes - <code>-o, --output PATH</code> Output file path (defaults to .dump) No - <code>--no-owner / --with-owner</code> Skip output of ownership commands  [default: no-owner] No - <code>--help</code> Show this message and exit. No -"},{"location":"cli/#autorag-research-data-upload","title":"<code>autorag-research data upload</code>","text":"<p>Upload a dump file to HuggingFace Hub.</p> <p>Requires authentication via HF_TOKEN environment variable or 'huggingface-cli login'.</p> <p>Examples:     autorag-research data upload ./scifact.dump beir scifact_openai-small     autorag-research data upload ./scifact.dump beir scifact_openai-small -m \"Add new dump\"     autorag-research data upload ./scifact.dump beir scifact_openai-small --repo myorg/custom-repo</p>"},{"location":"cli/#usage_8","title":"Usage","text":"<p><code>autorag-research data upload [OPTIONS] FILE INGESTOR FILENAME</code></p>"},{"location":"cli/#arguments_8","title":"Arguments","text":"Name Description Required <code>FILE</code> Path to the dump file to upload Yes <code>INGESTOR</code> Ingestor name (e.g., beir, mteb) Yes <code>FILENAME</code> Target filename without .dump extension Yes"},{"location":"cli/#options_8","title":"Options","text":"Name Description Required Default <code>-r, --repo TEXT</code> Override HuggingFace repo ID (e.g., myorg/my-repo) No - <code>-m, --message TEXT</code> Commit message for the upload No - <code>--help</code> Show this message and exit. No -"},{"location":"cli/#autorag-research-drop","title":"<code>autorag-research drop</code>","text":"<p>Drop PostgreSQL databases.</p>"},{"location":"cli/#usage_9","title":"Usage","text":"<p><code>autorag-research drop [OPTIONS] COMMAND [ARGS]...</code></p>"},{"location":"cli/#arguments_9","title":"Arguments","text":"<p>No arguments available</p>"},{"location":"cli/#options_9","title":"Options","text":"Name Description Required Default <code>--help</code> Show this message and exit. No -"},{"location":"cli/#subcommands_2","title":"Subcommands","text":""},{"location":"cli/#autorag-research-drop-database","title":"<code>autorag-research drop database</code>","text":"<p>Drop a PostgreSQL database.</p> <p>Examples:     autorag-research drop database --db-name=beir_scifact_test     autorag-research drop database --db-name=beir_scifact_test --yes</p>"},{"location":"cli/#usage_10","title":"Usage","text":"<p><code>autorag-research drop database [OPTIONS]</code></p>"},{"location":"cli/#arguments_10","title":"Arguments","text":"<p>No arguments available</p>"},{"location":"cli/#options_10","title":"Options","text":"Name Description Required Default <code>--db-name TEXT</code> Database name to drop Yes - <code>-y, --yes</code> Skip confirmation prompts No - <code>--help</code> Show this message and exit. No -"},{"location":"cli/#autorag-research-ingest","title":"<code>autorag-research ingest</code>","text":"<p>Ingest datasets into PostgreSQL.</p>"},{"location":"cli/#usage_11","title":"Usage","text":"<p><code>autorag-research ingest [OPTIONS] COMMAND [ARGS]...</code></p>"},{"location":"cli/#arguments_11","title":"Arguments","text":"<p>No arguments available</p>"},{"location":"cli/#options_11","title":"Options","text":"Name Description Required Default <code>-n, --name TEXT</code> Ingestor name (beir, mteb, ragbench, etc.). Use 'autorag-research show ingestors' to see all. No - <code>-e, --extra TEXT</code> Ingestor-specific params as key=value (e.g., --extra dataset-name=scifact) No - <code>--subset [train|dev|test]</code> Dataset split: train, dev, or test  [default: test] No - <code>--query-limit INTEGER</code> Maximum number of queries to ingest No - <code>--min-corpus-cnt INTEGER</code> Minimum number of corpus documents to ingest No - <code>--db-name TEXT</code> Custom database name (auto-generated if not specified) No - <code>--embedding-model TEXT</code> Embedding model config name from configs/embedding/  [default: openai-small] No - <code>--embed-batch-size INTEGER</code> Batch size for embedding  [default: 128] No - <code>--embed-concurrency INTEGER</code> Max concurrent embedding calls  [default: 16] No - <code>--skip-embedding</code> Skip embedding step (ingest data only) No - <code>--help</code> Show this message and exit. No -"},{"location":"cli/#autorag-research-plugin","title":"<code>autorag-research plugin</code>","text":"<p>Manage AutoRAG-Research plugins.</p>"},{"location":"cli/#usage_12","title":"Usage","text":"<p><code>autorag-research plugin [OPTIONS] COMMAND [ARGS]...</code></p>"},{"location":"cli/#arguments_12","title":"Arguments","text":"<p>No arguments available</p>"},{"location":"cli/#options_12","title":"Options","text":"Name Description Required Default <code>--help</code> Show this message and exit. No -"},{"location":"cli/#subcommands_3","title":"Subcommands","text":""},{"location":"cli/#autorag-research-plugin-sync","title":"<code>autorag-research plugin sync</code>","text":"<p>Discover installed plugins and copy their YAML configs into configs/.</p> <p>Scans all installed packages that register <code>autorag_research.pipelines</code> or <code>autorag_research.metrics</code> entry points. YAML files are copied into the local <code>configs/</code> directory. Existing files are never overwritten.</p> <p>Example::</p> <pre><code>pip install autorag-research-elasticsearch\nautorag-research plugin sync\n</code></pre>"},{"location":"cli/#usage_13","title":"Usage","text":"<p><code>autorag-research plugin sync [OPTIONS]</code></p>"},{"location":"cli/#arguments_13","title":"Arguments","text":"<p>No arguments available</p>"},{"location":"cli/#options_13","title":"Options","text":"Name Description Required Default <code>--help</code> Show this message and exit. No -"},{"location":"cli/#autorag-research-plugin-create","title":"<code>autorag-research plugin create</code>","text":"<p>Scaffold a new plugin project in the current directory.</p> <p>Creates a plugin directory structure with: - pyproject.toml with entry_points configured - Pipeline/metric skeleton code - YAML config file - Basic test file</p> <p>Example::</p> <pre><code>autorag-research plugin create my_search --type=retrieval\ncd my_search_plugin\npip install -e .\nautorag-research plugin sync\n</code></pre>"},{"location":"cli/#usage_14","title":"Usage","text":"<p><code>autorag-research plugin create [OPTIONS] NAME</code></p>"},{"location":"cli/#arguments_14","title":"Arguments","text":"Name Description Required <code>NAME</code> Plugin name (e.g., 'my_custom_retrieval') Yes"},{"location":"cli/#options_14","title":"Options","text":"Name Description Required Default <code>-t, --type TEXT</code> Plugin type: retrieval, generation, metric_retrieval, metric_generation, ingestor Yes - <code>--help</code> Show this message and exit. No -"},{"location":"cli/#quick-cleanup-example","title":"Quick Cleanup Example","text":"<pre><code>autorag-research drop database --db-name=beir_scifact_test_openai_small --yes\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>Guidelines for contributing to AutoRAG-Research.</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":"<ol> <li>Fork the repository</li> <li>Set up development environment</li> <li>Create a feature branch</li> <li>Make changes</li> <li>Submit a pull request</li> </ol>"},{"location":"contributing/#code-style","title":"Code Style","text":"<ul> <li>Python 3.10+ type hints</li> <li>Line length: 120 characters</li> <li>Ruff for linting and formatting</li> <li>ty for type checking</li> </ul>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<ol> <li>Write tests for new functionality</li> <li>Update documentation if needed</li> <li>Run <code>make check</code> before submitting</li> <li>Keep PRs focused on single changes</li> </ol>"},{"location":"contributing/#areas-for-contribution","title":"Areas for Contribution","text":"Area Description Pipelines New retrieval/generation algorithms Metrics Additional evaluation metrics Datasets New dataset ingestors Documentation Improvements and examples Bug fixes Issue resolution"},{"location":"contributing/#related","title":"Related","text":"<ul> <li>Development Setup - Environment setup</li> <li>Custom Pipeline Tutorial - Adding pipelines</li> <li>Custom Metric Tutorial - Adding metrics</li> </ul>"},{"location":"contributing/development-setup/","title":"Development Setup","text":"<p>Set up a development environment for contributing.</p>"},{"location":"contributing/development-setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>Docker</li> <li>Git</li> <li>uv (recommended) or pip</li> </ul>"},{"location":"contributing/development-setup/#clone-repository","title":"Clone Repository","text":"<pre><code>git clone https://github.com/NomaDamas/AutoRAG-Research.git\ncd AutoRAG-Research\n</code></pre>"},{"location":"contributing/development-setup/#install-dependencies","title":"Install Dependencies","text":"<pre><code># Using uv (recommended)\nuv sync --all-extras --all-groups\n\n# Using pip\npip install -e \".[dev,test]\"\n</code></pre>"},{"location":"contributing/development-setup/#start-postgresql","title":"Start PostgreSQL","text":"<pre><code>docker-compose up -d\n</code></pre> <p>Wait for PostgreSQL to be ready:</p> <pre><code>make docker-wait\n</code></pre>"},{"location":"contributing/development-setup/#run-tests","title":"Run Tests","text":"<pre><code># Full test suite\nmake test\n\n# Quick tests (assumes PostgreSQL running)\nmake test-only\n\n# Single test\nuv run pytest tests/path/to/test_file.py::test_function_name -v\n</code></pre>"},{"location":"contributing/development-setup/#code-quality","title":"Code Quality","text":"<pre><code># Run all checks\nmake check\n\n# Individual tools\nuv run ruff check .\nuv run ruff format .\nuv run ty\n</code></pre>"},{"location":"contributing/development-setup/#common-commands","title":"Common Commands","text":"Command Description <code>make install</code> Create venv, install deps <code>make check</code> Run all code quality checks <code>make test</code> Full test with Docker lifecycle <code>make test-only</code> Run tests (PostgreSQL assumed running) <code>make docs</code> Build and serve docs locally <code>make docker-up</code> Start PostgreSQL container <code>make docker-down</code> Stop container"},{"location":"contributing/development-setup/#test-markers","title":"Test Markers","text":"<pre><code>@pytest.mark.gpu       # Requires GPU\n@pytest.mark.api       # Requires LLM/API calls\n@pytest.mark.data      # Downloads external data\n@pytest.mark.asyncio   # Async test\n</code></pre>"},{"location":"contributing/development-setup/#directory-structure","title":"Directory Structure","text":"<pre><code>autorag_research/\n\u251c\u2500\u2500 cli/                 # CLI commands\n\u251c\u2500\u2500 data/\n\u2502   \u2514\u2500\u2500 ingestor/        # Dataset ingestors\n\u251c\u2500\u2500 evaluation/\n\u2502   \u2514\u2500\u2500 metrics/         # Evaluation metrics\n\u251c\u2500\u2500 orm/                 # Database layer\n\u2502   \u251c\u2500\u2500 models/          # SQLAlchemy models\n\u2502   \u251c\u2500\u2500 repository/      # Data access\n\u2502   \u251c\u2500\u2500 service/         # Business logic\n\u2502   \u2514\u2500\u2500 uow/             # Unit of Work\n\u251c\u2500\u2500 pipelines/\n\u2502   \u251c\u2500\u2500 retrieval/       # Retrieval pipelines\n\u2502   \u2514\u2500\u2500 generation/      # Generation pipelines\n\u251c\u2500\u2500 config.py            # Configuration classes\n\u251c\u2500\u2500 executor.py          # Pipeline orchestration\n\u2514\u2500\u2500 util.py              # Utilities\n</code></pre>"},{"location":"datasets/","title":"Datasets","text":"<p>Available benchmarks for RAG evaluation.</p>"},{"location":"datasets/#comparison","title":"Comparison","text":"Dataset Modality Queries Documents Generation GT Use BEIR Text 300-15k 3k-5M No Retrieval MTEB Text varies varies No Retrieval RAGBench Text varies varies Yes RAG MrTyDi Text varies varies No Multilingual BRIGHT Text varies varies No Retrieval Open-RAGBench Multimodal varies varies Yes RAG ViDoRe Multimodal varies varies No Visual ViDoRe v2 Multimodal varies varies No Visual ViDoRe v3 Multimodal varies varies No Visual VisRAG Multimodal varies varies No Visual"},{"location":"datasets/#text-vs-multimodal","title":"Text vs Multimodal","text":"<p>Text datasets:</p> <ul> <li>Documents as plain text</li> <li>Text embeddings + BM25 tokens</li> <li>Use: Text Retrieval Tutorial</li> </ul> <p>Multimodal datasets:</p> <ul> <li>Documents as images (PDF pages)</li> <li>Image embeddings (ColPali)</li> <li>Use: Multimodal Tutorial</li> </ul>"},{"location":"datasets/#browse","title":"Browse","text":"<ul> <li>Text Datasets</li> <li>Multimodal Datasets</li> </ul>"},{"location":"datasets/multimodal/","title":"Multimodal Datasets","text":"<p>Visual document benchmarks for image-based retrieval.</p>"},{"location":"datasets/multimodal/#available-datasets","title":"Available Datasets","text":"Dataset Description ViDoRe Visual Document Retrieval ViDoRe v2 Visual Document Retrieval v2 ViDoRe v3 Visual Document Retrieval v3 VisRAG Visual RAG benchmark Open-RAGBench Open RAG benchmark from arXiv PDFs"},{"location":"datasets/multimodal/#key-characteristics","title":"Key Characteristics","text":"<ul> <li>Documents are stored as images (PDF pages, screenshots)</li> <li>Requires multimodal embedding models (ColPali)</li> <li>BM25 text search not available</li> <li>GPU recommended for embedding generation</li> </ul>"},{"location":"datasets/multimodal/open-ragbench/","title":"Open-RAGBench","text":"<p>Open RAG Benchmark from arXiv PDFs by Vectara.</p>"},{"location":"datasets/multimodal/open-ragbench/#overview","title":"Overview","text":"Field Value Modality Multimodal Generation GT Yes HF Repository <code>open-ragbench-dumps</code>"},{"location":"datasets/multimodal/open-ragbench/#description","title":"Description","text":"<p>Open-RAGBench is created from arXiv papers, providing a benchmark for RAG systems with both retrieval and generation ground truth.</p>"},{"location":"datasets/multimodal/open-ragbench/#download","title":"Download","text":"<pre><code>autorag-research data restore open-ragbench &lt;dataset_name&gt;_&lt;embedding_model&gt;\n</code></pre>"},{"location":"datasets/multimodal/open-ragbench/#ingest-from-source","title":"Ingest from Source","text":"<pre><code>autorag-research ingest --name=open-ragbench --extra dataset-name=&lt;name&gt; --embedding-model=openai-small\n</code></pre>"},{"location":"datasets/multimodal/open-ragbench/#best-for","title":"Best For","text":"<ul> <li>Academic document retrieval</li> <li>Full RAG evaluation</li> <li>Scientific Q&amp;A</li> </ul>"},{"location":"datasets/multimodal/vidore/","title":"ViDoRe","text":"<p>Visual Document Retrieval benchmark (V1 QA datasets).</p>"},{"location":"datasets/multimodal/vidore/#overview","title":"Overview","text":"Field Value Modality Multimodal (Images) Generation GT Yes (varies by dataset) Structure 1:1 query-to-image pairs"},{"location":"datasets/multimodal/vidore/#description","title":"Description","text":"<p>ViDoRe (Visual Document Retrieval) V1 evaluates retrieval systems on document images, including PDF pages, slides, and screenshots. Each dataset contains query-image pairs where the task is to retrieve the relevant document image for a given query.</p>"},{"location":"datasets/multimodal/vidore/#available-datasets","title":"Available Datasets","text":"Dataset Rows Answer Format Language <code>arxivqa_test_subsampled</code> 500 Multiple choice (A/B/C/D) English <code>docvqa_test_subsampled</code> 500 Held out (test set) English <code>infovqa_test_subsampled</code> 500 Held out (test set) English <code>tabfquad_test_subsampled</code> 280 None French <code>tatdqa_test</code> 1663 None English <code>shiftproject_test</code> 1000 List of strings French <code>syntheticDocQA_artificial_intelligence_test</code> 1000 List of strings English <code>syntheticDocQA_energy_test</code> ~1000 List of strings English <code>syntheticDocQA_government_reports_test</code> ~1000 List of strings English <code>syntheticDocQA_healthcare_industry_test</code> ~1000 List of strings English"},{"location":"datasets/multimodal/vidore/#download","title":"Download","text":"<pre><code>autorag-research data restore vidore &lt;dataset_name&gt;_&lt;embedding_model&gt;\n</code></pre>"},{"location":"datasets/multimodal/vidore/#ingest-from-source","title":"Ingest from Source","text":"<pre><code># Ingest ArxivQA dataset with ColPali embeddings\nautorag-research ingest -n vidore --extra dataset-name=arxivqa_test_subsampled --embedding-model=colpali\n\n# Ingest DocVQA with custom query limit\nautorag-research ingest -n vidore --extra dataset-name=docvqa_test_subsampled --query-limit=100 --embedding-model=colpali\n\n# Skip embedding (ingest data only)\nautorag-research ingest -n vidore --extra dataset-name=tatdqa_test --skip-embedding\n</code></pre>"},{"location":"datasets/multimodal/vidore/#best-for","title":"Best For","text":"<ul> <li>Visual document understanding</li> <li>PDF page retrieval</li> <li>Multimodal embedding evaluation</li> <li>Document QA benchmarking</li> </ul>"},{"location":"datasets/multimodal/vidore/#reference","title":"Reference","text":"<ul> <li>ViDoRe Benchmark Collection</li> </ul>"},{"location":"datasets/multimodal/vidorev2/","title":"ViDoRe v2","text":"<p>Visual Document Retrieval benchmark version 2.</p>"},{"location":"datasets/multimodal/vidorev2/#overview","title":"Overview","text":"Field Value Modality Multimodal (Images) Generation GT No HF Repository <code>vidorev2-dumps</code>"},{"location":"datasets/multimodal/vidorev2/#description","title":"Description","text":"<p>ViDoRe v2 is an updated version of the Visual Document Retrieval benchmark with improved annotations and additional document types.</p>"},{"location":"datasets/multimodal/vidorev2/#download","title":"Download","text":"<pre><code>autorag-research data restore vidorev2 &lt;dataset_name&gt;_&lt;embedding_model&gt;\n</code></pre>"},{"location":"datasets/multimodal/vidorev2/#ingest-from-source","title":"Ingest from Source","text":"<pre><code>autorag-research ingest --name=vidorev2 --extra dataset-name=&lt;name&gt; --embedding-model=colpali\n</code></pre>"},{"location":"datasets/multimodal/vidorev2/#best-for","title":"Best For","text":"<ul> <li>Visual document understanding</li> <li>PDF page retrieval</li> <li>Multimodal embedding evaluation</li> </ul>"},{"location":"datasets/multimodal/vidorev3/","title":"ViDoRe v3","text":"<p>Visual Document Retrieval benchmark version 3.</p>"},{"location":"datasets/multimodal/vidorev3/#overview","title":"Overview","text":"Field Value Modality Multimodal (Images) Generation GT No HF Repository <code>vidorev3-dumps</code>"},{"location":"datasets/multimodal/vidorev3/#description","title":"Description","text":"<p>ViDoRe v3 is the latest version of the Visual Document Retrieval benchmark, featuring diverse document types across multiple domains.</p>"},{"location":"datasets/multimodal/vidorev3/#sub-datasets","title":"Sub-datasets","text":"Name Domain arxivqa Academic papers docvqa Document images infovqa Infographics tabfquad Tables"},{"location":"datasets/multimodal/vidorev3/#download","title":"Download","text":"<pre><code>autorag-research data restore vidorev3 arxivqa_colpali\n</code></pre>"},{"location":"datasets/multimodal/vidorev3/#ingest-from-source","title":"Ingest from Source","text":"<pre><code>autorag-research ingest --name=vidorev3 --extra dataset-name=arxivqa --embedding-model=colpali\n</code></pre>"},{"location":"datasets/multimodal/vidorev3/#best-for","title":"Best For","text":"<ul> <li>State-of-the-art visual retrieval evaluation</li> <li>Multi-domain document understanding</li> <li>ColPali model benchmarking</li> </ul>"},{"location":"datasets/multimodal/visrag/","title":"VisRAG","text":"<p>Visual RAG benchmark.</p>"},{"location":"datasets/multimodal/visrag/#overview","title":"Overview","text":"Field Value Modality Multimodal (Images) Generation GT No HF Repository <code>visrag-dumps</code>"},{"location":"datasets/multimodal/visrag/#description","title":"Description","text":"<p>VisRAG provides datasets for evaluating visual RAG systems, focusing on document images that require visual understanding for accurate retrieval.</p>"},{"location":"datasets/multimodal/visrag/#download","title":"Download","text":"<pre><code>autorag-research data restore visrag &lt;dataset_name&gt;_&lt;embedding_model&gt;\n</code></pre>"},{"location":"datasets/multimodal/visrag/#ingest-from-source","title":"Ingest from Source","text":"<pre><code>autorag-research ingest --name=visrag --extra dataset-name=&lt;name&gt; --embedding-model=colpali\n</code></pre>"},{"location":"datasets/multimodal/visrag/#best-for","title":"Best For","text":"<ul> <li>Visual RAG evaluation</li> <li>Document image understanding</li> <li>Multimodal retrieval research</li> </ul>"},{"location":"datasets/text/","title":"Text Datasets","text":"<p>Text-based benchmarks for retrieval and RAG evaluation.</p>"},{"location":"datasets/text/#available-datasets","title":"Available Datasets","text":"Dataset Description Generation GT BEIR Heterogeneous information retrieval No MTEB Massive Text Embedding Benchmark No RAGBench RAG evaluation benchmark Yes MrTyDi Multilingual retrieval No BRIGHT Reasoning-intensive retrieval No"},{"location":"datasets/text/beir/","title":"BEIR","text":"<p>Heterogeneous benchmark for information retrieval.</p>"},{"location":"datasets/text/beir/#overview","title":"Overview","text":"Field Value Modality Text Generation GT No HF Repository <code>beir-dumps</code> Paper Thakur et al., 2021"},{"location":"datasets/text/beir/#sub-datasets","title":"Sub-datasets","text":"Name Domain Queries Documents scifact Scientific 300 5,183 nfcorpus Biomedical 323 3,633 fiqa Financial 648 57,638 arguana Argument 1,406 8,674 scidocs Scientific 1,000 25,657 trec-covid Biomedical 50 171,332 nq Wikipedia 3,452 2,681,468 hotpotqa Wikipedia 7,405 5,233,329 msmarco Web 6,980 8,841,823 fever Fact 6,666 5,416,568 climate-fever Climate 1,535 5,416,593 dbpedia-entity Wikipedia 400 4,635,922 quora Questions 10,000 522,931 cqadupstack StackExchange 13,145 457,199"},{"location":"datasets/text/beir/#download","title":"Download","text":"<pre><code>autorag-research data restore beir scifact_openai-small\n</code></pre>"},{"location":"datasets/text/beir/#ingest-from-source","title":"Ingest from Source","text":"<pre><code>autorag-research ingest --name=beir --extra dataset-name=scifact --embedding-model=openai-small\n</code></pre>"},{"location":"datasets/text/beir/#best-for","title":"Best For","text":"<ul> <li>Text retrieval benchmarking</li> <li>Zero-shot evaluation</li> <li>Sparse vs dense comparison</li> </ul>"},{"location":"datasets/text/bright/","title":"BRIGHT","text":"<p>Reasoning-intensive retrieval benchmark.</p>"},{"location":"datasets/text/bright/#overview","title":"Overview","text":"Field Value Modality Text Generation GT No HF Repository <code>bright-dumps</code>"},{"location":"datasets/text/bright/#description","title":"Description","text":"<p>BRIGHT (Benchmarking Retrieval for Generative and Reasoning-Intensive Transformations) focuses on queries that require reasoning beyond simple keyword matching.</p>"},{"location":"datasets/text/bright/#download","title":"Download","text":"<pre><code>autorag-research data restore bright &lt;dataset_name&gt;_&lt;embedding_model&gt;\n</code></pre>"},{"location":"datasets/text/bright/#ingest-from-source","title":"Ingest from Source","text":"<pre><code>autorag-research ingest --name=bright --extra dataset-name=&lt;name&gt; --embedding-model=openai-small\n</code></pre>"},{"location":"datasets/text/bright/#best-for","title":"Best For","text":"<ul> <li>Complex query understanding</li> <li>Reasoning-based retrieval</li> <li>Advanced semantic matching</li> </ul>"},{"location":"datasets/text/mrtydi/","title":"MrTyDi","text":"<p>Multilingual retrieval benchmark.</p>"},{"location":"datasets/text/mrtydi/#overview","title":"Overview","text":"Field Value Modality Text Generation GT No HF Repository <code>mrtydi-dumps</code> Paper Zhang et al., 2021"},{"location":"datasets/text/mrtydi/#description","title":"Description","text":"<p>Mr. TyDi is a multilingual benchmark for retrieval, covering multiple languages with native speakers providing queries and relevance judgments.</p>"},{"location":"datasets/text/mrtydi/#languages","title":"Languages","text":"<ul> <li>Arabic</li> <li>Bengali</li> <li>English</li> <li>Finnish</li> <li>Indonesian</li> <li>Japanese</li> <li>Korean</li> <li>Russian</li> <li>Swahili</li> <li>Telugu</li> <li>Thai</li> </ul>"},{"location":"datasets/text/mrtydi/#download","title":"Download","text":"<pre><code>autorag-research data restore mrtydi &lt;language&gt;_&lt;embedding_model&gt;\n</code></pre>"},{"location":"datasets/text/mrtydi/#ingest-from-source","title":"Ingest from Source","text":"<pre><code>autorag-research ingest --name=mrtydi --extra language=english --embedding-model=openai-small\n</code></pre>"},{"location":"datasets/text/mrtydi/#best-for","title":"Best For","text":"<ul> <li>Multilingual retrieval evaluation</li> <li>Cross-lingual transfer</li> <li>Non-English benchmarking</li> </ul>"},{"location":"datasets/text/mteb/","title":"MTEB","text":"<p>Massive Text Embedding Benchmark retrieval tasks.</p>"},{"location":"datasets/text/mteb/#overview","title":"Overview","text":"Field Value Modality Text Generation GT No HF Repository <code>mteb-dumps</code> Paper Muennighoff et al., 2023"},{"location":"datasets/text/mteb/#description","title":"Description","text":"<p>MTEB provides a comprehensive evaluation of text embedding models across multiple tasks. The retrieval subset focuses on document retrieval performance.</p>"},{"location":"datasets/text/mteb/#download","title":"Download","text":"<pre><code>autorag-research data restore mteb &lt;dataset_name&gt;_&lt;embedding_model&gt;\n</code></pre>"},{"location":"datasets/text/mteb/#ingest-from-source","title":"Ingest from Source","text":"<pre><code>autorag-research ingest --name=mteb --extra dataset-name=&lt;name&gt; --embedding-model=openai-small\n</code></pre>"},{"location":"datasets/text/mteb/#best-for","title":"Best For","text":"<ul> <li>Embedding model comparison</li> <li>General text retrieval</li> <li>Dense retrieval evaluation</li> </ul>"},{"location":"datasets/text/ragbench/","title":"RAGBench","text":"<p>RAG evaluation benchmark with generation ground truth.</p>"},{"location":"datasets/text/ragbench/#overview","title":"Overview","text":"Field Value Modality Text Generation GT Yes HF Repository <code>ragbench-dumps</code>"},{"location":"datasets/text/ragbench/#description","title":"Description","text":"<p>RAGBench provides datasets specifically designed for evaluating full RAG pipelines, including both retrieval and generation components. Unlike retrieval-only benchmarks, it includes expected answers for generation evaluation.</p>"},{"location":"datasets/text/ragbench/#sub-datasets","title":"Sub-datasets","text":"Name Domain covidqa COVID-19 Q&amp;A pubmedqa Biomedical techqa Technical"},{"location":"datasets/text/ragbench/#download","title":"Download","text":"<pre><code>autorag-research data restore ragbench covidqa_openai-small\n</code></pre>"},{"location":"datasets/text/ragbench/#ingest-from-source","title":"Ingest from Source","text":"<pre><code>autorag-research ingest --name=ragbench --extra config=covidqa --embedding-model=openai-small\n</code></pre>"},{"location":"datasets/text/ragbench/#best-for","title":"Best For","text":"<ul> <li>Full RAG pipeline evaluation</li> <li>Generation quality assessment</li> <li>End-to-end benchmarking</li> </ul>"},{"location":"embeddings/","title":"Embeddings","text":"<p>Multi-vector embedding models for late interaction retrieval (MaxSim).</p>"},{"location":"embeddings/#overview","title":"Overview","text":"<p>Unlike single-vector embeddings that produce one vector per input, multi-vector models produce one vector per token/patch. This enables late interaction retrieval where query-document similarity is computed as a sum of maximum similarities between individual token vectors (MaxSim scoring).</p>"},{"location":"embeddings/#available-embeddings","title":"Available Embeddings","text":"Embedding Type Modality GPU Required Infinity API Text + Image No (server-side) ColPali Local Text + Image Yes BiPali Local Text + Image Yes"},{"location":"embeddings/#base-classes","title":"Base Classes","text":"<p>All multi-vector embeddings extend from the base classes in <code>autorag_research.embeddings.base</code>:</p> <pre><code>from autorag_research.embeddings.base import (\n    MultiVectorBaseEmbedding,       # Text-only multi-vector\n    MultiVectorMultiModalEmbedding, # Text + Image multi-vector\n)\n</code></pre>"},{"location":"embeddings/#methods","title":"Methods","text":""},{"location":"embeddings/#text-embedding","title":"Text Embedding","text":"Method Description <code>embed_text(text)</code> Embed a single text <code>aembed_text(text)</code> Async embed a single text <code>embed_query(query)</code> Embed a single query <code>aembed_query(query)</code> Async embed a single query <code>embed_documents(texts)</code> Embed multiple texts <code>aembed_documents(texts)</code> Async embed multiple texts <code>embed_documents_batch(texts)</code> Embed with automatic batching <code>aembed_documents_batch(texts)</code> Async embed with automatic batching"},{"location":"embeddings/#image-embedding-multimodal-only","title":"Image Embedding (MultiModal only)","text":"Method Description <code>embed_image(img)</code> Embed a single image <code>aembed_image(img)</code> Async embed a single image <code>embed_images(imgs)</code> Embed multiple images <code>aembed_images(imgs)</code> Async embed multiple images <code>embed_images_batch(imgs)</code> Embed with automatic batching <code>aembed_images_batch(imgs)</code> Async embed with automatic batching"},{"location":"embeddings/#image-input-types","title":"Image Input Types","text":"<p>Image methods accept any of the following types:</p> <pre><code>from autorag_research.types import ImageType\n# ImageType = str | bytes | Path | BytesIO\n</code></pre> <ul> <li><code>str</code> -- file path as string</li> <li><code>Path</code> -- <code>pathlib.Path</code> object</li> <li><code>bytes</code> -- raw image bytes</li> <li><code>BytesIO</code> -- in-memory file-like object</li> </ul>"},{"location":"embeddings/infinity/","title":"Infinity","text":"<p>Multi-vector embeddings via an Infinity Embedding API server.</p>"},{"location":"embeddings/infinity/#overview","title":"Overview","text":"Field Value Type API Modality Text + Image Provider Infinity (self-hosted) Default Model <code>michaelfeil/colqwen2-v0.1</code> Env Variable <code>INFINITY_API_URL</code> GPU Required No (client-side) <p>Connects to a running Infinity server that serves ColPali/ColQwen2 models. The server handles GPU inference; this client only needs HTTP access. Produces multi-vector embeddings (one vector per token/patch) for MaxSim late interaction retrieval.</p> <p>Uses the official <code>infinity-client</code> package (<code>InfinityVisionAPI</code>) which handles HTTP session management, retry with exponential backoff, base64 decoding, numpy array reshaping, and semaphore-based concurrency control internally.</p>"},{"location":"embeddings/infinity/#prerequisites","title":"Prerequisites","text":"<p>You need a running Infinity server. Start one with Docker:</p> <pre><code>docker run -it --gpus all \\\n  -v $HOME/.cache/huggingface:/root/.cache/huggingface \\\n  -p 7997:7997 \\\n  michaelf34/infinity:latest \\\n  v2 \\\n  --model-id michaelfeil/colqwen2-v0.1 \\\n  --port 7997\n</code></pre>"},{"location":"embeddings/infinity/#configuration","title":"Configuration","text":"<pre><code>_target_: autorag_research.embeddings.infinity.InfinityEmbeddings\nmodel_name: michaelfeil/colqwen2-v0.1\nurl: ${oc.env:INFINITY_API_URL,http://localhost:7997}\nencoding: base64\nembed_batch_size: 10\n</code></pre>"},{"location":"embeddings/infinity/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>url</code> str <code>http://localhost:7997</code> Infinity API server URL <code>model_name</code> str <code>michaelfeil/colqwen2-v0.1</code> Model name served by Infinity <code>encoding</code> str <code>base64</code> Response encoding: <code>base64</code> or <code>float</code> <code>embed_batch_size</code> int <code>10</code> Batch size for batch embedding methods"},{"location":"embeddings/infinity/#supported-models","title":"Supported Models","text":"<p>Any model supported by the Infinity server can be used. Common ColEncoder models:</p> Model Type Description <code>michaelfeil/colqwen2-v0.1</code> ColQwen2 Multi-modal (text + image), 128-dim <code>vidore/colpali-v1.3</code> ColPali Multi-modal (text + image), 128-dim <code>colbert-ir/colbertv2.0</code> ColBERT Text-only, 128-dim"},{"location":"learn/","title":"Learn","text":"<p>Read this section before starting the tutorial to understand core concepts.</p>"},{"location":"learn/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>Docker (for PostgreSQL)</li> <li>Command line familiarity</li> </ul>"},{"location":"learn/#learning-path","title":"Learning Path","text":"<ol> <li>Core Concepts - Understand datasets, pipelines, metrics</li> <li>Architecture - System design and data flow</li> <li>Tutorial - Hands-on step-by-step guides</li> </ol>"},{"location":"learn/#quick-reference","title":"Quick Reference","text":"<ul> <li>CLI Reference - All commands</li> <li>API Reference - Python API</li> </ul>"},{"location":"learn/architecture/","title":"Architecture","text":""},{"location":"learn/architecture/#system-overview","title":"System Overview","text":"<pre><code>graph TD\n    A[Dataset Source] --&gt; B[Ingestor]\n    B --&gt; C[PostgreSQL]\n    C --&gt; D[Retrieval Pipeline]\n    C --&gt; E[Generation Pipeline]\n    D --&gt; F[Metrics]\n    E --&gt; F\n    F --&gt; G[Results]</code></pre>"},{"location":"learn/architecture/#data-flow","title":"Data Flow","text":"<ol> <li>Ingest: Load dataset into PostgreSQL</li> <li>Embed: Generate vector embeddings</li> <li>Execute: Run pipelines on all queries</li> <li>Evaluate: Calculate metrics</li> <li>Store: Save results to database</li> </ol>"},{"location":"learn/architecture/#layered-architecture","title":"Layered Architecture","text":"<pre><code>Executor/Evaluator (config.py, executor.py, evaluator.py)\n    |\nPipeline Layer (pipelines/)\n    |\nService Layer (orm/service/) - Business logic\n    |\nUnit of Work (orm/uow/) - Transaction management\n    |\nRepository Layer (orm/repository/) - Data access\n    |\nORM Models (orm/models/) - SQLAlchemy with pgvector\n</code></pre>"},{"location":"learn/architecture/#extension-points","title":"Extension Points","text":"Extend Base Class Methods Retrieval <code>BaseRetrievalPipeline</code> <code>_get_retrieval_func()</code> Generation <code>BaseGenerationPipeline</code> <code>_generate()</code> Metric <code>BaseMetricConfig</code> <code>get_metric_func()</code> Dataset <code>DataIngestor</code> <code>ingest()</code>"},{"location":"learn/architecture/#key-components","title":"Key Components","text":""},{"location":"learn/architecture/#postgresql-with-vector-extensions","title":"PostgreSQL with Vector Extensions","text":"<ul> <li>pgvector: Vector similarity search</li> <li>VectorChord-BM25: Full-text BM25 retrieval</li> <li>Supports both dense (embedding) and sparse (keyword) retrieval</li> </ul>"},{"location":"learn/architecture/#configuration-system","title":"Configuration System","text":"<ul> <li>YAML-based experiment configuration</li> <li>Hydra for config composition</li> <li>Dataclass-based pipeline and metric configs</li> </ul>"},{"location":"learn/concepts/","title":"Core Concepts","text":""},{"location":"learn/concepts/#dataset","title":"Dataset","text":"<p>A dataset contains:</p> <ul> <li>Documents: Content to search (text or images)</li> <li>Queries: Questions to answer</li> <li>Ground Truth: Which documents are relevant to each query</li> </ul> <p>Stored in PostgreSQL with vector embeddings.</p>"},{"location":"learn/concepts/#retrieval-pipeline","title":"Retrieval Pipeline","text":"<p>Takes a query, returns relevant documents.</p> <pre><code>results = retrieval_pipeline.retrieve(query=\"What causes fever?\", top_k=10)\n# Returns: [{\"doc_id\": 42, \"score\": 0.95}, ...]\n</code></pre>"},{"location":"learn/concepts/#generation-pipeline","title":"Generation Pipeline","text":"<p>Takes a query + retrieved documents, generates an answer.</p> <pre><code>answer = generation_pipeline.generate(query=\"What causes fever?\", top_k=5)\n# Returns: \"Fever is caused by...\"\n</code></pre>"},{"location":"learn/concepts/#metric","title":"Metric","text":"<p>Measures pipeline quality against ground truth.</p> <p>Retrieval metrics: Did we find the right documents? (Recall, NDCG, MRR)</p> <p>Generation metrics: Is the answer correct? (ROUGE, BERTScore)</p>"},{"location":"learn/concepts/#executor","title":"Executor","text":"<p>Orchestrates pipeline execution and metric evaluation.</p> <pre><code># experiment.yaml\ndb_name: beir_scifact_test\n\npipelines:\n  retrieval: [bm25]\n\nmetrics:\n  retrieval: [recall, ndcg]\n</code></pre>"},{"location":"metrics/","title":"Metrics","text":"<p>Evaluation measures for retrieval and generation.</p>"},{"location":"metrics/#retrieval-metrics","title":"Retrieval Metrics","text":"Metric Measures Range Recall@k Ground truth coverage [0, 1] Precision@k Retrieved relevance [0, 1] F1@k Recall + Precision balance [0, 1] NDCG@k Ranking quality [0, 1] MRR First relevant position [0, 1] MAP Average precision [0, 1]"},{"location":"metrics/#generation-metrics","title":"Generation Metrics","text":"Metric Measures Range BLEU N-gram overlap [0, 1] METEOR Alignment [0, 1] ROUGE Recall overlap [0, 1] BERTScore Semantic similarity [-1, 1] SemScore Embedding similarity [-1, 1]"},{"location":"metrics/#choosing-metrics","title":"Choosing Metrics","text":"Goal Metrics Find all relevant docs Recall Rank correctly NDCG Find relevant quickly MRR Text similarity ROUGE, BLEU Semantic correctness BERTScore"},{"location":"metrics/#browse","title":"Browse","text":"<ul> <li>Retrieval Metrics</li> <li>Generation Metrics</li> </ul>"},{"location":"metrics/generation/","title":"Generation Metrics","text":"<p>Metrics for evaluating text generation quality.</p>"},{"location":"metrics/generation/#available-metrics","title":"Available Metrics","text":"Metric Measures When to Use BLEU N-gram precision Translation-style tasks METEOR Alignment Better for paraphrases ROUGE N-gram recall Summarization BERTScore Semantic similarity Meaning preservation SemScore Embedding similarity Semantic correctness"},{"location":"metrics/generation/#base-class","title":"Base Class","text":"<pre><code>from autorag_research.evaluation.metrics import BaseGenerationMetricConfig\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass MyMetricConfig(BaseGenerationMetricConfig):\n    def get_metric_func(self):\n        return my_metric_function\n</code></pre>"},{"location":"metrics/generation/bert-score/","title":"BERTScore","text":"<p>Semantic similarity using BERT embeddings.</p>"},{"location":"metrics/generation/bert-score/#overview","title":"Overview","text":"Field Value Type Generation Range [-1, 1] Higher is better Yes"},{"location":"metrics/generation/bert-score/#description","title":"Description","text":"<p>BERTScore computes token-level similarity using contextual embeddings from BERT. Captures semantic similarity beyond exact word matches.</p>"},{"location":"metrics/generation/bert-score/#configuration","title":"Configuration","text":"<pre><code>_target_: autorag_research.evaluation.metrics.generation.BertScoreConfig\nlang: en\nbatch: 64\nn_threads: 4\n</code></pre>"},{"location":"metrics/generation/bert-score/#options","title":"Options","text":"Option Type Default Description lang str <code>en</code> Language code batch int 64 Batch size n_threads int 4 Number of threads"},{"location":"metrics/generation/bert-score/#when-to-use","title":"When to Use","text":"<p>Good for:</p> <ul> <li>Semantic similarity assessment</li> <li>Paraphrase detection</li> <li>Meaning preservation evaluation</li> </ul> <p>Limitations:</p> <ul> <li>Computationally expensive</li> <li>Requires BERT model</li> <li>May not capture factual correctness</li> </ul>"},{"location":"metrics/generation/bleu/","title":"BLEU","text":"<p>Bilingual Evaluation Understudy - measures n-gram precision.</p>"},{"location":"metrics/generation/bleu/#overview","title":"Overview","text":"Field Value Type Generation Range [0, 1] Higher is better Yes"},{"location":"metrics/generation/bleu/#description","title":"Description","text":"<p>BLEU measures how many n-grams in the generated text appear in the reference text. Originally designed for machine translation evaluation.</p>"},{"location":"metrics/generation/bleu/#configuration","title":"Configuration","text":"<pre><code>_target_: autorag_research.evaluation.metrics.generation.BleuConfig\ntokenize: default\nsmooth_method: exp\nmax_ngram_order: 4\neffective_order: true\n</code></pre>"},{"location":"metrics/generation/bleu/#options","title":"Options","text":"Option Type Default Description tokenize str <code>default</code> Tokenization method smooth_method str <code>exp</code> Smoothing for zero counts max_ngram_order int 4 Maximum n-gram size effective_order bool true Use effective order"},{"location":"metrics/generation/bleu/#when-to-use","title":"When to Use","text":"<p>Good for:</p> <ul> <li>Translation-style tasks</li> <li>Tasks requiring exact phrase matching</li> <li>Comparing against single reference</li> </ul> <p>Limitations:</p> <ul> <li>Doesn't capture semantic similarity</li> <li>Penalizes paraphrases</li> <li>Requires exact n-gram matches</li> </ul>"},{"location":"metrics/generation/meteor/","title":"METEOR","text":"<p>Metric for Evaluation of Translation with Explicit ORdering.</p>"},{"location":"metrics/generation/meteor/#overview","title":"Overview","text":"Field Value Type Generation Range [0, 1] Higher is better Yes"},{"location":"metrics/generation/meteor/#description","title":"Description","text":"<p>METEOR aligns generated text with reference using exact matches, stems, synonyms, and paraphrases. More flexible than BLEU for capturing semantic similarity.</p>"},{"location":"metrics/generation/meteor/#configuration","title":"Configuration","text":"<pre><code>_target_: autorag_research.evaluation.metrics.generation.MeteorConfig\nalpha: 0.9\nbeta: 3.0\ngamma: 0.5\n</code></pre>"},{"location":"metrics/generation/meteor/#options","title":"Options","text":"Option Type Default Description alpha float 0.9 Recall weight beta float 3.0 Fragmentation penalty gamma float 0.5 Fragmentation weight"},{"location":"metrics/generation/meteor/#when-to-use","title":"When to Use","text":"<p>Good for:</p> <ul> <li>Tasks allowing paraphrasing</li> <li>When synonyms should be rewarded</li> <li>More forgiving evaluation than BLEU</li> </ul> <p>Limitations:</p> <ul> <li>Language-specific resources needed</li> <li>Slower than BLEU</li> </ul>"},{"location":"metrics/generation/rouge/","title":"ROUGE","text":"<p>Recall-Oriented Understudy for Gisting Evaluation.</p>"},{"location":"metrics/generation/rouge/#overview","title":"Overview","text":"Field Value Type Generation Range [0, 1] Higher is better Yes"},{"location":"metrics/generation/rouge/#description","title":"Description","text":"<p>ROUGE measures n-gram recall - how many reference n-grams appear in the generated text. Originally designed for summarization evaluation.</p>"},{"location":"metrics/generation/rouge/#configuration","title":"Configuration","text":"<pre><code>_target_: autorag_research.evaluation.metrics.generation.RougeConfig\nrouge_type: rougeL\nuse_stemmer: true\n</code></pre>"},{"location":"metrics/generation/rouge/#options","title":"Options","text":"Option Type Default Description rouge_type str <code>rougeL</code> ROUGE variant use_stemmer bool true Apply stemming"},{"location":"metrics/generation/rouge/#rouge-variants","title":"ROUGE Variants","text":"Variant Description rouge1 Unigram overlap rouge2 Bigram overlap rougeL Longest common subsequence rougeLSum LCS over sentences"},{"location":"metrics/generation/rouge/#when-to-use","title":"When to Use","text":"<p>Good for:</p> <ul> <li>Summarization tasks</li> <li>Content coverage evaluation</li> <li>Recall-focused assessment</li> </ul> <p>Limitations:</p> <ul> <li>Doesn't capture semantic similarity</li> <li>Position-insensitive (except rougeL)</li> </ul>"},{"location":"metrics/generation/sem-score/","title":"SemScore","text":"<p>Semantic similarity using embedding models.</p>"},{"location":"metrics/generation/sem-score/#overview","title":"Overview","text":"Field Value Type Generation Range [-1, 1] Higher is better Yes"},{"location":"metrics/generation/sem-score/#description","title":"Description","text":"<p>SemScore computes cosine similarity between sentence embeddings of generated and reference text. Uses configurable embedding models.</p>"},{"location":"metrics/generation/sem-score/#configuration","title":"Configuration","text":"<pre><code>_target_: autorag_research.evaluation.metrics.generation.SemScoreConfig\nembedding_model: openai-small\ntruncate_length: 8192\n</code></pre>"},{"location":"metrics/generation/sem-score/#options","title":"Options","text":"Option Type Default Description embedding_model str required Embedding model config name truncate_length int 8192 Max text length"},{"location":"metrics/generation/sem-score/#when-to-use","title":"When to Use","text":"<p>Good for:</p> <ul> <li>Semantic correctness evaluation</li> <li>When using same embeddings as retrieval</li> <li>Fast semantic comparison</li> </ul> <p>Limitations:</p> <ul> <li>Depends on embedding model quality</li> <li>May miss fine-grained differences</li> </ul>"},{"location":"metrics/retrieval/","title":"Retrieval Metrics","text":"<p>Metrics for evaluating document retrieval quality.</p>"},{"location":"metrics/retrieval/#available-metrics","title":"Available Metrics","text":"Metric Measures When to Use Recall@k Coverage Ensure all relevant docs found Full Recall@k Complete coverage All evidence groups must be retrieved Precision@k Relevance Minimize irrelevant results F1@k Balance Trade-off recall and precision NDCG@k Ranking Order matters MRR First hit Single answer tasks MAP Overall quality Comprehensive evaluation"},{"location":"metrics/retrieval/#common-parameters","title":"Common Parameters","text":"<p>All retrieval metrics use the top-k retrieved results compared against ground truth relevance judgments.</p>"},{"location":"metrics/retrieval/#base-class","title":"Base Class","text":"<pre><code>from autorag_research.evaluation.metrics import BaseRetrievalMetricConfig\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass MyMetricConfig(BaseRetrievalMetricConfig):\n    def get_metric_func(self):\n        return my_metric_function\n</code></pre>"},{"location":"metrics/retrieval/f1/","title":"F1","text":"<p>Harmonic mean of precision and recall.</p>"},{"location":"metrics/retrieval/f1/#overview","title":"Overview","text":"Field Value Type Retrieval Range [0, 1] Higher is better Yes"},{"location":"metrics/retrieval/f1/#formula","title":"Formula","text":"\\[F1@k = 2 \\cdot \\frac{Precision@k \\cdot Recall@k}{Precision@k + Recall@k}\\]"},{"location":"metrics/retrieval/f1/#interpretation","title":"Interpretation","text":"Score Meaning 1.0 Perfect precision and recall 0.8 Good balance 0.5 Moderate performance 0.0 Complete failure"},{"location":"metrics/retrieval/f1/#when-to-use","title":"When to Use","text":"<p>Use when you need a single metric that balances both precision and recall.</p>"},{"location":"metrics/retrieval/f1/#configuration","title":"Configuration","text":"<pre><code>_target_: autorag_research.evaluation.metrics.retrieval.F1Config\n</code></pre>"},{"location":"metrics/retrieval/full-recall/","title":"Full Recall","text":"<p>Measures whether all relevant document groups were retrieved (all-or-nothing).</p>"},{"location":"metrics/retrieval/full-recall/#overview","title":"Overview","text":"Field Value Type Retrieval Range {0, 1} Higher is better Yes"},{"location":"metrics/retrieval/full-recall/#formula","title":"Formula","text":"\\[FullRecall@k = \\begin{cases} 1.0 &amp; \\text{if every GT group has at least one hit in Retrieved@k} \\\\ 0.0 &amp; \\text{otherwise} \\end{cases}\\] <p>The final score is the average of Full Recall across all queries.</p>"},{"location":"metrics/retrieval/full-recall/#interpretation","title":"Interpretation","text":"Score Meaning 1.0 All evidence groups retrieved 0.0 At least one evidence group missing <p>Unlike standard Recall which measures the fraction of ground truth groups found, Full Recall is a strict binary measure -- a query scores 1.0 only when every group is satisfied.</p>"},{"location":"metrics/retrieval/full-recall/#when-to-use","title":"When to Use","text":"<p>Use when complete evidence coverage is required, such as:</p> <ul> <li>Multi-hop question answering (all supporting facts must be retrieved)</li> <li>Fact verification requiring multiple evidence pieces</li> <li>Measuring the \"success rate\" of queries with full coverage</li> </ul>"},{"location":"metrics/retrieval/full-recall/#configuration","title":"Configuration","text":"<pre><code>_target_: autorag_research.evaluation.metrics.retrieval.FullRecallConfig\n</code></pre>"},{"location":"metrics/retrieval/map/","title":"MAP","text":"<p>Mean Average Precision - comprehensive ranking metric.</p>"},{"location":"metrics/retrieval/map/#overview","title":"Overview","text":"Field Value Type Retrieval Range [0, 1] Higher is better Yes"},{"location":"metrics/retrieval/map/#formula","title":"Formula","text":"\\[AP = \\frac{1}{|Relevant|} \\sum_{k=1}^{n} P(k) \\cdot rel(k)\\] \\[MAP = \\frac{1}{|Q|} \\sum_{q=1}^{|Q|} AP_q\\] <p>Where \\(P(k)\\) is precision at position \\(k\\) and \\(rel(k)\\) indicates if result \\(k\\) is relevant.</p>"},{"location":"metrics/retrieval/map/#interpretation","title":"Interpretation","text":"Score Meaning 1.0 All relevant docs at top positions 0.8 Good overall ranking 0.5 Moderate ranking quality 0.0 No relevant docs found"},{"location":"metrics/retrieval/map/#when-to-use","title":"When to Use","text":"<p>Use for comprehensive evaluation of ranking quality across all queries:</p> <ul> <li>Benchmark comparisons</li> <li>System-level evaluation</li> <li>Research reporting</li> </ul>"},{"location":"metrics/retrieval/map/#configuration","title":"Configuration","text":"<pre><code>_target_: autorag_research.evaluation.metrics.retrieval.MAPConfig\n</code></pre>"},{"location":"metrics/retrieval/mrr/","title":"MRR","text":"<p>Mean Reciprocal Rank - measures position of first relevant result.</p>"},{"location":"metrics/retrieval/mrr/#overview","title":"Overview","text":"Field Value Type Retrieval Range [0, 1] Higher is better Yes"},{"location":"metrics/retrieval/mrr/#formula","title":"Formula","text":"\\[MRR = \\frac{1}{|Q|} \\sum_{i=1}^{|Q|} \\frac{1}{rank_i}\\] <p>Where \\(rank_i\\) is the position of the first relevant document for query \\(i\\).</p>"},{"location":"metrics/retrieval/mrr/#interpretation","title":"Interpretation","text":"Score Meaning 1.0 First result always relevant 0.5 First relevant at position 2 on average 0.33 First relevant at position 3 on average 0.0 No relevant docs in results"},{"location":"metrics/retrieval/mrr/#when-to-use","title":"When to Use","text":"<p>Use when users typically only care about the first relevant result:</p> <ul> <li>Q&amp;A systems</li> <li>Fact lookup</li> <li>Single-answer queries</li> </ul>"},{"location":"metrics/retrieval/mrr/#configuration","title":"Configuration","text":"<pre><code>_target_: autorag_research.evaluation.metrics.retrieval.MRRConfig\n</code></pre>"},{"location":"metrics/retrieval/ndcg/","title":"NDCG","text":"<p>Normalized Discounted Cumulative Gain - measures ranking quality.</p>"},{"location":"metrics/retrieval/ndcg/#overview","title":"Overview","text":"Field Value Type Retrieval Range [0, 1] Higher is better Yes"},{"location":"metrics/retrieval/ndcg/#formula","title":"Formula","text":"\\[DCG@k = \\sum_{i=1}^{k} \\frac{rel_i}{\\log_2(i+1)}\\] \\[NDCG@k = \\frac{DCG@k}{IDCG@k}\\] <p>Where IDCG is the ideal DCG (perfect ranking).</p>"},{"location":"metrics/retrieval/ndcg/#interpretation","title":"Interpretation","text":"Score Meaning 1.0 Perfect ranking 0.8 Good ranking 0.5 Moderate ranking 0.0 No relevant docs"},{"location":"metrics/retrieval/ndcg/#when-to-use","title":"When to Use","text":"<p>Use when the order of results matters, not just their presence:</p> <ul> <li>Search result ranking</li> <li>Recommendation systems</li> <li>Any task where position affects user experience</li> </ul>"},{"location":"metrics/retrieval/ndcg/#configuration","title":"Configuration","text":"<pre><code>_target_: autorag_research.evaluation.metrics.retrieval.NDCGConfig\n</code></pre>"},{"location":"metrics/retrieval/ndcg/#reference","title":"Reference","text":"<p>Jarvelin &amp; Kekalainen, 2002</p>"},{"location":"metrics/retrieval/precision/","title":"Precision","text":"<p>Measures how many retrieved documents were relevant.</p>"},{"location":"metrics/retrieval/precision/#overview","title":"Overview","text":"Field Value Type Retrieval Range [0, 1] Higher is better Yes"},{"location":"metrics/retrieval/precision/#formula","title":"Formula","text":"\\[Precision@k = \\frac{|Retrieved@k \\cap Relevant|}{k}\\]"},{"location":"metrics/retrieval/precision/#interpretation","title":"Interpretation","text":"Score Meaning 1.0 All retrieved docs relevant 0.5 Half of retrieved docs relevant 0.1 1 in 10 retrieved docs relevant 0.0 No retrieved docs relevant"},{"location":"metrics/retrieval/precision/#when-to-use","title":"When to Use","text":"<p>Use when minimizing irrelevant results is important, such as:</p> <ul> <li>User-facing search interfaces</li> <li>Limited display space</li> <li>High-precision applications</li> </ul>"},{"location":"metrics/retrieval/precision/#configuration","title":"Configuration","text":"<pre><code>_target_: autorag_research.evaluation.metrics.retrieval.PrecisionConfig\n</code></pre>"},{"location":"metrics/retrieval/recall/","title":"Recall","text":"<p>Measures how many relevant documents were retrieved.</p>"},{"location":"metrics/retrieval/recall/#overview","title":"Overview","text":"Field Value Type Retrieval Range [0, 1] Higher is better Yes"},{"location":"metrics/retrieval/recall/#formula","title":"Formula","text":"\\[Recall@k = \\frac{|Retrieved@k \\cap Relevant|}{|Relevant|}\\]"},{"location":"metrics/retrieval/recall/#interpretation","title":"Interpretation","text":"Score Meaning 1.0 All relevant docs retrieved 0.8 80% of relevant docs found 0.5 Half of relevant docs found 0.0 No relevant docs retrieved"},{"location":"metrics/retrieval/recall/#when-to-use","title":"When to Use","text":"<p>Use when finding all relevant documents is critical, such as:</p> <ul> <li>Legal document search</li> <li>Medical information retrieval</li> <li>Comprehensive research</li> </ul>"},{"location":"metrics/retrieval/recall/#configuration","title":"Configuration","text":"<pre><code>_target_: autorag_research.evaluation.metrics.retrieval.RecallConfig\n</code></pre>"},{"location":"pipelines/","title":"Pipelines","text":"<p>Algorithms for retrieval and generation.</p>"},{"location":"pipelines/#comparison","title":"Comparison","text":"Pipeline Type Algorithm Modality BM25 Retrieval Sparse Text BasicRAG Generation Retrieve+Generate Text"},{"location":"pipelines/#types","title":"Types","text":"<p>Retrieval: Query -&gt; Relevant documents</p> <p>Generation: Query + Documents -&gt; Answer</p>"},{"location":"pipelines/#browse","title":"Browse","text":"<ul> <li>Retrieval Pipelines</li> <li>Generation Pipelines</li> </ul>"},{"location":"pipelines/generation/","title":"Generation Pipelines","text":"<p>Algorithms that take a query and retrieved documents to generate an answer.</p>"},{"location":"pipelines/generation/#available-pipelines","title":"Available Pipelines","text":"Pipeline Algorithm BasicRAG Single retrieve + generate IRCoT Iterative retrieve + chain-of-thought reasoning"},{"location":"pipelines/generation/#base-class","title":"Base Class","text":"<p>All generation pipelines extend <code>BaseGenerationPipeline</code>:</p> <pre><code>from autorag_research.pipelines.generation import (\n    BaseGenerationPipeline,\n    GenerationResult,\n)\n\n\nclass MyRAGPipeline(BaseGenerationPipeline):\n    def _generate(self, query: str, top_k: int) -&gt; GenerationResult:\n        # 1. Retrieve documents\n        retrieved = self._retrieval_pipeline.retrieve(query, top_k)\n\n        # 2. Build context and generate\n        answer = self._llm.complete(...)\n\n        return GenerationResult(\n            text=answer.text,\n            token_usage={\"prompt\": 100, \"completion\": 50},\n            metadata={},\n        )\n\n    def _get_pipeline_config(self):\n        return {\"type\": \"my_rag\"}\n</code></pre>"},{"location":"pipelines/generation/#composition","title":"Composition","text":"<p>Generation pipelines compose with retrieval pipelines:</p> <pre><code>_target_: autorag_research.pipelines.generation.basic_rag.BasicRAGPipelineConfig\nname: my_rag\nretrieval_pipeline_name: bm25  # References a retrieval pipeline\nllm: gpt-4o-mini\n</code></pre>"},{"location":"pipelines/generation/#methods","title":"Methods","text":"Method Description <code>run(top_k, batch_size)</code> Batch generation for all queries"},{"location":"pipelines/generation/basic-rag/","title":"BasicRAG","text":"<p>Simple single-call RAG: retrieve once, build prompt, generate once.</p>"},{"location":"pipelines/generation/basic-rag/#overview","title":"Overview","text":"Field Value Type Generation Algorithm Retrieve + Generate Modality Text"},{"location":"pipelines/generation/basic-rag/#how-it-works","title":"How It Works","text":"<ol> <li>Retrieve top-k documents using configured retrieval pipeline</li> <li>Build context from retrieved documents</li> <li>Generate answer using LLM with prompt template</li> </ol>"},{"location":"pipelines/generation/basic-rag/#configuration","title":"Configuration","text":"<pre><code>_target_: autorag_research.pipelines.generation.basic_rag.BasicRAGPipelineConfig\nname: basic_rag\nretrieval_pipeline_name: bm25\nllm: gpt-4o-mini\nprompt_template: |\n  Context:\n  {context}\n\n  Question: {query}\n\n  Answer:\ntop_k: 5\nbatch_size: 100\n</code></pre>"},{"location":"pipelines/generation/basic-rag/#options","title":"Options","text":"Option Type Default Description name str required Unique pipeline instance name retrieval_pipeline_name str required Name of retrieval pipeline to use llm str or BaseLLM required LLM instance or config name prompt_template str default Template with <code>{context}</code> and <code>{query}</code> top_k int 10 Documents to retrieve batch_size int 100 Queries per batch"},{"location":"pipelines/generation/basic-rag/#prompt-template-variables","title":"Prompt Template Variables","text":"Variable Description <code>{context}</code> Retrieved document contents <code>{query}</code> Original query"},{"location":"pipelines/generation/basic-rag/#when-to-use","title":"When to Use","text":"<p>Good for:</p> <ul> <li>Simple Q&amp;A tasks</li> <li>Baseline RAG implementation</li> <li>Quick prototyping</li> </ul> <p>Consider advanced pipelines for:</p> <ul> <li>Multi-hop reasoning</li> <li>Iterative retrieval</li> <li>Complex answer synthesis</li> </ul>"},{"location":"pipelines/generation/basic-rag/#citation","title":"Citation","text":"<pre><code>@article{lewis2020retrieval,\n  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},\n  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\\\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\\\"a}schel, Tim and others},\n  journal={Advances in neural information processing systems},\n  volume={33},\n  pages={9459--9474},\n  year={2020}\n}\n</code></pre>"},{"location":"pipelines/generation/ircot/","title":"IRCoT","text":"<p>Interleaving Retrieval with Chain-of-Thought reasoning for multi-step question answering.</p>"},{"location":"pipelines/generation/ircot/#overview","title":"Overview","text":"Field Value Type Generation Algorithm Iterative Retrieve + Reason Modality Text Paper ACL 2023"},{"location":"pipelines/generation/ircot/#how-it-works","title":"How It Works","text":"<p>IRCoT alternates between reasoning and retrieval in an iterative loop:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  1. Initial Retrieval                               \u2502\n\u2502     Query \u2192 Retrieve k paragraphs                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  2. Iterative Loop (up to max_steps)                \u2502\n\u2502                                                     \u2502\n\u2502     a) Generate CoT sentence (reasoning step)       \u2502\n\u2502     b) Check: contains \"answer is:\"? \u2192 Exit         \u2502\n\u2502     c) Use CoT sentence as query \u2192 Retrieve more    \u2502\n\u2502     d) Add to paragraph collection (cap at budget)  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  3. Final QA Generation                             \u2502\n\u2502     All paragraphs + question \u2192 Final answer        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Key insight: Each reasoning step guides the next retrieval, and each retrieval informs subsequent reasoning, creating a symbiotic improvement cycle.</p>"},{"location":"pipelines/generation/ircot/#configuration","title":"Configuration","text":"<pre><code>_target_: autorag_research.pipelines.generation.ircot.IRCoTGenerationPipelineConfig\nname: ircot\nretrieval_pipeline_name: bm25\nllm: gpt-4o-mini\nk_per_step: 4\nmax_steps: 8\nparagraph_budget: 15\nstop_sequence: \"answer is:\"\ntop_k: 4\nbatch_size: 10\n</code></pre>"},{"location":"pipelines/generation/ircot/#options","title":"Options","text":"Option Type Default Description name str required Unique pipeline instance name retrieval_pipeline_name str required Name of retrieval pipeline (BM25 recommended) llm str or BaseLLM required LLM instance or config name k_per_step int 4 Paragraphs to retrieve per reasoning step max_steps int 8 Maximum reasoning-retrieval iterations paragraph_budget int 15 Maximum total paragraphs to collect stop_sequence str \"answer is:\" Termination string (case-insensitive) reasoning_prompt_template str default Template for reasoning steps qa_prompt_template str default Template for final QA top_k int 4 Alias for k_per_step batch_size int 10 Queries per batch"},{"location":"pipelines/generation/ircot/#prompt-template-variables","title":"Prompt Template Variables","text":""},{"location":"pipelines/generation/ircot/#reasoning-prompt","title":"Reasoning Prompt","text":"Variable Description <code>{query}</code> Original question <code>{paragraphs}</code> Retrieved paragraphs (numbered) <code>{cot_history}</code> Previous reasoning steps"},{"location":"pipelines/generation/ircot/#qa-prompt","title":"QA Prompt","text":"Variable Description <code>{query}</code> Original question <code>{paragraphs}</code> All collected paragraphs"},{"location":"pipelines/generation/ircot/#custom-prompt-templates","title":"Custom Prompt Templates","text":"<pre><code>reasoning_prompt_template: |\n  Question: {query}\n\n  Context:\n  {paragraphs}\n\n  Previous reasoning:\n  {cot_history}\n\n  Think step-by-step. Write \"The answer is: X\" when ready.\n\nqa_prompt_template: |\n  Question: {query}\n\n  Documents:\n  {paragraphs}\n\n  Answer concisely:\n</code></pre>"},{"location":"pipelines/generation/ircot/#algorithm-details","title":"Algorithm Details","text":""},{"location":"pipelines/generation/ircot/#termination-conditions","title":"Termination Conditions","text":"<p>The iterative loop terminates when:</p> <ol> <li>Answer detected: Generated CoT contains \"answer is:\" (case-insensitive)</li> <li>Max steps reached: Completed <code>max_steps</code> iterations</li> </ol>"},{"location":"pipelines/generation/ircot/#paragraph-budget","title":"Paragraph Budget","text":"<ul> <li>Paragraphs are capped at <code>paragraph_budget</code> using FIFO strategy</li> <li>Earlier paragraphs (from initial retrieval) are retained</li> <li>Prevents unbounded context growth</li> </ul>"},{"location":"pipelines/generation/ircot/#first-sentence-extraction","title":"First Sentence Extraction","text":"<p>Only the first sentence from each CoT generation is kept:</p> <ul> <li>Prevents runaway generation</li> <li>Keeps reasoning steps focused</li> <li>Matches original paper implementation</li> </ul>"},{"location":"pipelines/generation/ircot/#performance","title":"Performance","text":"<p>From the original paper (GPT-3 + BM25):</p> Dataset Retrieval Recall QA Accuracy HotpotQA +21 points vs OneR +15 points 2WikiMultihopQA +18 points +12 points MuSiQue +15 points +10 points"},{"location":"pipelines/generation/ircot/#when-to-use","title":"When to Use","text":"<p>Good for:</p> <ul> <li>Multi-hop reasoning questions</li> <li>Questions requiring information synthesis</li> <li>Complex factual queries</li> <li>Knowledge-intensive tasks</li> </ul> <p>Consider BasicRAG for:</p> <ul> <li>Simple single-fact questions</li> <li>Low-latency requirements (IRCoT makes multiple LLM calls)</li> <li>Cost-sensitive applications</li> </ul>"},{"location":"pipelines/generation/ircot/#example-usage","title":"Example Usage","text":""},{"location":"pipelines/generation/ircot/#python","title":"Python","text":"<pre><code>from langchain_openai import ChatOpenAI\n\nfrom autorag_research.orm.connection import DBConnection\nfrom autorag_research.pipelines.generation.ircot import IRCoTGenerationPipeline\nfrom autorag_research.pipelines.retrieval.bm25 import BM25RetrievalPipeline\n\ndb = DBConnection.from_config()\nsession_factory = db.get_session_factory()\n\n# Create retrieval pipeline\nretrieval = BM25RetrievalPipeline(\n    session_factory=session_factory,\n    name=\"bm25_retriever\",\n)\n\n# Create IRCoT pipeline\npipeline = IRCoTGenerationPipeline(\n    session_factory=session_factory,\n    name=\"ircot_gpt4\",\n    llm=ChatOpenAI(model=\"gpt-4o-mini\"),\n    retrieval_pipeline=retrieval,\n    k_per_step=4,\n    max_steps=8,\n    paragraph_budget=15,\n)\n\n# Run on all queries\nresults = pipeline.run(top_k=4)\nprint(f\"Processed {results['total_queries']} queries\")\n</code></pre>"},{"location":"pipelines/generation/ircot/#single-query","title":"Single Query","text":"<pre><code>result = pipeline._generate(\"What is the relationship between X and Y?\", top_k=4)\nprint(f\"Answer: {result.text}\")\nprint(f\"Steps taken: {result.metadata['steps']}\")\nprint(f\"CoT history: {result.metadata['cot_sentences']}\")\n</code></pre>"},{"location":"pipelines/generation/ircot/#output-metadata","title":"Output Metadata","text":"<p>The <code>GenerationResult.metadata</code> contains:</p> Field Type Description <code>cot_sentences</code> list[str] Chain-of-thought reasoning history <code>chunk_ids</code> list[int] IDs of all retrieved chunks <code>steps</code> int Number of reasoning steps completed"},{"location":"pipelines/generation/ircot/#references","title":"References","text":"<ul> <li>Paper: Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions</li> <li>arXiv: 2212.10509</li> <li>Code: StonyBrookNLP/ircot</li> </ul>"},{"location":"pipelines/generation/ircot/#citation","title":"Citation","text":"<pre><code>@inproceedings{trivedi2023interleaving,\n  title={Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions},\n  author={Trivedi, Harsh and Balasubramanian, Niranjan and Khot, Tushar and Sabharwal, Ashish},\n  booktitle={Proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers)},\n  pages={10014--10037},\n  year={2023}\n}\n</code></pre>"},{"location":"pipelines/retrieval/","title":"Retrieval Pipelines","text":"<p>Algorithms that take a query and return relevant documents.</p>"},{"location":"pipelines/retrieval/#available-pipelines","title":"Available Pipelines","text":"Pipeline Algorithm Modality BM25 Sparse (term frequency) Text Hybrid RRF / Convex Combination Text Vector Search Dense (vector similarity) Text HyDE Dense (hypothetical document embeddings) Text"},{"location":"pipelines/retrieval/#base-class","title":"Base Class","text":"<p>All retrieval pipelines extend <code>BaseRetrievalPipeline</code>:</p> <pre><code>from autorag_research.pipelines.retrieval import BaseRetrievalPipeline\n\n\nclass MyRetrievalPipeline(BaseRetrievalPipeline):\n    def _get_retrieval_func(self):\n        def retrieve(queries: list[str], top_k: int) -&gt; list[list[dict]]:\n            # Return list of results per query\n            # Each result: {\"doc_id\": ..., \"score\": ...}\n            pass\n\n        return retrieve\n\n    def _get_pipeline_config(self):\n        return {\"type\": \"my_pipeline\"}\n</code></pre>"},{"location":"pipelines/retrieval/#methods","title":"Methods","text":"Method Description <code>retrieve(query, top_k)</code> Single query retrieval <code>run(top_k, batch_size)</code> Batch retrieval for all queries"},{"location":"pipelines/retrieval/bm25/","title":"BM25","text":"<p>Sparse retrieval based on term frequency.</p>"},{"location":"pipelines/retrieval/bm25/#overview","title":"Overview","text":"Field Value Type Retrieval Algorithm TF-IDF variant Modality Text Paper Robertson &amp; Zaragoza, 2009"},{"location":"pipelines/retrieval/bm25/#how-it-works","title":"How It Works","text":"<p>Ranks documents by:</p> <ol> <li>Term frequency in document</li> <li>Inverse document frequency</li> <li>Document length normalization</li> </ol> <p>Uses VectorChord-BM25 PostgreSQL extension for efficient full-text search.</p>"},{"location":"pipelines/retrieval/bm25/#configuration","title":"Configuration","text":"<pre><code>_target_: autorag_research.pipelines.retrieval.bm25.BM25PipelineConfig\nname: bm25\ntokenizer: bert\ntop_k: 10\nbatch_size: 100\n</code></pre>"},{"location":"pipelines/retrieval/bm25/#options","title":"Options","text":"Option Type Default Description name str required Unique pipeline instance name tokenizer str <code>bert</code> Tokenization method index_name str <code>idx_chunk_bm25</code> BM25 index name in PostgreSQL top_k int 10 Results per query batch_size int 100 Queries per batch"},{"location":"pipelines/retrieval/bm25/#tokenizers","title":"Tokenizers","text":"Tokenizer Description bert BERT WordPiece wiki_tocken Wikipedia-based gemma2b Gemma 2B model llmlingua2 LLMLingua2"},{"location":"pipelines/retrieval/bm25/#when-to-use","title":"When to Use","text":"<p>Good for:</p> <ul> <li>Keyword queries</li> <li>Exact term matching</li> <li>Low latency requirements</li> <li>No embedding model needed</li> </ul> <p>Consider dense retrieval for:</p> <ul> <li>Semantic similarity</li> <li>Paraphrase matching</li> <li>Multilingual queries</li> </ul>"},{"location":"pipelines/retrieval/bm25/#citation","title":"Citation","text":"<pre><code>@article{robertson2009probabilistic,\n  title={The probabilistic relevance framework: BM25 and beyond},\n  author={Robertson, Stephen and Zaragoza, Hugo and others},\n  journal={Foundations and trends{\\textregistered} in information retrieval},\n  volume={3},\n  number={4},\n  pages={333--389},\n  year={2009},\n  publisher={Now Publishers, Inc.}\n}\n\n@book{robertson1995okapi,\n  title={Okapi at TREC-3},\n  author={Robertson, Stephen E and Walker, Steve and Jones, Susan and Hancock-Beaulieu, Micheline M and Gatford, Mike and others},\n  year={1995},\n  publisher={British Library Research and Development Department}\n}\n</code></pre>"},{"location":"pipelines/retrieval/hybrid/","title":"Hybrid Retrieval","text":"<p>Combine multiple retrieval pipelines by its relevance scores. There are two types of fusion strategies: RRF and CC (Convex Combination; Weighted Sum)</p>"},{"location":"pipelines/retrieval/hybrid/#overview","title":"Overview","text":"Field Value Type Retrieval Methods RRF, Convex Combination Pipelines Any 2 retrieval pipelines"},{"location":"pipelines/retrieval/hybrid/#methods","title":"Methods","text":""},{"location":"pipelines/retrieval/hybrid/#reciprocal-rank-fusion-rrf","title":"Reciprocal Rank Fusion (RRF)","text":"<p>Combines results based on rank positions, ignoring raw scores.</p> <p>Formula: <code>RRF(d) = sum(1/(k + rank_i(d)))</code></p> <p>Advantages:</p> <ul> <li>Score-scale independent</li> <li>Robust to different retrieval methods</li> <li>No normalization needed</li> </ul> <p>Missing Document Handling:</p> <p>Documents that appear in only one pipeline are assigned rank <code>fetch_k + 1</code> for the missing pipeline, giving them a small but non-zero contribution: <code>1/(k + fetch_k + 1)</code>. This prevents documents from being unfairly penalized when they're highly relevant in one pipeline but absent from the other.</p>"},{"location":"pipelines/retrieval/hybrid/#convex-combination-cc","title":"Convex Combination (CC)","text":"<p>Combines normalized scores with configurable weights.</p> <p>Formula: <code>combined = weight * norm(score_1) + (1-weight) * norm(score_2)</code></p> <p>Normalization Methods:</p> Method Description Missing Score Floor <code>mm</code> Min-max scaling to [0, 1] using actual min/max 0.0 <code>tmm</code> Theoretical min with actual max (e.g., BM25 min=0, cosine min=-1) 0.0 <code>z</code> Z-score standardization -3.0 <code>dbsf</code> 3-sigma distribution-based 0.0 <p>Missing Document Handling:</p> <p>Documents that appear in only one pipeline receive a semantically correct floor value after normalization:</p> <ul> <li>Missing scores are excluded from normalization statistics (min/max/mean/std)</li> <li>After normalization, missing scores are replaced with method-specific floor values</li> <li>For z-score, -3.0 represents 3 standard deviations below the mean (very low relevance)</li> <li>For other methods, 0.0 represents the minimum of the normalized range</li> </ul>"},{"location":"pipelines/retrieval/hybrid/#configuration","title":"Configuration","text":""},{"location":"pipelines/retrieval/hybrid/#rrf-pipeline","title":"RRF Pipeline","text":"<pre><code>_target_: autorag_research.pipelines.retrieval.hybrid.HybridRRFRetrievalPipelineConfig\nname: hybrid_rrf\nretrieval_pipeline_1_name: vector_search\nretrieval_pipeline_2_name: bm25\nrrf_k: 60\nfetch_k_multiplier: 2\ntop_k: 10\n</code></pre>"},{"location":"pipelines/retrieval/hybrid/#cc-pipeline","title":"CC Pipeline","text":"<pre><code>_target_: autorag_research.pipelines.retrieval.hybrid.HybridCCRetrievalPipelineConfig\nname: hybrid_cc\nretrieval_pipeline_1_name: vector_search\nretrieval_pipeline_2_name: bm25\nweight: 0.5\nnormalize_method: mm\nfetch_k_multiplier: 2\ntop_k: 10\n</code></pre>"},{"location":"pipelines/retrieval/hybrid/#options","title":"Options","text":""},{"location":"pipelines/retrieval/hybrid/#common-options","title":"Common Options","text":"Option Type Default Description name str required Unique pipeline name retrieval_pipeline_1_name str required First pipeline name retrieval_pipeline_2_name str required Second pipeline name fetch_k_multiplier int 2 Multiplier for top_k when fetching from sub-pipelines. Each sub-pipeline fetches <code>top_k * fetch_k_multiplier</code> results before fusion. top_k int 10 Results per query batch_size int 100 Queries per batch"},{"location":"pipelines/retrieval/hybrid/#rrf-options","title":"RRF Options","text":"Option Type Default Description rrf_k int 60 RRF constant (higher = more top-rank emphasis)"},{"location":"pipelines/retrieval/hybrid/#cc-options","title":"CC Options","text":"Option Type Default Description weight float 0.5 Weight for pipeline_1 (0=full pipeline_2, 1=full pipeline_1) normalize_method str mm Normalization: mm, tmm, z, dbsf pipeline_1_min float None Theoretical min score for tmm (pipeline_1) pipeline_2_min float None Theoretical min score for tmm (pipeline_2)"},{"location":"pipelines/retrieval/hybrid/#usage","title":"Usage","text":""},{"location":"pipelines/retrieval/hybrid/#programmatic","title":"Programmatic","text":"<pre><code>from autorag_research.pipelines.retrieval import (\n    HybridRRFRetrievalPipeline,\n    HybridCCRetrievalPipeline,\n    VectorSearchRetrievalPipeline,\n    BM25RetrievalPipeline,\n)\n\n# Create sub-pipelines\nvector = VectorSearchRetrievalPipeline(session_factory, \"vector\")\nbm25 = BM25RetrievalPipeline(session_factory, \"bm25\")\n\n# Create RRF hybrid with instantiated pipelines\nhybrid_rrf = HybridRRFRetrievalPipeline(\n    session_factory=session_factory,\n    name=\"hybrid_rrf\",\n    retrieval_pipeline_1=vector,\n    retrieval_pipeline_2=bm25,\n    rrf_k=60,\n    fetch_k_multiplier=2,  # Fetch 2x top_k from each pipeline\n)\n\n# Create CC hybrid with pipeline names (auto-loaded from YAML)\nhybrid_cc = HybridCCRetrievalPipeline(\n    session_factory=session_factory,\n    name=\"hybrid_cc\",\n    retrieval_pipeline_1=\"vector_search\",  # Loads from configs/\n    retrieval_pipeline_2=\"bm25\",\n    weight=0.6,  # 60% vector, 40% BM25\n    normalize_method=\"mm\",\n    fetch_k_multiplier=3,  # Fetch 3x top_k for better fusion\n)\n\nresults = hybrid_rrf.retrieve(\"What is machine learning?\", top_k=10)\n</code></pre>"},{"location":"pipelines/retrieval/hybrid/#cli","title":"CLI","text":"<pre><code>autorag run --pipeline hybrid_rrf --top-k 10\n</code></pre>"},{"location":"pipelines/retrieval/hybrid/#when-to-use","title":"When to Use","text":"<p>Use RRF when:</p> <ul> <li>Combining different retrieval paradigms (dense + sparse)</li> <li>Score scales differ significantly</li> <li>Want robust, parameter-free fusion</li> </ul> <p>Use CC when:</p> <ul> <li>Fine-tuning weight between pipelines</li> <li>Score distributions are known</li> <li>Want explicit control over fusion balance</li> </ul>"},{"location":"pipelines/retrieval/hybrid/#references","title":"References","text":"<ul> <li>Reciprocal Rank Fusion - Cormack et al., 2009</li> <li>Hybrid Search - Survey on hybrid retrieval methods</li> </ul>"},{"location":"pipelines/retrieval/hybrid/#citation","title":"Citation","text":"<pre><code>@inproceedings{cormack2009reciprocal,\n  title={Reciprocal rank fusion outperforms condorcet and individual rank learning methods},\n  author={Cormack, Gordon V and Clarke, Charles LA and Buettcher, Stefan},\n  booktitle={Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval},\n  pages={758--759},\n  year={2009}\n}\n\n@article{bruch2023analysis,\n  title={An analysis of fusion functions for hybrid retrieval},\n  author={Bruch, Sebastian and Gai, Siyu and Ingber, Amir},\n  journal={ACM Transactions on Information Systems},\n  volume={42},\n  number={1},\n  pages={1--35},\n  year={2023},\n  publisher={ACM New York, NY}\n}\n</code></pre>"},{"location":"pipelines/retrieval/hyde/","title":"HyDE (Hypothetical Document Embeddings)","text":"<p>Dense retrieval using LLM-generated hypothetical documents.</p>"},{"location":"pipelines/retrieval/hyde/#overview","title":"Overview","text":"Field Value Type Retrieval Algorithm Dense (vector similarity with hypothetical documents) Modality Text Paper Precise Zero-Shot Dense Retrieval without Relevance Labels"},{"location":"pipelines/retrieval/hyde/#how-it-works","title":"How It Works","text":"<ol> <li>Receives a query</li> <li>Uses LLM to generate a hypothetical document that would answer the query</li> <li>Embeds the hypothetical document (not the original query)</li> <li>Performs vector similarity search with the hypothetical embedding</li> </ol> <p>This bridges the semantic gap between queries and documents by generating document-like text.</p>"},{"location":"pipelines/retrieval/hyde/#configuration","title":"Configuration","text":"<pre><code>_target_: autorag_research.pipelines.retrieval.hyde.HyDEPipelineConfig\nname: hyde_gpt4\nllm: openai-gpt4\nembedding: openai-small\nprompt_template: |\n  Please write a passage to answer the question.\n  Question: {query}\n  Passage:\ntop_k: 10\nbatch_size: 100\n</code></pre>"},{"location":"pipelines/retrieval/hyde/#options","title":"Options","text":"Option Type Default Description name str required Unique pipeline instance name llm str required LLM config name (from configs/llm/) embedding str required Embedding config name (from configs/embedding/) prompt_template str see below Template with {query} placeholder top_k int 10 Results per query batch_size int 100 Queries per batch <p>Default prompt template: <pre><code>Please write a passage to answer the question.\nQuestion: {query}\nPassage:\n</code></pre></p>"},{"location":"pipelines/retrieval/hyde/#custom-prompts","title":"Custom Prompts","text":"<p>The paper recommends domain-specific prompts. Examples:</p> <p>Web search (DL19/20, DBPedia): <pre><code>prompt_template: |\n  Please write a passage to answer the question\n  Question: {query}\n  Passage:\n</code></pre></p> <p>SciFact: <pre><code>prompt_template: |\n  Please write a scientific paper passage to support/refute the claim\n  Claim: {query}\n  Passage:\n</code></pre></p> <p>TREC-COVID: <pre><code>prompt_template: |\n  Please write a scientific paper passage to answer the question\n  Question: {query}\n  Passage:\n</code></pre></p> <p>FiQA: <pre><code>prompt_template: |\n  Please write a financial article passage to answer the question\n  Question: {query}\n  Passage:\n</code></pre></p> <p>TREC-NEWS: <pre><code>prompt_template: |\n  Please write a news passage about the topic.\n  Topic: {query}\n  Passage:\n</code></pre></p> <p>ArguAna: <pre><code>prompt_template: |\n  Please write a counter argument for the passage\n  Passage: {query}\n  Counter Argument:\n</code></pre></p> <p>Mr.TyDi (Multilingual): <pre><code>prompt_template: |\n  Please write a passage in Korean to answer the question in detail.\n  Question: {query}\n  Passage:\n</code></pre></p>"},{"location":"pipelines/retrieval/hyde/#usage","title":"Usage","text":""},{"location":"pipelines/retrieval/hyde/#python-api","title":"Python API","text":"<pre><code>from langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom autorag_research.orm.connection import DBConnection\nfrom autorag_research.pipelines.retrieval.hyde import HyDERetrievalPipeline\n\ndb = DBConnection.from_config()\nsession_factory = db.get_session_factory()\n\npipeline = HyDERetrievalPipeline(\n    session_factory=session_factory,\n    name=\"hyde_gpt4\",\n    llm=ChatOpenAI(model=\"gpt-4\"),\n    embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n    # Optional: custom prompt for domain-specific documents\n    prompt_template=\"Write a Wikipedia passage about: {query}\\n\\nPassage:\",\n)\n\n# Single query\nresults = pipeline.retrieve(\"What is machine learning?\", top_k=10)\n\n# Batch processing\nstats = pipeline.run(top_k=10)\n</code></pre>"},{"location":"pipelines/retrieval/hyde/#with-config","title":"With Config","text":"<pre><code>from autorag_research.pipelines.retrieval.hyde import HyDEPipelineConfig\n\nconfig = HyDEPipelineConfig(\n    name=\"hyde_gpt4\",\n    llm=\"openai-gpt4\",      # Auto-converted to LLM instance\n    embedding=\"openai-small\", # Auto-converted to Embeddings instance\n    top_k=10,\n)\n</code></pre>"},{"location":"pipelines/retrieval/hyde/#when-to-use","title":"When to Use","text":"<p>Good for:</p> <ul> <li>Zero-shot retrieval (no labeled data needed)</li> <li>Bridging query-document semantic gap</li> <li>Complex questions requiring reasoning</li> </ul> <p>Consider other methods when:</p> <ul> <li>Low latency is critical (LLM adds latency)</li> <li>Embedding model cost is a concern</li> <li>Pre-computed query embeddings are available</li> </ul>"},{"location":"pipelines/retrieval/vector-search/","title":"Vector Search","text":"<p>Dense retrieval based on vector similarity.</p>"},{"location":"pipelines/retrieval/vector-search/#overview","title":"Overview","text":"Field Value Type Retrieval Algorithm Dense vector similarity Modality Text Extension VectorChord"},{"location":"pipelines/retrieval/vector-search/#how-it-works","title":"How It Works","text":"<p>Ranks documents by computing similarity between query and document embeddings.</p> <p>Supports two search modes:</p> <ol> <li>Single-vector: Standard dense retrieval with one embedding per document</li> <li>Multi-vector: Late interaction (MaxSim) with multiple token-level embeddings</li> </ol> <p>Uses VectorChord PostgreSQL extension for efficient vector search.</p>"},{"location":"pipelines/retrieval/vector-search/#score-metric","title":"Score Metric","text":"<p>All scores are relevance scores in the range [-1, 1] where higher values indicate greater relevance.</p>"},{"location":"pipelines/retrieval/vector-search/#single-vector-mode-cosine-similarity","title":"Single-Vector Mode (Cosine Similarity)","text":"<p>Uses cosine similarity between query and document embeddings:</p> \\[ \\text{score} = 1 - \\text{cosine\\_distance} = \\cos(\\theta) = \\frac{\\mathbf{q} \\cdot \\mathbf{d}}{\\|\\mathbf{q}\\| \\|\\mathbf{d}\\|} \\] <p>Where:</p> <ul> <li>\\(\\mathbf{q}\\) is the query embedding vector</li> <li>\\(\\mathbf{d}\\) is the document embedding vector</li> </ul>"},{"location":"pipelines/retrieval/vector-search/#multi-vector-mode-normalized-late-interaction","title":"Multi-Vector Mode (Normalized Late Interaction)","text":"<p>Uses MaxSim operation normalized by the number of query vectors:</p> \\[ \\text{score} = \\frac{1}{n} \\sum_{i=1}^{n} \\max_{j} (\\mathbf{q}_i \\cdot \\mathbf{d}_j) \\] <p>Where:</p> <ul> <li>\\(n\\) is the number of query token vectors</li> <li>\\(\\mathbf{q}_i\\) is the \\(i\\)-th query token embedding</li> <li>\\(\\mathbf{d}_j\\) is the \\(j\\)-th document token embedding</li> </ul> <p>This normalization ensures scores remain in [-1, 1] regardless of query length, making them comparable across different queries and compatible with single-vector scores for hybrid search.</p>"},{"location":"pipelines/retrieval/vector-search/#configuration","title":"Configuration","text":"<pre><code>_target_: autorag_research.pipelines.retrieval.vector_search.VectorSearchPipelineConfig\nname: vector_search\nsearch_mode: single\ntop_k: 10\nbatch_size: 100\n</code></pre>"},{"location":"pipelines/retrieval/vector-search/#options","title":"Options","text":"Option Type Default Description name str required Unique pipeline instance name search_mode str <code>single</code> Embedding mode (<code>single</code> or <code>multi</code>) top_k int 10 Results per query batch_size int 100 Queries per batch"},{"location":"pipelines/retrieval/vector-search/#search-modes","title":"Search Modes","text":"Mode Embedding Field Algorithm Use Case single <code>query.embedding</code> Cosine similarity Standard dense retrieval multi <code>query.embeddings</code> MaxSim (late interaction) Fine-grained token matching"},{"location":"pipelines/retrieval/vector-search/#prerequisites","title":"Prerequisites","text":"<p>Queries must have pre-computed embeddings before running the pipeline:</p> <pre><code>from autorag_research.data_ingestor import DataIngestor\n\ningestor = DataIngestor(session_factory)\ningestor.embed_all(embedding_model)  # Populates embedding/embeddings fields\n</code></pre>"},{"location":"pipelines/retrieval/vector-search/#when-to-use","title":"When to Use","text":"<p>Good for:</p> <ul> <li>Semantic similarity search</li> <li>Paraphrase and synonym matching</li> <li>Cross-lingual retrieval (with multilingual embeddings)</li> <li>Fine-grained matching (multi-vector mode)</li> </ul> <p>Consider BM25 for:</p> <ul> <li>Exact keyword matching</li> <li>Low latency requirements</li> <li>No embedding model available</li> </ul>"},{"location":"plugins/","title":"Plugins","text":"<p>Extend AutoRAG-Research with custom pipelines and metrics.</p>"},{"location":"plugins/#overview","title":"Overview","text":"<p>AutoRAG-Research supports external plugins that add custom pipelines, metrics, and data ingestors. Plugins use Python's <code>entry_points</code> mechanism for discovery. Install a plugin package, run <code>plugin sync</code> to copy its configs (pipelines/metrics), or use <code>autorag-research ingest</code> to run ingestor plugins directly.</p>"},{"location":"plugins/#how-it-works","title":"How It Works","text":"<ol> <li>Install a plugin package (<code>pip install autorag-research-elasticsearch</code>)</li> <li>The package registers entry_points under <code>autorag_research.pipelines</code>, <code>autorag_research.metrics</code>, or <code>autorag_research.ingestors</code></li> <li>For pipelines/metrics: run <code>autorag-research plugin sync</code> to copy YAML configs into <code>configs/</code></li> <li>For ingestors: run <code>autorag-research ingest --name=&lt;plugin_name&gt;</code> to ingest data directly</li> </ol>"},{"location":"plugins/#plugin-types","title":"Plugin Types","text":"Type Entry Point Group Base Config Class Base Pipeline/Metric Class Retrieval Pipeline <code>autorag_research.pipelines</code> <code>BaseRetrievalPipelineConfig</code> <code>BaseRetrievalPipeline</code> Generation Pipeline <code>autorag_research.pipelines</code> <code>BaseGenerationPipelineConfig</code> <code>BaseGenerationPipeline</code> Retrieval Metric <code>autorag_research.metrics</code> <code>BaseRetrievalMetricConfig</code> Function-based Generation Metric <code>autorag_research.metrics</code> <code>BaseGenerationMetricConfig</code> Function-based Data Ingestor <code>autorag_research.ingestors</code> N/A (<code>@register_ingestor</code>) <code>TextEmbeddingDataIngestor</code>"},{"location":"plugins/#quick-start","title":"Quick Start","text":"<pre><code># Scaffold a new retrieval plugin\nautorag-research plugin create my_search --type=retrieval\n\n# Edit the generated code\ncd my_search_plugin\n# ... implement your logic in src/my_search_plugin/pipeline.py\n\n# Install in development mode\npip install -e .\n\n# Sync configs to your project\ncd /path/to/your/project\nautorag-research plugin sync\n</code></pre>"},{"location":"plugins/#ingestor-plugin","title":"Ingestor Plugin","text":"<pre><code># Scaffold a new ingestor plugin\nautorag-research plugin create my_dataset --type=ingestor\n\n# Edit the generated code\ncd my_dataset_plugin\n# ... implement your logic in src/my_dataset_plugin/ingestor.py\n\n# Install in development mode\npip install -e .\n\n# Run the ingestor\nautorag-research ingest --name=my_dataset\n</code></pre>"},{"location":"plugins/#next-steps","title":"Next Steps","text":"<ul> <li>CLI Reference</li> <li>Retrieval Pipeline</li> <li>Generation Pipeline</li> <li>Metrics</li> <li>Best Practices</li> </ul>"},{"location":"plugins/best-practices/","title":"Best Practices","text":"<p>Guidelines, caveats, and common pitfalls for developing AutoRAG-Research plugins.</p>"},{"location":"plugins/best-practices/#security","title":"Security","text":"<p>Security Note</p> <p>Plugin discovery calls <code>ep.load()</code> which executes code from installed packages. Only install plugins from trusted sources. Review plugin code before installation.</p> <ul> <li><code>plugin sync</code> loads plugin modules via <code>entry_points()</code> + <code>ep.load()</code> -- this runs arbitrary code from the installed package.</li> <li>Only install plugins from trusted, reviewed sources.</li> <li>Plugin names are validated against <code>^[a-z][a-z0-9_]*$</code> to prevent path traversal and injection.</li> </ul>"},{"location":"plugins/best-practices/#plugin-naming","title":"Plugin Naming","text":"<p>Plugin names must start with a lowercase letter and contain only lowercase letters, digits, and underscores.</p> <p>Regex: <code>^[a-z][a-z0-9_]*$</code></p> Name Valid Reason <code>my_search</code> Yes <code>es_retrieval</code> Yes <code>custom_bm25</code> Yes <code>MySearch</code> No Uppercase letters <code>123plugin</code> No Starts with digit <code>my-search</code> No Hyphens not allowed <code>_private</code> No Starts with underscore"},{"location":"plugins/best-practices/#package-layout","title":"Package Layout","text":"<p>Use a nested layout with subcategory directories for YAML configs:</p> <pre><code>src/my_plugin/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 pipeline.py          # or metric.py\n\u2514\u2500\u2500 retrieval/           # subcategory directory\n    \u2514\u2500\u2500 my_search.yaml\n</code></pre> <p>For ingestor plugins, no YAML config directory is needed:</p> <pre><code>src/my_dataset_plugin/\n\u251c\u2500\u2500 __init__.py\n\u2514\u2500\u2500 ingestor.py          # @register_ingestor decorated class\n</code></pre> <p>The subcategory directory determines where configs are synced (pipelines/metrics only):</p> <ul> <li><code>retrieval/</code> syncs to <code>configs/pipelines/retrieval/</code> or <code>configs/metrics/retrieval/</code></li> <li><code>generation/</code> syncs to <code>configs/pipelines/generation/</code> or <code>configs/metrics/generation/</code></li> </ul> <p>Place the YAML file in the correct subcategory directory or it will not be discovered.</p>"},{"location":"plugins/best-practices/#config-sync-behavior","title":"Config Sync Behavior","text":"<ul> <li><code>plugin sync</code> never overwrites existing files. To re-sync a config, delete the existing file first.</li> <li>Configs are copied, not symlinked. Editing the local copy does not affect the plugin source.</li> <li>Install a plugin first, then run <code>plugin sync</code>. Order matters -- discovery requires the package to be installed.</li> </ul>"},{"location":"plugins/best-practices/#testing-guidelines","title":"Testing Guidelines","text":"<ul> <li>Use <code>MagicMock()</code> for LLM and <code>session_factory</code> in unit tests.</li> <li>Test config instantiation and abstract method implementations separately.</li> <li>Use pytest markers: <code>@pytest.mark.api</code> for tests needing real LLM calls.</li> <li>The scaffold includes a basic test file. Extend it with integration tests.</li> </ul> <pre><code>from unittest.mock import MagicMock\n\nimport pytest\n\n\ndef test_pipeline_config():\n    \"\"\"Test config can be created and returns correct class.\"\"\"\n    config = MySearchPipelineConfig(name=\"test\")\n    assert config.get_pipeline_class() is MySearchPipeline\n    assert \"index_path\" in config.get_pipeline_kwargs()\n\n\n@pytest.mark.api\ndef test_pipeline_integration(db_session):\n    \"\"\"Integration test with real database (requires Docker).\"\"\"\n    # Use db_session fixture from conftest.py\n    pass\n</code></pre> <p>For ingestor plugins, use <code>FakeEmbeddings</code> from langchain_core:</p> <pre><code>from langchain_core.embeddings import FakeEmbeddings\n\n\ndef test_ingestor_instantiation():\n    \"\"\"Test ingestor can be created with fake embeddings.\"\"\"\n    embeddings = FakeEmbeddings(size=128)\n    ingestor = MyDatasetIngestor(\n        embedding_model=embeddings,\n        dataset_name=\"dataset_a\",\n    )\n    assert ingestor.dataset_name == \"dataset_a\"\n</code></pre>"},{"location":"plugins/best-practices/#development-workflow","title":"Development Workflow","text":"<ol> <li>Scaffold -- <code>autorag-research plugin create NAME --type=TYPE</code></li> <li>Implement -- edit <code>pipeline.py</code> or <code>metric.py</code> with your logic</li> <li>Configure -- edit the YAML config to set parameters</li> <li>Test -- <code>pytest tests/</code> to run plugin tests</li> <li>Install -- <code>pip install -e .</code> to install in dev mode</li> <li>Sync -- <code>autorag-research plugin sync</code> to copy configs into the project</li> <li>Integrate -- add the plugin name to your experiment config</li> <li>Run -- <code>autorag-research run --config-name=experiment</code></li> </ol>"},{"location":"plugins/best-practices/#common-pitfalls","title":"Common Pitfalls","text":"Pitfall Solution Forgot <code>pip install -e .</code> Plugin won't be discovered. Install before sync. Config not appearing after sync Check entry_points in <code>pyproject.toml</code>. Run <code>pip show your-plugin</code> to verify installation. Existing config not updated <code>plugin sync</code> never overwrites. Delete the old file and re-sync. <code>_target_</code> path wrong Must be fully-qualified: <code>package.module.ClassName</code> LLM string not loading Ensure the LLM provider package is installed (e.g., <code>langchain-openai</code>). <code>get_pipeline_kwargs()</code> missing custom params Only extra kwargs beyond <code>session_factory</code>, <code>name</code>, <code>schema</code> need to be returned."},{"location":"plugins/best-practices/#see-also","title":"See Also","text":"<ul> <li>Plugin Overview</li> <li>CLI Reference</li> <li>Custom Pipeline Tutorial</li> <li>Custom Metric Tutorial</li> </ul>"},{"location":"plugins/cli/","title":"Plugin CLI","text":"<p>Command group for plugin discovery and scaffolding.</p>"},{"location":"plugins/cli/#overview","title":"Overview","text":"<p>The <code>autorag-research plugin</code> command group manages plugin lifecycle: creating new plugins from templates and syncing installed plugin configs into your project.</p>"},{"location":"plugins/cli/#plugin-create","title":"plugin create","text":"<p>Scaffolds a new plugin project with build config, skeleton code, YAML config, and tests.</p>"},{"location":"plugins/cli/#synopsis","title":"Synopsis","text":"<pre><code>autorag-research plugin create NAME --type=TYPE\n</code></pre>"},{"location":"plugins/cli/#arguments","title":"Arguments","text":"Argument Required Description <code>NAME</code> Yes Plugin name (lowercase letters, digits, underscores only. Must start with a letter.) <code>--type</code>, <code>-t</code> Yes Plugin type: <code>retrieval</code>, <code>generation</code>, <code>metric_retrieval</code>, <code>metric_generation</code>, <code>ingestor</code>"},{"location":"plugins/cli/#name-validation","title":"Name Validation","text":"<p>Names must match <code>^[a-z][a-z0-9_]*$</code>.</p> Valid Invalid <code>my_search</code> <code>MySearch</code> <code>es_retrieval</code> <code>123plugin</code> <code>custom_bm25</code> <code>../evil</code>"},{"location":"plugins/cli/#generated-structure","title":"Generated Structure","text":"<pre><code>my_search_plugin/\n\u251c\u2500\u2500 pyproject.toml           # Build config with entry_points\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 my_search_plugin/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 pipeline.py      # Pipeline skeleton (or metric.py for metrics)\n\u2502       \u2514\u2500\u2500 retrieval/       # Subcategory directory\n\u2502           \u2514\u2500\u2500 my_search.yaml\n\u2514\u2500\u2500 tests/\n    \u2514\u2500\u2500 test_my_search.py\n</code></pre>"},{"location":"plugins/cli/#generated-files","title":"Generated Files","text":"<p><code>pyproject.toml</code> -- Hatchling build system, <code>autorag-research</code> dependency, and entry_points registration under the appropriate group.</p> <p><code>pipeline.py</code> (or <code>metric.py</code>) -- Skeleton class inheriting the correct base class with <code>NotImplementedError</code> stubs for required methods.</p> <p>YAML config -- Hydra-style config with <code>_target_</code> pointing to the config class.</p> <p>Test file -- Basic config instantiation test.</p>"},{"location":"plugins/cli/#examples","title":"Examples","text":"<pre><code># Retrieval pipeline plugin\nautorag-research plugin create my_search --type=retrieval\n\n# Generation pipeline plugin\nautorag-research plugin create my_rag --type=generation\n\n# Retrieval metric plugin\nautorag-research plugin create my_recall --type=metric_retrieval\n\n# Generation metric plugin\nautorag-research plugin create my_bleu --type=metric_generation\n\n# Ingestor plugin\nautorag-research plugin create my_dataset --type=ingestor\n</code></pre>"},{"location":"plugins/cli/#plugin-sync","title":"plugin sync","text":"<p>Discovers installed plugins and copies their YAML configs into the local project.</p>"},{"location":"plugins/cli/#synopsis_1","title":"Synopsis","text":"<pre><code>autorag-research plugin sync\n</code></pre>"},{"location":"plugins/cli/#how-it-works","title":"How It Works","text":"<ol> <li>Scans <code>autorag_research.pipelines</code> and <code>autorag_research.metrics</code> entry_points</li> <li>Loads each plugin module and finds YAML config files</li> <li>Copies YAMLs to <code>configs/pipelines/{subcategory}/</code> or <code>configs/metrics/{subcategory}/</code></li> </ol> <p>Existing files are never overwritten. Delete a file manually to re-sync it.</p>"},{"location":"plugins/cli/#output","title":"Output","text":"<pre><code>Copied 2 config(s):\n  + pipelines/retrieval/es_search.yaml  (from plugin: elasticsearch)\n  + metrics/retrieval/custom_recall.yaml  (from plugin: my_metrics)\n\nSkipped 1 config(s) (already exist):\n  = pipelines/retrieval/bm25_custom.yaml  (from plugin: custom_bm25)\n\nTotal: 2 copied, 1 skipped\n</code></pre>"},{"location":"plugins/cli/#ingestor-workflow","title":"Ingestor Workflow","text":"<p>Ingestor plugins use decorator-based registration (<code>@register_ingestor</code>) instead of Hydra YAML configs. No <code>plugin sync</code> step is needed.</p> <pre><code># 1. Scaffold\nautorag-research plugin create my_dataset --type=ingestor\n\n# 2. Implement\ncd my_dataset_plugin\n# Edit src/my_dataset_plugin/ingestor.py\n\n# 3. Test locally\npytest tests/\n\n# 4. Install\npip install -e .\n# or for uv users:\nuv pip install -e ./my_dataset_plugin\n\n# 5. Run the ingestor\nautorag-research ingest --name=my_dataset\n</code></pre>"},{"location":"plugins/cli/#pipeline-metric-workflow","title":"Pipeline / Metric Workflow","text":"<p>Full development lifecycle for a plugin:</p> <pre><code># 1. Scaffold\nautorag-research plugin create my_search --type=retrieval\n\n# 2. Implement\ncd my_search_plugin\n# Edit src/my_search_plugin/pipeline.py\n\n# 3. Test locally\npytest tests/\n\n# 4. Install\npip install -e .\n# or for uv users:\nuv pip install -e ./my_search_plugin\n\n# 5. Sync configs\ncd /path/to/your/project\nautorag-research plugin sync\n\n# 6. Use in experiment\n# Edit configs/experiment.yaml to include my_search\nautorag-research run --config-name=experiment\n</code></pre>"},{"location":"plugins/generation-pipeline/","title":"Generation Pipeline Plugin","text":"<p>Implement a custom generation pipeline and distribute it as a plugin.</p>"},{"location":"plugins/generation-pipeline/#overview","title":"Overview","text":"Field Value Entry Point Group <code>autorag_research.pipelines</code> Config Base Class <code>BaseGenerationPipelineConfig</code> Pipeline Base Class <code>BaseGenerationPipeline</code> Config Module <code>autorag_research.config</code> Pipeline Module <code>autorag_research.pipelines.generation.base</code> <p>A generation pipeline composes a retrieval pipeline with an LLM to produce answers. The config declares model settings and which retrieval pipeline to use; the pipeline class implements the retrieve-then-generate logic.</p>"},{"location":"plugins/generation-pipeline/#scaffold","title":"Scaffold","text":"<p>Generate boilerplate with the CLI:</p> <pre><code>autorag-research plugin create my_rag --type=generation\n</code></pre> <p>This creates a package under <code>my_rag_plugin/</code> with config, pipeline, YAML, and <code>pyproject.toml</code> pre-wired.</p>"},{"location":"plugins/generation-pipeline/#config-class","title":"Config Class","text":"<p>Subclass <code>BaseGenerationPipelineConfig</code> and define custom parameters. Implement <code>get_pipeline_class()</code> and <code>get_pipeline_kwargs()</code>.</p> <pre><code>from dataclasses import dataclass, field\nfrom typing import Any\n\nfrom autorag_research.config import BaseGenerationPipelineConfig, PipelineType\n\n\n@dataclass(kw_only=True)\nclass MyRAGPipelineConfig(BaseGenerationPipelineConfig):\n    \"\"\"Configuration for MyRAG generation pipeline.\"\"\"\n\n    pipeline_type: PipelineType = field(default=PipelineType.GENERATION, init=False)\n\n    # Add custom config fields\n    temperature: float = 0.7\n    system_prompt: str = \"You are a helpful assistant.\"\n\n    def get_pipeline_class(self) -&gt; type[\"MyRAGPipeline\"]:\n        return MyRAGPipeline\n\n    def get_pipeline_kwargs(self) -&gt; dict[str, Any]:\n        return {\n            \"temperature\": self.temperature,\n            \"system_prompt\": self.system_prompt,\n        }\n</code></pre>"},{"location":"plugins/generation-pipeline/#inherited-fields","title":"Inherited Fields","text":"<p>In addition to the base fields shared with retrieval configs (<code>name</code>, <code>description</code>, <code>top_k</code>, <code>batch_size</code>, <code>max_concurrency</code>, <code>max_retries</code>, <code>retry_delay</code>), generation configs add:</p> Field Type Default Description <code>llm</code> <code>str \\| BaseLanguageModel</code> required LLM model name (auto-loaded) or LangChain instance <code>retrieval_pipeline_name</code> <code>str</code> required Name of retrieval pipeline to compose with <p>When <code>llm</code> is a string such as <code>\"gpt-4o-mini\"</code>, the framework loads it automatically via <code>load_llm()</code>. The <code>retrieval_pipeline_name</code> references a retrieval pipeline already registered in the experiment; the Executor resolves and injects it at runtime.</p>"},{"location":"plugins/generation-pipeline/#abstract-methods","title":"Abstract Methods","text":"Method Returns Purpose <code>get_pipeline_class()</code> <code>type[BaseGenerationPipeline]</code> Pipeline class to instantiate <code>get_pipeline_kwargs()</code> <code>dict[str, Any]</code> Custom kwargs passed to the pipeline constructor (beyond <code>session_factory</code>, <code>name</code>, <code>llm</code>, <code>retrieval_pipeline</code>, <code>schema</code> which are injected automatically)"},{"location":"plugins/generation-pipeline/#pipeline-class","title":"Pipeline Class","text":"<p>Subclass <code>BaseGenerationPipeline</code> and implement <code>_generate()</code> and <code>_get_pipeline_config()</code>.</p> <pre><code>from typing import Any\n\nfrom langchain_core.language_models import BaseLanguageModel\nfrom sqlalchemy.orm import Session, sessionmaker\n\nfrom autorag_research.orm.service.generation_pipeline import GenerationResult\nfrom autorag_research.pipelines.generation.base import BaseGenerationPipeline\nfrom autorag_research.pipelines.retrieval.base import BaseRetrievalPipeline\n\n\nclass MyRAGPipeline(BaseGenerationPipeline):\n    \"\"\"MyRAG generation pipeline.\"\"\"\n\n    def __init__(\n        self,\n        session_factory: sessionmaker[Session],\n        name: str,\n        llm: BaseLanguageModel,\n        retrieval_pipeline: BaseRetrievalPipeline,\n        schema: Any | None = None,\n        temperature: float = 0.7,\n        system_prompt: str = \"You are a helpful assistant.\",\n    ):\n        super().__init__(session_factory, name, llm, retrieval_pipeline, schema)\n        self.temperature = temperature\n        self.system_prompt = system_prompt\n\n    def _get_pipeline_config(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": \"my_rag\",\n            \"temperature\": self.temperature,\n        }\n\n    async def _generate(self, query_id: int, top_k: int) -&gt; GenerationResult:\n        # Step 1: Retrieve relevant chunks\n        results = await self._retrieval_pipeline._retrieve_by_id(query_id, top_k)\n        chunk_ids = [r[\"doc_id\"] for r in results]\n        chunk_contents = self._service.get_chunk_contents(chunk_ids)\n\n        # Step 2: Get query text\n        query_text = self._get_query_text(query_id)\n\n        # Step 3: Build prompt and generate\n        context = \"\\n\\n\".join(chunk_contents)\n        prompt = f\"{self.system_prompt}\\n\\nContext:\\n{context}\\n\\nQuestion: {query_text}\\n\\nAnswer:\"\n        response = await self._llm.ainvoke(prompt)\n\n        return GenerationResult(text=str(response.content))\n</code></pre>"},{"location":"plugins/generation-pipeline/#available-resources","title":"Available Resources","text":"<p>Inside <code>_generate()</code>, you have access to:</p> Resource Access Description Retrieval pipeline <code>self._retrieval_pipeline</code> Composed retrieval pipeline instance LLM <code>self._llm</code> LangChain <code>BaseLanguageModel</code> (use <code>.ainvoke()</code> for async) Service <code>self._service</code> <code>GenerationPipelineService</code> for DB operations Query text <code>self._get_query_text(query_id)</code> Gets query text (uses <code>query_to_llm</code> if available)"},{"location":"plugins/generation-pipeline/#generationresult","title":"GenerationResult","text":"<p><code>_generate()</code> must return a <code>GenerationResult</code> dataclass:</p> <pre><code>@dataclass\nclass GenerationResult:\n    text: str                              # Generated answer text\n    token_usage: dict[str, int] | None = None  # Optional: {\"prompt\": N, \"completion\": M}\n    metadata: dict[str, Any] | None = None     # Optional: extra metadata\n</code></pre> <p>Only <code>text</code> is required. Populate <code>token_usage</code> if your LLM response includes token counts -- the executor persists these for cost tracking. Use <code>metadata</code> for any additional information you want stored alongside the result.</p>"},{"location":"plugins/generation-pipeline/#yaml-configuration","title":"YAML Configuration","text":"<p>Place a YAML file inside your plugin package:</p> <pre><code># src/my_rag_plugin/generation/my_rag.yaml\n_target_: my_rag_plugin.pipeline.MyRAGPipelineConfig\ndescription: \"MyRAG generation pipeline\"\nname: my_rag\nretrieval_pipeline_name: bm25\nllm: gpt-4o-mini\ntop_k: 10\ntemperature: 0.7\nsystem_prompt: \"You are a helpful assistant.\"\n</code></pre> <p><code>_target_</code> is the fully-qualified class name of your config dataclass (Hydra-style instantiation). The remaining keys map directly to dataclass fields.</p> <p><code>retrieval_pipeline_name</code> must match the <code>name</code> field of a retrieval pipeline config in the same experiment. The Executor resolves this reference and injects the live pipeline instance into your generation pipeline at runtime.</p>"},{"location":"plugins/generation-pipeline/#entry-points","title":"Entry Points","text":"<p>Register under the same group as retrieval plugins:</p> <pre><code>[project.entry-points.\"autorag_research.pipelines\"]\nmy_rag = \"my_rag_plugin\"\n</code></pre> <p>The key (<code>my_rag</code>) is the plugin name. The value is the top-level package containing your YAML configs.</p> <p>After installing the package, run:</p> <pre><code>autorag-research plugin sync\n</code></pre> <p>This copies your YAML files into the project's <code>configs/</code> directory.</p>"},{"location":"plugins/generation-pipeline/#testing","title":"Testing","text":"<p>Test the config independently. Use <code>MagicMock()</code> for the <code>llm</code> field to avoid loading a real model:</p> <pre><code>from unittest.mock import MagicMock\n\nfrom my_rag_plugin.pipeline import MyRAGPipelineConfig\n\n\ndef test_config():\n    config = MyRAGPipelineConfig(\n        name=\"my_rag\",\n        llm=MagicMock(),\n        retrieval_pipeline_name=\"bm25\",\n    )\n    assert config.name == \"my_rag\"\n\n\ndef test_config_custom_fields():\n    config = MyRAGPipelineConfig(\n        name=\"my_rag\",\n        llm=MagicMock(),\n        retrieval_pipeline_name=\"bm25\",\n        temperature=0.3,\n        system_prompt=\"Answer concisely.\",\n    )\n    kwargs = config.get_pipeline_kwargs()\n    assert kwargs[\"temperature\"] == 0.3\n    assert kwargs[\"system_prompt\"] == \"Answer concisely.\"\n</code></pre> <p>For integration tests that exercise <code>_generate()</code>, use <code>FakeListLLM</code> from <code>langchain_core.llms</code> and the <code>db_session</code> fixture from the test <code>conftest.py</code>.</p>"},{"location":"plugins/generation-pipeline/#next","title":"Next","text":"<ul> <li>Retrieval Pipeline -- build a retrieval pipeline plugin</li> <li>Metrics -- implement a custom metric plugin</li> <li>Best Practices -- packaging, versioning, and distribution tips</li> </ul>"},{"location":"plugins/metrics/","title":"Metric Plugins","text":"<p>Metric plugins add custom evaluation metrics to AutoRAG-Research. There are two types: retrieval metrics evaluate search quality, and generation metrics evaluate answer quality.</p> Type Entry Point Group Base Config Class Implementation Retrieval Metric <code>autorag_research.metrics</code> <code>BaseRetrievalMetricConfig</code> Function-based Generation Metric <code>autorag_research.metrics</code> <code>BaseGenerationMetricConfig</code> Function-based <p>Both types follow the same pattern: a standalone metric function paired with a dataclass config that wraps it.</p>"},{"location":"plugins/metrics/#scaffold","title":"Scaffold","text":"<p>Use the CLI to generate a starter plugin:</p> <pre><code># Retrieval metric\nautorag-research plugin create my_recall --type=metric_retrieval\n\n# Generation metric\nautorag-research plugin create my_bleu --type=metric_generation\n</code></pre> <p>This creates a project directory with the config class, metric function stub, YAML config, <code>pyproject.toml</code>, and a basic test file.</p>"},{"location":"plugins/metrics/#retrieval-metric","title":"Retrieval Metric","text":"<p>A retrieval metric is a plain function that computes a score. The config class wraps it via <code>get_metric_func()</code>.</p> <pre><code>from collections.abc import Callable\nfrom dataclasses import dataclass\n\nfrom autorag_research.config import BaseRetrievalMetricConfig\n\n\ndef my_recall_metric(**kwargs) -&gt; float:\n    \"\"\"Compute custom recall metric.\"\"\"\n    # Your metric logic here\n    return score\n\n\n@dataclass\nclass MyRecallMetricConfig(BaseRetrievalMetricConfig):\n    \"\"\"Configuration for custom recall metric.\"\"\"\n\n    def get_metric_func(self) -&gt; Callable:\n        return my_recall_metric\n</code></pre> <p>Key points:</p> <ul> <li>The metric is a standalone function, not a class method.</li> <li>The config class wraps the function and exposes it through <code>get_metric_func()</code>.</li> <li><code>BaseRetrievalMetricConfig</code> automatically sets <code>metric_type = MetricType.RETRIEVAL</code>.</li> <li><code>get_metric_name()</code> is inherited and returns the function name by default.</li> </ul>"},{"location":"plugins/metrics/#inherited-fields","title":"Inherited Fields","text":"<p><code>BaseMetricConfig</code> provides these fields to all metric configs:</p> Field Type Default Description <code>description</code> <code>str</code> <code>\"\"</code> Optional description <code>metric_type</code> <code>MetricType</code> Auto-set <code>RETRIEVAL</code> or <code>GENERATION</code> <p>Override <code>get_metric_kwargs()</code> to pass extra arguments to the metric function at evaluation time.</p>"},{"location":"plugins/metrics/#generation-metric","title":"Generation Metric","text":"<p>Generation metrics follow the same pattern but extend <code>BaseGenerationMetricConfig</code>.</p> <pre><code>from collections.abc import Callable\nfrom dataclasses import dataclass\n\nfrom autorag_research.config import BaseGenerationMetricConfig\n\n\ndef my_bleu_metric(**kwargs) -&gt; float:\n    \"\"\"Compute custom BLEU metric.\"\"\"\n    # Your metric logic here\n    return score\n\n\n@dataclass\nclass MyBleuMetricConfig(BaseGenerationMetricConfig):\n    \"\"\"Configuration for custom BLEU metric.\"\"\"\n\n    def get_metric_func(self) -&gt; Callable:\n        return my_bleu_metric\n</code></pre> <p>The only difference is the base class: <code>BaseGenerationMetricConfig</code> sets <code>metric_type = MetricType.GENERATION</code>.</p>"},{"location":"plugins/metrics/#yaml-configuration","title":"YAML Configuration","text":"<p>Each metric plugin ships a YAML config file in a subcategory directory.</p> <p>Retrieval metric:</p> <pre><code># src/my_recall_plugin/retrieval/my_recall.yaml\n_target_: my_recall_plugin.metric.MyRecallMetricConfig\ndescription: \"Custom recall metric\"\n</code></pre> <p>Generation metric:</p> <pre><code># src/my_bleu_plugin/generation/my_bleu.yaml\n_target_: my_bleu_plugin.metric.MyBleuMetricConfig\ndescription: \"Custom BLEU metric\"\n</code></pre> <p>The <code>_target_</code> field must be the fully-qualified path to the config class.</p>"},{"location":"plugins/metrics/#entry-points","title":"Entry Points","text":"<p>Register the plugin in <code>pyproject.toml</code> so AutoRAG-Research can discover it:</p> <pre><code>[project.entry-points.\"autorag_research.metrics\"]\nmy_recall = \"my_recall_plugin\"\n</code></pre> <p>After editing <code>pyproject.toml</code>, reinstall the package (<code>pip install -e .</code>) and run <code>autorag-research plugin sync</code> to copy configs into the project.</p>"},{"location":"plugins/metrics/#use-in-experiment","title":"Use in Experiment","text":"<p>Reference your metric by name in the experiment config:</p> <pre><code># configs/experiment.yaml\nmetrics:\n  retrieval:\n    - recall       # built-in\n    - my_recall    # your plugin\n  generation:\n    - rouge        # built-in\n    - my_bleu      # your plugin\n</code></pre> <p>The metric name matches the entry point key defined in <code>pyproject.toml</code>.</p>"},{"location":"plugins/metrics/#testing","title":"Testing","text":"<p>Test that the config class instantiates correctly and returns a callable metric function:</p> <pre><code>from my_recall_plugin.metric import MyRecallMetricConfig\n\n\ndef test_metric_config():\n    config = MyRecallMetricConfig()\n    func = config.get_metric_func()\n    assert func is not None\n    assert callable(func)\n</code></pre> <p>For integration tests that call real APIs or require data, use the <code>@pytest.mark.api</code> or <code>@pytest.mark.data</code> markers.</p>"},{"location":"plugins/metrics/#next","title":"Next","text":"<ul> <li>Retrieval Pipeline -- build a custom retrieval pipeline plugin</li> <li>Generation Pipeline -- build a custom generation pipeline plugin</li> <li>Best Practices -- naming, security, and common pitfalls</li> </ul>"},{"location":"plugins/retrieval-pipeline/","title":"Retrieval Pipeline Plugin","text":"<p>Implement a custom retrieval pipeline and distribute it as a plugin.</p>"},{"location":"plugins/retrieval-pipeline/#overview","title":"Overview","text":"Field Value Entry Point Group <code>autorag_research.pipelines</code> Config Base Class <code>BaseRetrievalPipelineConfig</code> Pipeline Base Class <code>BaseRetrievalPipeline</code> Config Module <code>autorag_research.config</code> Pipeline Module <code>autorag_research.pipelines.retrieval.base</code> <p>A retrieval pipeline plugin consists of two classes: a config dataclass that declares parameters and a pipeline class that implements search logic. The config tells the executor how to build the pipeline; the pipeline performs the actual retrieval.</p>"},{"location":"plugins/retrieval-pipeline/#scaffold","title":"Scaffold","text":"<p>Generate boilerplate with the CLI:</p> <pre><code>autorag-research plugin create my_search --type=retrieval\n</code></pre> <p>This creates a ready-to-edit package under <code>my_search_plugin/</code> with config, pipeline, YAML, and <code>pyproject.toml</code> pre-wired.</p>"},{"location":"plugins/retrieval-pipeline/#config-class","title":"Config Class","text":"<p>Subclass <code>BaseRetrievalPipelineConfig</code> and define your custom parameters as dataclass fields. Implement <code>get_pipeline_class()</code> and <code>get_pipeline_kwargs()</code>.</p> <pre><code>from dataclasses import dataclass, field\nfrom typing import Any\n\nfrom autorag_research.config import BaseRetrievalPipelineConfig, PipelineType\n\n\n@dataclass(kw_only=True)\nclass MySearchPipelineConfig(BaseRetrievalPipelineConfig):\n    \"\"\"Configuration for MySearch retrieval pipeline.\"\"\"\n\n    pipeline_type: PipelineType = field(default=PipelineType.RETRIEVAL, init=False)\n\n    # Add custom config fields\n    index_path: str = \"/data/index\"\n    similarity_threshold: float = 0.5\n\n    def get_pipeline_class(self) -&gt; type[\"MySearchPipeline\"]:\n        return MySearchPipeline\n\n    def get_pipeline_kwargs(self) -&gt; dict[str, Any]:\n        return {\n            \"index_path\": self.index_path,\n            \"similarity_threshold\": self.similarity_threshold,\n        }\n</code></pre>"},{"location":"plugins/retrieval-pipeline/#inherited-fields","title":"Inherited Fields","text":"<p>Every retrieval config inherits these fields from <code>BasePipelineConfig</code>:</p> Field Type Default Description <code>name</code> <code>str</code> required Unique pipeline instance name <code>description</code> <code>str</code> <code>\"\"</code> Optional description <code>top_k</code> <code>int</code> <code>10</code> Results per query <code>batch_size</code> <code>int</code> <code>128</code> Queries per DB batch <code>max_concurrency</code> <code>int</code> <code>16</code> Max concurrent async operations <code>max_retries</code> <code>int</code> <code>3</code> Retry attempts for failed queries <code>retry_delay</code> <code>float</code> <code>1.0</code> Base delay for exponential backoff"},{"location":"plugins/retrieval-pipeline/#abstract-methods","title":"Abstract Methods","text":"Method Returns Purpose <code>get_pipeline_class()</code> <code>type[BaseRetrievalPipeline]</code> Pipeline class to instantiate <code>get_pipeline_kwargs()</code> <code>dict[str, Any]</code> Custom kwargs passed to the pipeline constructor (beyond <code>session_factory</code>, <code>name</code>, <code>schema</code> which are injected automatically) <code>get_run_kwargs()</code> <code>dict[str, Any]</code> Already implemented by <code>BaseRetrievalPipelineConfig</code> -- returns <code>top_k</code>, <code>batch_size</code>, <code>max_concurrency</code>, <code>max_retries</code>, and <code>retry_delay</code> <p>You must implement <code>get_pipeline_class()</code> and <code>get_pipeline_kwargs()</code>. <code>get_run_kwargs()</code> is provided by the base class and normally does not need overriding.</p>"},{"location":"plugins/retrieval-pipeline/#pipeline-class","title":"Pipeline Class","text":"<p>Subclass <code>BaseRetrievalPipeline</code> and implement the three abstract methods.</p> <pre><code>from typing import Any\n\nfrom sqlalchemy.orm import Session, sessionmaker\n\nfrom autorag_research.pipelines.retrieval.base import BaseRetrievalPipeline\n\n\nclass MySearchPipeline(BaseRetrievalPipeline):\n    \"\"\"MySearch retrieval pipeline.\"\"\"\n\n    def __init__(\n        self,\n        session_factory: sessionmaker[Session],\n        name: str,\n        schema: Any | None = None,\n        index_path: str = \"/data/index\",\n        similarity_threshold: float = 0.5,\n    ):\n        super().__init__(session_factory, name, schema)\n        self.index_path = index_path\n        self.similarity_threshold = similarity_threshold\n\n    def _get_pipeline_config(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": \"my_search\",\n            \"index_path\": self.index_path,\n            \"similarity_threshold\": self.similarity_threshold,\n        }\n\n    async def _retrieve_by_id(self, query_id: int | str, top_k: int) -&gt; list[dict[str, Any]]:\n        \"\"\"Retrieve using query ID (query exists in database).\n\n        Used for batch processing where queries have pre-computed embeddings.\n        \"\"\"\n        # Access query embedding from DB via self._service\n        # Perform your search logic\n        return [{\"doc_id\": chunk_id, \"score\": score}]\n\n    async def _retrieve_by_text(self, query_text: str, top_k: int) -&gt; list[dict[str, Any]]:\n        \"\"\"Retrieve using raw query text (may need to compute embedding).\n\n        Used for ad-hoc retrieval and by generation pipelines.\n        \"\"\"\n        # Compute embedding on-the-fly if needed\n        # Perform your search logic\n        return [{\"doc_id\": chunk_id, \"score\": score}]\n</code></pre>"},{"location":"plugins/retrieval-pipeline/#abstract-methods_1","title":"Abstract Methods","text":"Method When Called Use Case <code>_retrieve_by_id(query_id, top_k)</code> <code>pipeline.run()</code> batch processing Queries exist in DB with stored embeddings <code>_retrieve_by_text(query_text, top_k)</code> <code>pipeline.retrieve()</code> single query Ad-hoc queries, used by generation pipelines <code>_get_pipeline_config()</code> Pipeline initialization Returns dict stored in DB for reproducibility"},{"location":"plugins/retrieval-pipeline/#return-format","title":"Return Format","text":"<p>Both <code>_retrieve_by_id</code> and <code>_retrieve_by_text</code> return a list of dicts. Each dict contains:</p> Key Type Description <code>doc_id</code> <code>int \\| str</code> Chunk ID in the database <code>score</code> <code>float</code> Relevance score (higher is better) <p>The base class handles persisting these results to <code>ChunkRetrievedResult</code> rows automatically.</p>"},{"location":"plugins/retrieval-pipeline/#yaml-configuration","title":"YAML Configuration","text":"<p>Place a YAML file inside your plugin package. The executor loads it via Hydra-style instantiation.</p> <pre><code># src/my_search_plugin/retrieval/my_search.yaml\n_target_: my_search_plugin.pipeline.MySearchPipelineConfig\ndescription: \"MySearch retrieval pipeline\"\nname: my_search\ntop_k: 10\nbatch_size: 128\nmax_concurrency: 16\nindex_path: /data/index\nsimilarity_threshold: 0.5\n</code></pre> <p><code>_target_</code> is the fully-qualified class name of your config dataclass. The remaining keys map directly to dataclass fields. When the executor loads this file, it instantiates <code>MySearchPipelineConfig</code> with these values.</p>"},{"location":"plugins/retrieval-pipeline/#entry-points","title":"Entry Points","text":"<p>Register your plugin in <code>pyproject.toml</code> so the framework discovers it at runtime:</p> <pre><code>[project.entry-points.\"autorag_research.pipelines\"]\nmy_search = \"my_search_plugin\"\n</code></pre> <p>The key (<code>my_search</code>) is the plugin name used in <code>plugin sync</code>. The value is the top-level package that contains your YAML configs.</p> <p>After installing the package, run:</p> <pre><code>autorag-research plugin sync\n</code></pre> <p>This copies your YAML files into the project's <code>configs/</code> directory.</p>"},{"location":"plugins/retrieval-pipeline/#testing","title":"Testing","text":"<p>Test the config independently of the database:</p> <pre><code>from my_search_plugin.pipeline import MySearchPipelineConfig\n\n\ndef test_config():\n    config = MySearchPipelineConfig(name=\"my_search\")\n    assert config.name == \"my_search\"\n    assert config.get_pipeline_class() is not None\n\n\ndef test_config_custom_fields():\n    config = MySearchPipelineConfig(\n        name=\"my_search\",\n        index_path=\"/custom/index\",\n        similarity_threshold=0.8,\n    )\n    kwargs = config.get_pipeline_kwargs()\n    assert kwargs[\"index_path\"] == \"/custom/index\"\n    assert kwargs[\"similarity_threshold\"] == 0.8\n</code></pre> <p>For integration tests that exercise <code>_retrieve_by_id</code> and <code>_retrieve_by_text</code>, use the <code>db_session</code> fixture from the test <code>conftest.py</code> and seed test data per the project testing guidelines.</p>"},{"location":"plugins/retrieval-pipeline/#next","title":"Next","text":"<ul> <li>Generation Pipeline -- build a generation pipeline plugin</li> <li>Best Practices -- packaging, versioning, and distribution tips</li> </ul>"},{"location":"reference/","title":"API Reference","text":"<p>Auto-generated from source code.</p>"},{"location":"reference/#configuration","title":"Configuration","text":""},{"location":"reference/#autorag_research.config.ExecutorConfig","title":"<code>autorag_research.config.ExecutorConfig</code>  <code>dataclass</code>","text":"<p>Configuration for the Executor.</p> <p>Attributes:</p> Name Type Description <code>pipelines</code> <code>list[BasePipelineConfig]</code> <p>List of pipeline configurations to run.</p> <code>metrics</code> <code>list[BaseMetricConfig]</code> <p>List of metric configurations to evaluate.</p> <code>max_retries</code> <code>int</code> <p>Maximum number of retry attempts for failed pipelines.</p> <code>eval_batch_size</code> <code>int</code> <p>Batch size for metric evaluation.</p> Example <pre><code>config = ExecutorConfig(\n    pipelines=[\n        BM25PipelineConfig(name=\"bm25_v1\", tokenizer=\"bert\"),\n    ],\n    metrics=[\n        RecallConfig(),\n        NDCGConfig(),\n    ],\n    max_retries=3,\n)\n</code></pre>"},{"location":"reference/#autorag_research.config.BasePipelineConfig","title":"<code>autorag_research.config.BasePipelineConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base configuration for all pipelines.</p> <p>Subclasses should define their specific configuration parameters as dataclass fields and implement the abstract methods.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Unique name for this pipeline instance.</p> <code>description</code> <code>str</code> <p>Optional description of the pipeline.</p> <code>pipeline_type</code> <code>PipelineType</code> <p>Type of pipeline (RETRIEVAL or GENERATION).</p> <code>top_k</code> <code>int</code> <p>Number of results to retrieve per query. Default: 10.</p> <code>batch_size</code> <code>int</code> <p>Number of queries to fetch from DB at once. Default: 128.</p> <code>max_concurrency</code> <code>int</code> <p>Maximum concurrent async operations (semaphore limit). Default: 16.</p> <code>max_retries</code> <code>int</code> <p>Maximum retry attempts for failed queries (uses tenacity). Default: 3.</p> <code>retry_delay</code> <code>float</code> <p>Base delay in seconds for exponential backoff between retries. Default: 1.0.</p> Example <pre><code>@dataclass\nclass BM25PipelineConfig(BasePipelineConfig):\n    tokenizer: str = \"bert\"\n    index_name: str = \"idx_chunk_bm25\"\n    pipeline_type: PipelineType = field(default=PipelineType.RETRIEVAL, init=False)\n\n    def get_pipeline_class(self) -&gt; Type:\n        from autorag_research.pipelines.retrieval.bm25 import BM25RetrievalPipeline\n        return BM25RetrievalPipeline\n\n    def get_pipeline_kwargs(self) -&gt; dict[str, Any]:\n        return {\"tokenizer\": self.tokenizer, \"index_name\": self.index_name}\n</code></pre>"},{"location":"reference/#autorag_research.config.BasePipelineConfig.get_pipeline_class","title":"<code>get_pipeline_class()</code>  <code>abstractmethod</code>","text":"<p>Return the pipeline class to instantiate.</p> <p>Returns:</p> Type Description <code>type[BaseRetrievalPipeline]</code> <p>The pipeline class type.</p>"},{"location":"reference/#autorag_research.config.BasePipelineConfig.get_pipeline_kwargs","title":"<code>get_pipeline_kwargs()</code>  <code>abstractmethod</code>","text":"<p>Return kwargs for pipeline constructor.</p> <p>These kwargs are passed to the pipeline constructor along with session_factory, name, and schema (which are handled by Executor).</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary of keyword arguments for the pipeline constructor.</p>"},{"location":"reference/#autorag_research.config.BasePipelineConfig.get_run_kwargs","title":"<code>get_run_kwargs()</code>  <code>abstractmethod</code>","text":"<p>Return kwargs for pipeline.run() method.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary of keyword arguments for the run method.</p>"},{"location":"reference/#autorag_research.config.BaseMetricConfig","title":"<code>autorag_research.config.BaseMetricConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base configuration for all metrics.</p> <p>Subclasses should define their specific configuration parameters as dataclass fields and implement the abstract methods.</p> <p>Attributes:</p> Name Type Description <code>metric_type</code> <code>MetricType</code> <p>Type of metric (RETRIEVAL or GENERATION).</p> Example <pre><code>@dataclass\nclass RecallConfig(BaseMetricConfig):\n    metric_type: MetricType = field(default=MetricType.RETRIEVAL, init=False)\n\n    def get_metric_name(self) -&gt; str:\n        return \"retrieval_recall\"\n\n    def get_metric_func(self) -&gt; Callable:\n        from autorag_research.evaluation.metrics import retrieval_recall\n        return retrieval_recall\n</code></pre>"},{"location":"reference/#autorag_research.config.BaseMetricConfig.get_metric_func","title":"<code>get_metric_func()</code>  <code>abstractmethod</code>","text":"<p>Return the metric function.</p> <p>Returns:</p> Type Description <code>Callable</code> <p>The callable metric function.</p>"},{"location":"reference/#autorag_research.config.BaseMetricConfig.get_metric_kwargs","title":"<code>get_metric_kwargs()</code>","text":"<p>Return optional kwargs for the metric function.</p> <p>Override this method if the metric function accepts additional arguments.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary of keyword arguments for the metric function.</p>"},{"location":"reference/#autorag_research.config.BaseMetricConfig.get_metric_name","title":"<code>get_metric_name()</code>","text":"<p>Return the metric name for database storage.</p> <p>Returns:</p> Type Description <code>str</code> <p>The metric name string.</p>"},{"location":"reference/#executor","title":"Executor","text":""},{"location":"reference/#autorag_research.executor.Executor","title":"<code>autorag_research.executor.Executor</code>","text":"<p>Orchestrates pipeline execution and metric evaluation.</p> <p>The Executor coordinates: 1. Sequential execution of configured pipelines 2. Verification that all queries have results 3. Retry logic for failed pipelines 4. Metric evaluation for each pipeline (immediately after pipeline completes)</p> <p>Metric Evaluation Rules: - Retrieval pipelines: Only retrieval metrics are evaluated - Generation pipelines: Both retrieval AND generation metrics are evaluated</p> Example <pre><code>from autorag_research.config import ExecutorConfig\nfrom autorag_research.executor import Executor\nfrom autorag_research.orm.connection import DBConnection\nfrom autorag_research.pipelines.retrieval.bm25 import BM25PipelineConfig\nfrom autorag_research.evaluation.metrics.retrieval import RecallConfig, NDCGConfig\n\ndb = DBConnection.from_config()  # or DBConnection.from_env()\nsession_factory = db.get_session_factory()\n\nconfig = ExecutorConfig(\n    pipelines=[\n        BM25PipelineConfig(\n            name=\"bm25_baseline\",\n            tokenizer=\"bert\",\n            top_k=10,\n        ),\n    ],\n    metrics=[\n        RecallConfig(),\n        NDCGConfig(),\n    ],\n    max_retries=3,\n)\n\nexecutor = Executor(session_factory, config)\nresult = executor.run()\n</code></pre>"},{"location":"reference/#autorag_research.executor.Executor.__init__","title":"<code>__init__(session_factory, config, schema=None, config_dir=None)</code>","text":"<p>Initialize Executor.</p> <p>Parameters:</p> Name Type Description Default <code>session_factory</code> <code>sessionmaker[Session]</code> <p>SQLAlchemy sessionmaker for database connections.</p> required <code>config</code> <code>ExecutorConfig</code> <p>Executor configuration.</p> required <code>schema</code> <code>Any | None</code> <p>Schema namespace from create_schema(). If None, uses default schema.</p> <code>None</code> <code>config_dir</code> <code>Path | None</code> <p>Directory containing pipeline YAML configs. If None, attempts to use Hydra's config path if initialized, otherwise falls back to CWD/configs.</p> <code>None</code>"},{"location":"reference/#autorag_research.executor.Executor.run","title":"<code>run()</code>","text":"<p>Run all configured pipelines and evaluate metrics.</p> <p>For each pipeline: 1. Run the pipeline with retry logic 2. Verify completion 3. Evaluate applicable metrics (before moving to next pipeline)</p> <p>Returns:</p> Type Description <code>ExecutorResult</code> <p>ExecutorResult with comprehensive execution statistics.</p>"},{"location":"reference/#pipelines","title":"Pipelines","text":""},{"location":"reference/#retrieval","title":"Retrieval","text":""},{"location":"reference/#autorag_research.pipelines.retrieval.base.BaseRetrievalPipeline","title":"<code>autorag_research.pipelines.retrieval.base.BaseRetrievalPipeline</code>","text":"<p>               Bases: <code>BasePipeline</code>, <code>ABC</code></p> <p>Abstract base class for all retrieval pipelines.</p> <p>This class provides common functionality for retrieval pipelines: - Service initialization - Pipeline creation in database - Abstract retrieve methods for subclasses to implement</p> <p>Subclasses must implement: - <code>_retrieve_by_id()</code>: Async method for retrieval using query ID (query exists in DB) - <code>_retrieve_by_text()</code>: Async method for retrieval using raw query text - <code>_get_pipeline_config()</code>: Return the pipeline configuration dict</p>"},{"location":"reference/#autorag_research.pipelines.retrieval.base.BaseRetrievalPipeline.__init__","title":"<code>__init__(session_factory, name, schema=None)</code>","text":"<p>Initialize retrieval pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>session_factory</code> <code>sessionmaker[Session]</code> <p>SQLAlchemy sessionmaker for database connections.</p> required <code>name</code> <code>str</code> <p>Name for this pipeline.</p> required <code>schema</code> <code>Any | None</code> <p>Schema namespace from create_schema(). If None, uses default schema.</p> <code>None</code>"},{"location":"reference/#autorag_research.pipelines.retrieval.base.BaseRetrievalPipeline.retrieve","title":"<code>retrieve(query_text, top_k=10)</code>  <code>async</code>","text":"<p>Retrieve chunks for a single query (async).</p> <p>This method provides single-query retrieval, designed for use within GenerationPipeline where queries are processed one at a time.</p> <p>Checks if query exists in DB: - If exists: uses _retrieve_by_id() (faster, uses stored embedding) - If not: uses _retrieve_by_text() (may trigger embedding computation)</p> <p>Parameters:</p> Name Type Description Default <code>query_text</code> <code>str</code> <p>The query text to retrieve for.</p> required <code>top_k</code> <code>int</code> <p>Number of chunks to retrieve.</p> <code>10</code> <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>List of dicts with 'doc_id' (chunk ID) and 'score' keys.</p>"},{"location":"reference/#autorag_research.pipelines.retrieval.base.BaseRetrievalPipeline.run","title":"<code>run(top_k=10, batch_size=128, max_concurrency=16, max_retries=3, retry_delay=1.0)</code>","text":"<p>Run the retrieval pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>top_k</code> <code>int</code> <p>Number of top documents to retrieve per query.</p> <code>10</code> <code>batch_size</code> <code>int</code> <p>Number of queries to fetch from DB at once.</p> <code>128</code> <code>max_concurrency</code> <code>int</code> <p>Maximum number of concurrent async operations.</p> <code>16</code> <code>max_retries</code> <code>int</code> <p>Maximum number of retry attempts for failed queries.</p> <code>3</code> <code>retry_delay</code> <code>float</code> <p>Base delay in seconds for exponential backoff between retries.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with pipeline execution statistics:</p> <code>dict[str, Any]</code> <ul> <li>pipeline_id: The pipeline ID</li> </ul> <code>dict[str, Any]</code> <ul> <li>total_queries: Number of queries processed successfully</li> </ul> <code>dict[str, Any]</code> <ul> <li>total_results: Number of results stored</li> </ul> <code>dict[str, Any]</code> <ul> <li>failed_queries: List of query IDs that failed after all retries</li> </ul>"},{"location":"reference/#autorag_research.pipelines.retrieval.bm25.BM25RetrievalPipeline","title":"<code>autorag_research.pipelines.retrieval.bm25.BM25RetrievalPipeline</code>","text":"<p>               Bases: <code>BaseRetrievalPipeline</code></p> <p>Pipeline for running VectorChord-BM25 retrieval.</p> <p>This pipeline wraps RetrievalPipelineService with BM25DBModule, providing a convenient interface for BM25-based retrieval using PostgreSQL's VectorChord-BM25 extension.</p> <p>BM25 does not require embeddings, so both _retrieve_by_id() and _retrieve_by_text() work without any embedding model.</p> Example <pre><code>from autorag_research.orm.connection import DBConnection\nfrom autorag_research.pipelines.retrieval.bm25 import BM25RetrievalPipeline\n\ndb = DBConnection.from_config()  # or DBConnection.from_env()\nsession_factory = db.get_session_factory()\n\n# Initialize pipeline\npipeline = BM25RetrievalPipeline(\n    session_factory=session_factory,\n    name=\"bm25_baseline\",\n    tokenizer=\"bert\",\n)\n\n# Run pipeline on all queries in DB\nresults = pipeline.run(top_k=10)\n\n# Or retrieve for a single query\nchunks = await pipeline.retrieve(\"What is machine learning?\", top_k=10)\n</code></pre>"},{"location":"reference/#autorag_research.pipelines.retrieval.bm25.BM25RetrievalPipeline.__init__","title":"<code>__init__(session_factory, name, tokenizer='bert', index_name=DEFAULT_BM25_INDEX_NAME, schema=None)</code>","text":"<p>Initialize BM25 retrieval pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>session_factory</code> <code>sessionmaker[Session]</code> <p>SQLAlchemy sessionmaker for database connections.</p> required <code>name</code> <code>str</code> <p>Name for this pipeline.</p> required <code>tokenizer</code> <code>str</code> <p>Tokenizer name for BM25 (default: \"bert\" for bert_base_uncased). Available tokenizers (pg_tokenizer pre-built models):     - \"bert\": bert-base-uncased (Hugging Face) - Default     - \"wiki_tocken\": Wikitext-103 trained model     - \"gemma2b\": Google lightweight model (~100MB memory)     - \"llmlingua2\": Microsoft summarization model (~200MB memory) See: https://github.com/tensorchord/pg_tokenizer.rs/blob/main/docs/06-model.md</p> <code>'bert'</code> <code>index_name</code> <code>str</code> <p>Name of the BM25 index (default: \"idx_chunk_bm25\").</p> <code>DEFAULT_BM25_INDEX_NAME</code> <code>schema</code> <code>Any | None</code> <p>Schema namespace from create_schema(). If None, uses default schema.</p> <code>None</code>"},{"location":"reference/#generation","title":"Generation","text":""},{"location":"reference/#autorag_research.pipelines.generation.base.BaseGenerationPipeline","title":"<code>autorag_research.pipelines.generation.base.BaseGenerationPipeline</code>","text":"<p>               Bases: <code>BasePipeline</code>, <code>ABC</code></p> <p>Abstract base class for all generation pipelines.</p> <p>This class provides common functionality for generation pipelines: - Composition with a retrieval pipeline for flexible retrieval strategies - Service initialization for database operations - Pipeline creation in database - Abstract generate method for subclasses to implement</p> <p>Subclasses must implement: - <code>_generate()</code>: Async generate method given a query ID (has access to a retrieval pipeline) - <code>_get_pipeline_config()</code>: Return the pipeline configuration dict</p> Example <pre><code>class BasicRAGPipeline(BaseGenerationPipeline):\n    async def _generate(self, query_id: int | str, top_k: int) -&gt; GenerationResult:\n        # Retrieve relevant chunks by query_id (async)\n        results = await self._retrieval_pipeline._retrieve_by_id(query_id, top_k)\n        chunk_ids = [r[\"doc_id\"] for r in results]\n        chunk_contents = self._service.get_chunk_contents(chunk_ids)\n\n        # Retrieve relevant chunks (async)\n        results = await self._retrieval_pipeline.retrieve(query_text, top_k)\n        chunks = [self.get_chunk_content(r[\"doc_id\"]) for r in results]\n        # Get query text (uses query_to_llm if available, else contents)\n        query_text = self._get_query_text(query_id)\n\n        # Build prompt and generate (async)\n        context = \"\\n\\n\".join(chunk_contents)\n        prompt = f\"Context:\\n{context}\\n\\nQuestion: {query_text}\\n\\nAnswer:\"\n        response = await self._llm.ainvoke(prompt)\n\n        return GenerationResult(text=str(response.content))\n</code></pre>"},{"location":"reference/#autorag_research.pipelines.generation.base.BaseGenerationPipeline.__init__","title":"<code>__init__(session_factory, name, llm, retrieval_pipeline, schema=None)</code>","text":"<p>Initialize generation pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>session_factory</code> <code>sessionmaker[Session]</code> <p>SQLAlchemy sessionmaker for database connections.</p> required <code>name</code> <code>str</code> <p>Name for this pipeline.</p> required <code>llm</code> <code>BaseLanguageModel</code> <p>LangChain BaseLanguageModel instance for text generation.</p> required <code>retrieval_pipeline</code> <code>BaseRetrievalPipeline</code> <p>Retrieval pipeline instance for fetching relevant context.</p> required <code>schema</code> <code>Any | None</code> <p>Schema namespace from create_schema(). If None, uses default schema.</p> <code>None</code>"},{"location":"reference/#autorag_research.pipelines.generation.base.BaseGenerationPipeline.run","title":"<code>run(top_k=10, batch_size=128, max_concurrency=16, max_retries=3, retry_delay=1.0)</code>","text":"<p>Run the generation pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>top_k</code> <code>int</code> <p>Number of top documents to retrieve per query.</p> <code>10</code> <code>batch_size</code> <code>int</code> <p>Number of queries to fetch from DB at once.</p> <code>128</code> <code>max_concurrency</code> <code>int</code> <p>Maximum number of concurrent async operations.</p> <code>16</code> <code>max_retries</code> <code>int</code> <p>Maximum number of retry attempts for failed queries.</p> <code>3</code> <code>retry_delay</code> <code>float</code> <p>Base delay in seconds for exponential backoff between retries.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with pipeline execution statistics:</p> <code>dict[str, Any]</code> <ul> <li>pipeline_id: The pipeline ID</li> </ul> <code>dict[str, Any]</code> <ul> <li>total_queries: Number of queries processed successfully</li> </ul> <code>dict[str, Any]</code> <ul> <li>total_tokens: Total tokens used (if available)</li> </ul> <code>dict[str, Any]</code> <ul> <li>avg_execution_time_ms: Average execution time per query</li> </ul> <code>dict[str, Any]</code> <ul> <li>failed_queries: List of query IDs that failed after all retries</li> </ul>"},{"location":"reference/#autorag_research.pipelines.generation.basic_rag.BasicRAGPipeline","title":"<code>autorag_research.pipelines.generation.basic_rag.BasicRAGPipeline</code>","text":"<p>               Bases: <code>BaseGenerationPipeline</code></p> <p>Simple single-call RAG pipeline: retrieve once -&gt; build prompt -&gt; generate once.</p> <p>This pipeline implements the most basic RAG pattern: 1. Take a query 2. Retrieve relevant chunks using the composed retrieval pipeline 3. Build a prompt with retrieved context 4. Call LLM once to generate the answer</p> <p>The retrieval pipeline can be any BaseRetrievalPipeline implementation (BM25, vector search, hybrid, HyDE, etc.), providing flexibility in the retrieval strategy while keeping the generation simple.</p> Example <pre><code>from langchain_openai import ChatOpenAI\n\nfrom autorag_research.orm.connection import DBConnection\nfrom autorag_research.pipelines.generation.basic_rag import BasicRAGPipeline\nfrom autorag_research.pipelines.retrieval.bm25 import BM25RetrievalPipeline\n\ndb = DBConnection.from_config()  # or DBConnection.from_env()\nsession_factory = db.get_session_factory()\n\n# Create retrieval pipeline\nretrieval_pipeline = BM25RetrievalPipeline(\n    session_factory=session_factory,\n    name=\"bm25_baseline\",\n    tokenizer=\"bert\",\n)\n\n# Create generation pipeline\npipeline = BasicRAGPipeline(\n    session_factory=session_factory,\n    name=\"basic_rag_v1\",\n    llm=ChatOpenAI(model=\"gpt-4\"),\n    retrieval_pipeline=retrieval_pipeline,\n)\n\n# Run pipeline\nresults = pipeline.run(top_k=5)\n</code></pre>"},{"location":"reference/#autorag_research.pipelines.generation.basic_rag.BasicRAGPipeline.__init__","title":"<code>__init__(session_factory, name, llm, retrieval_pipeline, prompt_template=DEFAULT_PROMPT_TEMPLATE, schema=None)</code>","text":"<p>Initialize Basic RAG pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>session_factory</code> <code>sessionmaker[Session]</code> <p>SQLAlchemy sessionmaker for database connections.</p> required <code>name</code> <code>str</code> <p>Name for this pipeline.</p> required <code>llm</code> <code>BaseLanguageModel</code> <p>LangChain BaseLanguageModel instance for text generation.</p> required <code>retrieval_pipeline</code> <code>BaseRetrievalPipeline</code> <p>Retrieval pipeline for fetching relevant context.</p> required <code>prompt_template</code> <code>str</code> <p>Template string with {context} and {query} placeholders.</p> <code>DEFAULT_PROMPT_TEMPLATE</code> <code>schema</code> <code>Any | None</code> <p>Schema namespace from create_schema(). If None, uses default schema.</p> <code>None</code>"},{"location":"reference/#metrics","title":"Metrics","text":""},{"location":"reference/#retrieval_1","title":"Retrieval","text":""},{"location":"reference/#autorag_research.evaluation.metrics.retrieval","title":"<code>autorag_research.evaluation.metrics.retrieval</code>","text":""},{"location":"reference/#autorag_research.evaluation.metrics.retrieval.F1Config","title":"<code>F1Config</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseRetrievalMetricConfig</code></p> <p>Configuration for retrieval F1 metric.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.retrieval.F1Config.get_metric_func","title":"<code>get_metric_func()</code>","text":"<p>Return the metric function.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.retrieval.FullRecallConfig","title":"<code>FullRecallConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseRetrievalMetricConfig</code></p> <p>Configuration for retrieval full recall metric.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.retrieval.FullRecallConfig.get_metric_func","title":"<code>get_metric_func()</code>","text":"<p>Return the metric function.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.retrieval.MAPConfig","title":"<code>MAPConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseRetrievalMetricConfig</code></p> <p>Configuration for retrieval MAP metric.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.retrieval.MAPConfig.get_metric_func","title":"<code>get_metric_func()</code>","text":"<p>Return the metric function.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.retrieval.MRRConfig","title":"<code>MRRConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseRetrievalMetricConfig</code></p> <p>Configuration for retrieval MRR metric.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.retrieval.MRRConfig.get_metric_func","title":"<code>get_metric_func()</code>","text":"<p>Return the metric function.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.retrieval.NDCGConfig","title":"<code>NDCGConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseRetrievalMetricConfig</code></p> <p>Configuration for retrieval NDCG metric.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.retrieval.NDCGConfig.get_metric_func","title":"<code>get_metric_func()</code>","text":"<p>Return the metric function.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.retrieval.PrecisionConfig","title":"<code>PrecisionConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseRetrievalMetricConfig</code></p> <p>Configuration for retrieval precision metric.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.retrieval.PrecisionConfig.get_metric_func","title":"<code>get_metric_func()</code>","text":"<p>Return the metric function.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.retrieval.RecallConfig","title":"<code>RecallConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseRetrievalMetricConfig</code></p> <p>Configuration for retrieval recall metric.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.retrieval.RecallConfig.get_metric_func","title":"<code>get_metric_func()</code>","text":"<p>Return the metric function.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.retrieval.retrieval_f1","title":"<code>retrieval_f1(metric_input)</code>","text":"<p>Compute f1 score for retrieval.</p> <p>Parameters:</p> Name Type Description Default <code>metric_input</code> <code>MetricInput</code> <p>The MetricInput schema for AutoRAG metric.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The f1 score.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.retrieval.retrieval_full_recall","title":"<code>retrieval_full_recall(metric_input)</code>","text":"<p>Compute full recall (binary) for retrieval.</p> <p>Returns 1.0 if ALL ground truth groups are satisfied (at least one item from each OR-group is retrieved), 0.0 otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>metric_input</code> <code>MetricInput</code> <p>The MetricInput schema for AutoRAG metric.</p> required <p>Returns:</p> Type Description <code>float</code> <p>1.0 if all GT groups are satisfied, 0.0 otherwise.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.retrieval.retrieval_map","title":"<code>retrieval_map(metric_input)</code>","text":"<p>Compute MAP (Mean Average Precision) score for retrieval.</p> <p>Mean Average Precision (MAP) is the mean of Average Precision (AP) for all queries.</p> <p>Parameters:</p> Name Type Description Default <code>metric_input</code> <code>MetricInput</code> <p>The MetricInput schema for AutoRAG metric.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The MAP score.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.retrieval.retrieval_mrr","title":"<code>retrieval_mrr(metric_input)</code>","text":"<p>Compute MRR (Mean Reciprocal Rank) score for retrieval.</p> <p>Reciprocal Rank (RR) is the reciprocal of the rank of the first relevant item. Mean of RR in whole queries is MRR.</p> <p>Parameters:</p> Name Type Description Default <code>metric_input</code> <code>MetricInput</code> <p>The MetricInput schema for AutoRAG metric.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The MRR score.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.retrieval.retrieval_ndcg","title":"<code>retrieval_ndcg(metric_input)</code>","text":"<p>Compute NDCG for multi-hop retrieval with AND-OR group semantics.</p> <p>Ground truth structure: [[A, B], [C]] means (A OR B) AND C - Each inner list is an OR group (any item satisfies the group) - Outer list is AND (all groups must be satisfied for complete retrieval)</p> <p>A retrieved item contributes to DCG only when it's the FIRST to satisfy a previously unsatisfied group. Subsequent items from the same group don't add value (they're redundant for answering the query).</p> <p>Supports graded relevance when <code>metric_input.relevance_scores</code> is provided. Falls back to binary relevance (score=1) when relevance_scores is None.</p> <p>Parameters:</p> Name Type Description Default <code>metric_input</code> <code>MetricInput</code> <p>The MetricInput schema for AutoRAG metric. - retrieval_gt: 2D list of ground truth IDs (AND/OR structure) - retrieved_ids: list of retrieved IDs - relevance_scores: optional dict mapping doc_id -&gt; graded relevance score   (e.g., 0=not relevant, 1=somewhat relevant, 2=highly relevant)</p> required <p>Returns:</p> Type Description <code>float</code> <p>The NDCG score.</p> <p>Examples:</p> <p>GT: [[A, B], [C]] (need A-or-B AND C) Retrieved: [A, C] -&gt; Perfect (both groups satisfied at top positions) Retrieved: [A, B] -&gt; Partial (group 1 not satisfied, B is redundant) Retrieved: [C, A] -&gt; Good but suboptimal ordering</p>"},{"location":"reference/#autorag_research.evaluation.metrics.retrieval.retrieval_precision","title":"<code>retrieval_precision(metric_input)</code>","text":"<p>Compute precision score for retrieval.</p> <p>Parameters:</p> Name Type Description Default <code>metric_input</code> <code>MetricInput</code> <p>The MetricInput schema for AutoRAG metric.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The precision score.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.retrieval.retrieval_recall","title":"<code>retrieval_recall(metric_input)</code>","text":"<p>Compute recall score for retrieval.</p> <p>Parameters:</p> Name Type Description Default <code>metric_input</code> <code>MetricInput</code> <p>The MetricInput schema for AutoRAG metric.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The recall score.</p>"},{"location":"reference/#generation_1","title":"Generation","text":""},{"location":"reference/#autorag_research.evaluation.metrics.generation","title":"<code>autorag_research.evaluation.metrics.generation</code>","text":""},{"location":"reference/#autorag_research.evaluation.metrics.generation.BertScoreConfig","title":"<code>BertScoreConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseGenerationMetricConfig</code></p> <p>Configuration for BERTScore metric.</p> <p>Attributes:</p> Name Type Description <code>lang</code> <code>str</code> <p>Language code for the text.</p> <code>batch</code> <code>int</code> <p>Batch size for processing.</p> <code>n_threads</code> <code>int | None</code> <p>Number of threads to use.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.generation.BertScoreConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Set default n_threads if not provided.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.generation.BertScoreConfig.get_metric_func","title":"<code>get_metric_func()</code>","text":"<p>Return the metric function.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.generation.BertScoreConfig.get_metric_kwargs","title":"<code>get_metric_kwargs()</code>","text":"<p>Return kwargs for the metric function.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.generation.BleuConfig","title":"<code>BleuConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseGenerationMetricConfig</code></p> <p>Configuration for BLEU metric.</p> <p>Attributes:</p> Name Type Description <code>tokenize</code> <code>str | None</code> <p>The tokenizer to use. If None, defaults to language-specific tokenizers.</p> <code>smooth_method</code> <code>str</code> <p>The smoothing method ('floor', 'add-k', 'exp' or 'none').</p> <code>smooth_value</code> <code>float | None</code> <p>The smoothing value for 'floor' and 'add-k' methods.</p> <code>max_ngram_order</code> <code>int</code> <p>Maximum n-gram order when computing precisions.</p> <code>effective_order</code> <code>bool</code> <p>Stop including n-gram orders for which precision is 0.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.generation.BleuConfig.get_metric_func","title":"<code>get_metric_func()</code>","text":"<p>Return the metric function.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.generation.BleuConfig.get_metric_kwargs","title":"<code>get_metric_kwargs()</code>","text":"<p>Return kwargs for the metric function.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.generation.MeteorConfig","title":"<code>MeteorConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseGenerationMetricConfig</code></p> <p>Configuration for METEOR metric.</p> <p>Attributes:</p> Name Type Description <code>alpha</code> <code>float</code> <p>Parameter for controlling relative weights of precision and recall.</p> <code>beta</code> <code>float</code> <p>Parameter for controlling shape of penalty as a function of fragmentation.</p> <code>gamma</code> <code>float</code> <p>Relative weight assigned to fragmentation penalty.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.generation.MeteorConfig.get_metric_func","title":"<code>get_metric_func()</code>","text":"<p>Return the metric function.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.generation.MeteorConfig.get_metric_kwargs","title":"<code>get_metric_kwargs()</code>","text":"<p>Return kwargs for the metric function.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.generation.RougeConfig","title":"<code>RougeConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseGenerationMetricConfig</code></p> <p>Configuration for ROUGE metric.</p> <p>Attributes:</p> Name Type Description <code>rouge_type</code> <code>str</code> <p>Rouge type to use ('rouge1', 'rouge2', 'rougeL', 'rougeLSum').</p> <code>use_stemmer</code> <code>bool</code> <p>Whether to use Porter stemmer for word suffix stripping.</p> <code>split_summaries</code> <code>bool</code> <p>Whether to add newlines between sentences for rougeLsum.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.generation.RougeConfig.get_metric_func","title":"<code>get_metric_func()</code>","text":"<p>Return the metric function.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.generation.RougeConfig.get_metric_kwargs","title":"<code>get_metric_kwargs()</code>","text":"<p>Return kwargs for the metric function.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.generation.RougeConfig.get_metric_name","title":"<code>get_metric_name()</code>","text":"<p>Return the metric name.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.generation.SemScoreConfig","title":"<code>SemScoreConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseGenerationMetricConfig</code></p> <p>Configuration for SemScore (semantic similarity) metric.</p> <p>Attributes:</p> Name Type Description <code>embedding_model</code> <code>Embeddings | str</code> <p>Embedding model config name (e.g., \"openai-large\") or Embeddings instance.</p> <code>truncate_length</code> <code>int</code> <p>Maximum length of texts to embed.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.generation.SemScoreConfig.get_metric_func","title":"<code>get_metric_func()</code>","text":"<p>Return the metric function.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.generation.SemScoreConfig.get_metric_kwargs","title":"<code>get_metric_kwargs()</code>","text":"<p>Return kwargs for the metric function.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.generation.bert_score","title":"<code>bert_score(metric_inputs, lang='en', batch=128, n_threads=os.cpu_count())</code>","text":"<p>Compute BERTScore metric for generation.</p> <p>Parameters:</p> Name Type Description Default <code>metric_inputs</code> <code>list[MetricInput]</code> <p>A list of MetricInput schema (Required Field -&gt; \"generation_gt\", \"generated_texts\").</p> required <code>lang</code> <code>str</code> <p>Language code for the text. Default is \"en\".</p> <code>'en'</code> <code>batch</code> <code>int</code> <p>Batch size for processing. Default is 128.</p> <code>128</code> <code>n_threads</code> <code>int | None</code> <p>Number of threads to use. Default is the number of CPU cores.</p> <code>cpu_count()</code> <p>Returns:</p> Type Description <code>list[float]</code> <p>A list of BERTScore F1 scores.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.generation.bleu","title":"<code>bleu(metric_inputs, tokenize=None, smooth_method='exp', smooth_value=None, max_ngram_order=4, trg_lang='', effective_order=True, **kwargs)</code>","text":"<p>Computes the BLEU metric given pred and ground-truth.</p> <p>Parameters:</p> Name Type Description Default <code>metric_inputs</code> <code>list[MetricInput]</code> <p>A list of MetricInput schema (Required Field -&gt; \"generation_gt\", \"generated_texts\").</p> required <code>tokenize</code> <code>str | None</code> <p>The tokenizer to use. If None, defaults to language-specific tokenizers with '13a' as the fallback default. check https://github.com/mjpost/sacrebleu/blob/master/sacrebleu/metrics/bleu.py</p> <code>None</code> <code>smooth_method</code> <code>str</code> <p>The smoothing method to use ('floor', 'add-k', 'exp' or 'none').</p> <code>'exp'</code> <code>smooth_value</code> <code>float | None</code> <p>The smoothing value for <code>floor</code> and <code>add-k</code> methods. <code>None</code> falls back to default value.</p> <code>None</code> <code>max_ngram_order</code> <code>int</code> <p>If given, it overrides the maximum n-gram order (default: 4) when computing precisions.</p> <code>4</code> <code>trg_lang</code> <code>str</code> <p>An optional language code to raise potential tokenizer warnings.</p> <code>''</code> <code>effective_order</code> <code>bool</code> <p>If <code>True</code>, stop including n-gram orders for which precision is 0. This should be <code>True</code>, if sentence-level BLEU will be computed.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[float]</code> <p>A list of BLEU scores.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.generation.huggingface_evaluate","title":"<code>huggingface_evaluate(instance, key, metric_inputs, **kwargs)</code>","text":"<p>Compute huggingface evaluate metric.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>Any</code> <p>The instance of huggingface evaluates metric.</p> required <code>key</code> <code>str</code> <p>The key to retrieve result score from huggingface evaluate result.</p> required <code>metric_inputs</code> <code>list[MetricInput]</code> <p>A list of MetricInput schema.</p> required <code>**kwargs</code> <code>Any</code> <p>The additional arguments for metric function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[float]</code> <p>The list of scores.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.generation.meteor","title":"<code>meteor(metric_inputs, alpha=0.9, beta=3.0, gamma=0.5)</code>","text":"<p>Compute meteor score for generation.</p> <p>Parameters:</p> Name Type Description Default <code>metric_inputs</code> <code>list[MetricInput]</code> <p>A list of MetricInput schema (Required Field -&gt; \"generation_gt\", \"generated_texts\").</p> required <code>alpha</code> <code>float</code> <p>Parameter for controlling relative weights of precision and recall. Default is 0.9.</p> <code>0.9</code> <code>beta</code> <code>float</code> <p>Parameter for controlling shape of penalty as a function of as a function of fragmentation. Default is 3.0.</p> <code>3.0</code> <code>gamma</code> <code>float</code> <p>Relative weight assigned to fragmentation penalty. Default is 0.5.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>list[float]</code> <p>A list of computed metric scores.</p>"},{"location":"reference/#autorag_research.evaluation.metrics.generation.rouge","title":"<code>rouge(metric_inputs, rouge_type='rougeL', use_stemmer=False, split_summaries=False)</code>","text":"<p>Compute rouge score for generation.</p> <pre><code>Args:\n    metric_inputs: A list of MetricInput schema (Required Field -&gt; \"generation_gt\", \"generated_texts\").\n    rouge_type: A rouge type to use for evaluation. Default is 'RougeL'.\n        Choose between rouge1, rouge2, rougeL, and rougeLSum.\n        - rouge1: unigram (1-gram) based scoring.\n        - rouge2: bigram (2-gram) based scoring.\n        - rougeL: Longest Common Subsequence based scoring.\n        - rougeLSum: splits text using \"\n</code></pre> <p>\"         use_stemmer: Bool indicating whether Porter stemmer should be used to             strip word suffixes to improve matching. This arg is used in the             DefaultTokenizer, but other tokenizers might or might not choose to             use this. Default is False.         split_summaries: Whether to add newlines between sentences for rougeLsum. Default is False.</p> <pre><code>Returns:\n    A list of computed metric scores.\n</code></pre>"},{"location":"reference/#autorag_research.evaluation.metrics.generation.sem_score","title":"<code>sem_score(metric_inputs, embedding_model, truncate_length=4096)</code>","text":"<p>Compute sem score between generation gt and pred with cosine similarity.</p> <p>Parameters:</p> Name Type Description Default <code>metric_inputs</code> <code>list[MetricInput]</code> <p>A list of MetricInput schema (Required Field -&gt; \"generation_gt\", \"generated_texts\").</p> required <code>embedding_model</code> <code>Embeddings | str</code> <p>Embedding model to use for compute cosine similarity. Can be an Embeddings instance or a string config name (e.g., \"openai-large\").</p> required <code>truncate_length</code> <code>int</code> <p>Maximum length of texts to embedding. Default is 4096.</p> <code>4096</code> <p>Returns:</p> Type Description <code>list[float]</code> <p>A list of computed metric scores.</p>"},{"location":"reference/#data-ingestion","title":"Data Ingestion","text":""},{"location":"reference/#autorag_research.data.base.DataIngestor","title":"<code>autorag_research.data.base.DataIngestor</code>","text":"<p>               Bases: <code>ABC</code></p>"},{"location":"reference/#autorag_research.data.base.DataIngestor.detect_primary_key_type","title":"<code>detect_primary_key_type()</code>  <code>abstractmethod</code>","text":"<p>Detect the primary key type used in the dataset.</p>"},{"location":"reference/#autorag_research.data.base.DataIngestor.ingest","title":"<code>ingest(subset='test', query_limit=None, min_corpus_cnt=None)</code>  <code>abstractmethod</code>","text":"<p>Ingest data from the specified source. This process does not include an embedding process.</p> <p>Parameters:</p> Name Type Description Default <code>subset</code> <code>Literal['train', 'dev', 'test']</code> <p>Dataset split to ingest (train, dev, or test).</p> <code>'test'</code> <code>query_limit</code> <code>int | None</code> <p>Maximum number of queries to ingest. None means no limit.</p> <code>None</code> <code>min_corpus_cnt</code> <code>int | None</code> <p>Maximum number of corpus items to ingest.           When set, gold IDs from selected queries are always included,           plus random samples to reach the limit. None means no limit.</p> <code>None</code>"},{"location":"reference/#autorag_research.data.base.TextEmbeddingDataIngestor","title":"<code>autorag_research.data.base.TextEmbeddingDataIngestor</code>","text":"<p>               Bases: <code>DataIngestor</code>, <code>ABC</code></p>"},{"location":"reference/#autorag_research.data.base.TextEmbeddingDataIngestor.embed_all","title":"<code>embed_all(max_concurrency=16, batch_size=128)</code>","text":"<p>Embed all queries and text chunks.</p>"},{"location":"reference/#autorag_research.data.base.MultiModalEmbeddingDataIngestor","title":"<code>autorag_research.data.base.MultiModalEmbeddingDataIngestor</code>","text":"<p>               Bases: <code>DataIngestor</code>, <code>ABC</code></p>"},{"location":"reference/#autorag_research.data.base.MultiModalEmbeddingDataIngestor.embed_all","title":"<code>embed_all(max_concurrency=16, batch_size=128)</code>","text":"<p>Embed all queries and image chunks using single-vector embedding model.</p>"},{"location":"reference/#autorag_research.data.base.MultiModalEmbeddingDataIngestor.embed_all_late_interaction","title":"<code>embed_all_late_interaction(max_concurrency=16, batch_size=128)</code>","text":"<p>Embed all queries and image chunks using multi-vector embedding model.</p>"},{"location":"reference/#orm","title":"ORM","text":""},{"location":"reference/#repository","title":"Repository","text":""},{"location":"reference/#autorag_research.orm.repository","title":"<code>autorag_research.orm.repository</code>","text":"<p>Repository module for AutoRAG-Research ORM.</p> <p>This module provides repository classes for data access layer operations.</p>"},{"location":"reference/#autorag_research.orm.repository.BaseVectorRepository","title":"<code>BaseVectorRepository</code>","text":"<p>               Bases: <code>GenericRepository[T]</code></p> <p>Base repository with vector search capabilities.</p> <p>Extends GenericRepository with vector search methods for use with pgvector and VectorChord for efficient similarity search.</p>"},{"location":"reference/#autorag_research.orm.repository.BaseVectorRepository.maxsim_search","title":"<code>maxsim_search(query_vectors, vector_column='embeddings', limit=10)</code>","text":"<p>Perform MaxSim search using VectorChord's @# operator.</p> <p>MaxSim computes late interaction similarity: for each query vector, find the closest document vector, compute dot product, and sum results.</p> <p>Parameters:</p> Name Type Description Default <code>query_vectors</code> <code>list[list[float]]</code> <p>List of query embedding vectors (multi-vector query).</p> required <code>vector_column</code> <code>str</code> <p>Name of the multi-vector column to search.</p> <code>'embeddings'</code> <code>limit</code> <code>int</code> <p>Maximum number of results to return.</p> <code>10</code> <p>Returns:</p> Type Description <code>list[tuple[T, float]]</code> <p>List of tuples (entity, distance_score) ordered by similarity.</p> <code>list[tuple[T, float]]</code> <p>Lower distance scores indicate higher similarity.</p> <code>list[tuple[T, float]]</code> <p>The distance score calculated by (1 - maxsim_score), thus the range is [-infinity, 0].</p> <code>list[tuple[T, float]]</code> <p>You might normalize this score by dividing by the number of query vectors.</p> Note <p>Requires VectorChord extension and vchordrq index with vector_maxsim_ops. Example index: CREATE INDEX ON table USING vchordrq (embeddings vector_maxsim_ops);</p>"},{"location":"reference/#autorag_research.orm.repository.BaseVectorRepository.maxsim_search_with_ids","title":"<code>maxsim_search_with_ids(query_vectors, vector_column='embeddings', id_column='id', limit=10)</code>","text":"<p>Perform MaxSim search and return only IDs with scores.</p> <p>This is more efficient when you only need IDs and scores.</p> <p>Parameters:</p> Name Type Description Default <code>query_vectors</code> <code>list[list[float]]</code> <p>List of query embedding vectors (multi-vector query).</p> required <code>vector_column</code> <code>str</code> <p>Name of the multi-vector column to search.</p> <code>'embeddings'</code> <code>id_column</code> <code>str</code> <p>Name of the primary key column.</p> <code>'id'</code> <code>limit</code> <code>int</code> <p>Maximum number of results to return.</p> <code>10</code> <p>Returns:</p> Type Description <code>list[tuple[int | str, float]]</code> <p>List of tuples (entity_id, distance_score) ordered by similarity.</p>"},{"location":"reference/#autorag_research.orm.repository.BaseVectorRepository.set_multi_vector_embedding","title":"<code>set_multi_vector_embedding(entity_id, embeddings, vector_column='embeddings', id_column='id')</code>","text":"<p>Set multi-vector embedding for an entity using raw SQL.</p> <p>This method bypasses SQLAlchemy's type processing to properly format vector arrays for VectorChord compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>int | str</code> <p>The entity's primary key.</p> required <code>embeddings</code> <code>list[list[float]]</code> <p>List of embedding vectors (list of list of floats).</p> required <code>vector_column</code> <code>str</code> <p>Name of the multi-vector column (default: \"embeddings\").</p> <code>'embeddings'</code> <code>id_column</code> <code>str</code> <p>Name of the primary key column (default: \"id\").</p> <code>'id'</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if update was successful, False otherwise.</p>"},{"location":"reference/#autorag_research.orm.repository.BaseVectorRepository.set_multi_vector_embeddings_batch","title":"<code>set_multi_vector_embeddings_batch(entity_ids, embeddings_list, vector_column='embeddings', id_column='id')</code>","text":"<p>Batch set multi-vector embeddings for multiple entities.</p> <p>Parameters:</p> Name Type Description Default <code>entity_ids</code> <code>list[int | str]</code> <p>List of entity primary keys.</p> required <code>embeddings_list</code> <code>list[list[list[float]]]</code> <p>List of multi-vector embeddings (one per entity).</p> required <code>vector_column</code> <code>str</code> <p>Name of the multi-vector column (default: \"embeddings\").</p> <code>'embeddings'</code> <code>id_column</code> <code>str</code> <p>Name of the primary key column (default: \"id\").</p> <code>'id'</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of entities successfully updated.</p>"},{"location":"reference/#autorag_research.orm.repository.BaseVectorRepository.vector_search","title":"<code>vector_search(query_vector, vector_column='embedding', limit=10)</code>","text":"<p>Perform vector similarity search using VectorChord's cosine distance.</p> <p>Uses raw SQL with VectorChord's &lt;=&gt; operator for cosine distance. This approach avoids SQLAlchemy type processing issues with Vector objects.</p> <p>Parameters:</p> Name Type Description Default <code>query_vector</code> <code>list[float]</code> <p>The query embedding vector as a plain Python list of floats.</p> required <code>vector_column</code> <code>str</code> <p>Name of the vector column to search.</p> <code>'embedding'</code> <code>limit</code> <code>int</code> <p>Maximum number of results to return.</p> <code>10</code> <p>Returns:</p> Type Description <code>list[T]</code> <p>List of entities ordered by similarity (most similar first).</p> Note <p>Requires VectorChord extension and vchordrq index on the embedding column. Example index: CREATE INDEX ON table USING vchordrq (embedding vector_cosine_ops);</p>"},{"location":"reference/#autorag_research.orm.repository.BaseVectorRepository.vector_search_with_scores","title":"<code>vector_search_with_scores(query_vector, vector_column='embedding', limit=10)</code>","text":"<p>Perform vector similarity search using VectorChord's cosine distance.</p> <p>Uses raw SQL with VectorChord's &lt;=&gt; operator for cosine distance. This approach avoids SQLAlchemy type processing issues with Vector objects.</p> <p>Parameters:</p> Name Type Description Default <code>query_vector</code> <code>list[float]</code> <p>The query embedding vector as a plain Python list of floats.</p> required <code>vector_column</code> <code>str</code> <p>Name of the vector column to search.</p> <code>'embedding'</code> <code>limit</code> <code>int</code> <p>Maximum number of results to return.</p> <code>10</code> <p>Returns:</p> Type Description <code>list[tuple[T, float]]</code> <p>List of tuples (entity, distance_score) ordered by similarity.</p> <code>list[tuple[T, float]]</code> <p>Lower distance scores indicate higher similarity.</p> <code>list[tuple[T, float]]</code> <p>The score is the cosine distance, which is calculated as (1 - cosine_similarity).</p> <code>list[tuple[T, float]]</code> <p>0 means identical, 2 means opposite, and 1 means orthogonal.</p> Note <p>Requires VectorChord extension and vchordrq index on the embedding column. Example index: CREATE INDEX ON table USING vchordrq (embedding vector_cosine_ops);</p>"},{"location":"reference/#autorag_research.orm.repository.ChunkRepository","title":"<code>ChunkRepository</code>","text":"<p>               Bases: <code>BaseVectorRepository[Any]</code>, <code>BaseEmbeddingRepository[Any]</code></p> <p>Repository for Chunk entity with vector search capabilities.</p>"},{"location":"reference/#autorag_research.orm.repository.ChunkRepository.__init__","title":"<code>__init__(session, model_cls=None)</code>","text":"<p>Initialize chunk repository.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>SQLAlchemy session for database operations.</p> required <code>model_cls</code> <code>type | None</code> <p>The Chunk model class to use. If None, uses default schema.</p> <code>None</code>"},{"location":"reference/#autorag_research.orm.repository.ChunkRepository.bm25_search","title":"<code>bm25_search(query_text, index_name='idx_chunk_bm25', limit=10, tokenizer='bert')</code>","text":"<p>Perform VectorChord BM25 search.</p> <p>Uses VectorChord-BM25's &lt;&amp;&gt; operator for full-text search. Returns entities with their BM25 scores (converted to positive values).</p> <p>Parameters:</p> Name Type Description Default <code>query_text</code> <code>str</code> <p>The query text to search for.</p> required <code>index_name</code> <code>str</code> <p>Name of the BM25 index (default: \"idx_chunk_bm25\").</p> <code>'idx_chunk_bm25'</code> <code>limit</code> <code>int</code> <p>Maximum number of results to return.</p> <code>10</code> <code>tokenizer</code> <code>str</code> <p>Tokenizer to use for query (default: \"bert\"). Available tokenizers (pg_tokenizer pre-built models):     - \"bert\": bert-base-uncased (Hugging Face) - Default     - \"wiki_tocken\": Wikitext-103 trained model     - \"gemma2b\": Google lightweight model (~100MB memory)     - \"llmlingua2\": Microsoft summarization model (~200MB memory) See: https://github.com/tensorchord/pg_tokenizer.rs/blob/main/docs/06-model.md</p> <code>'bert'</code> <p>Returns:</p> Type Description <code>list[tuple[Any, float]]</code> <p>List of tuples (entity, score) ordered by relevance.</p> <code>list[tuple[Any, float]]</code> <p>Higher scores indicate higher relevance.</p> Note <p>BM25 scores from VectorChord are negative (more negative = more relevant). This method negates the scores so higher = more relevant.</p>"},{"location":"reference/#autorag_research.orm.repository.ChunkRepository.get_by_contents_exact","title":"<code>get_by_contents_exact(contents)</code>","text":"<p>Retrieve chunks with exact contents match.</p> <p>Parameters:</p> Name Type Description Default <code>contents</code> <code>str</code> <p>The exact contents to search for.</p> required <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of chunks with matching contents.</p>"},{"location":"reference/#autorag_research.orm.repository.ChunkRepository.get_by_table_type","title":"<code>get_by_table_type(table_type)</code>","text":"<p>Retrieve chunks with a specific table_type.</p> <p>Parameters:</p> Name Type Description Default <code>table_type</code> <code>str</code> <p>The table format type (e.g., 'markdown', 'xml', 'html').</p> required <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of chunks with the specified table_type.</p>"},{"location":"reference/#autorag_research.orm.repository.ChunkRepository.get_chunks_with_empty_content","title":"<code>get_chunks_with_empty_content(limit=None)</code>","text":"<p>Retrieve chunks that have empty or whitespace-only contents.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int | None</code> <p>Maximum number of results to return.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of chunks with empty content.</p>"},{"location":"reference/#autorag_research.orm.repository.ChunkRepository.get_non_table_chunks","title":"<code>get_non_table_chunks()</code>","text":"<p>Retrieve chunks that are not tables (is_table=False).</p> <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of chunks where is_table is False.</p>"},{"location":"reference/#autorag_research.orm.repository.ChunkRepository.get_table_chunks","title":"<code>get_table_chunks()</code>","text":"<p>Retrieve chunks that are tables (is_table=True).</p> <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of chunks where is_table is True.</p>"},{"location":"reference/#autorag_research.orm.repository.ChunkRepository.get_with_all_relations","title":"<code>get_with_all_relations(chunk_id)</code>","text":"<p>Retrieve a chunk with all relationships eagerly loaded.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_id</code> <code>int | str</code> <p>The chunk ID.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The chunk with all relations loaded, None if not found.</p>"},{"location":"reference/#autorag_research.orm.repository.ChunkRepository.get_with_chunk_retrieved_results","title":"<code>get_with_chunk_retrieved_results(chunk_id)</code>","text":"<p>Retrieve a chunk with its chunk retrieved results eagerly loaded.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_id</code> <code>int | str</code> <p>The chunk ID.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The chunk with chunk retrieved results loaded, None if not found.</p>"},{"location":"reference/#autorag_research.orm.repository.ChunkRepository.get_with_page_chunk_relations","title":"<code>get_with_page_chunk_relations(chunk_id)</code>","text":"<p>Retrieve a chunk with its page-chunk relations eagerly loaded.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_id</code> <code>int | str</code> <p>The chunk ID.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The chunk with page-chunk relations loaded, None if not found.</p>"},{"location":"reference/#autorag_research.orm.repository.ChunkRepository.get_with_retrieval_relations","title":"<code>get_with_retrieval_relations(chunk_id)</code>","text":"<p>Retrieve a chunk with its retrieval relations eagerly loaded.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_id</code> <code>int | str</code> <p>The chunk ID.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The chunk with retrieval relations loaded, None if not found.</p>"},{"location":"reference/#autorag_research.orm.repository.ChunkRepository.search_by_contents","title":"<code>search_by_contents(search_text)</code>","text":"<p>Search chunks by contents using SQL LIKE.</p> <p>Parameters:</p> Name Type Description Default <code>search_text</code> <code>str</code> <p>The text to search for (use % as wildcard).</p> required <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of matching chunks.</p>"},{"location":"reference/#autorag_research.orm.repository.ChunkRetrievedResultRepository","title":"<code>ChunkRetrievedResultRepository</code>","text":"<p>               Bases: <code>BaseRetrievedResultRepository</code></p> <p>Repository for ChunkRetrievedResult entity.</p>"},{"location":"reference/#autorag_research.orm.repository.ChunkRetrievedResultRepository.__init__","title":"<code>__init__(session, model_cls=None)</code>","text":"<p>Initialize chunk retrieved result repository.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>SQLAlchemy session for database operations.</p> required <code>model_cls</code> <code>type | None</code> <p>The ChunkRetrievedResult model class to use. If None, uses default schema.</p> <code>None</code>"},{"location":"reference/#autorag_research.orm.repository.DocumentRepository","title":"<code>DocumentRepository</code>","text":"<p>               Bases: <code>GenericRepository</code></p> <p>Repository for Document entity with specialized queries.</p>"},{"location":"reference/#autorag_research.orm.repository.DocumentRepository.__init__","title":"<code>__init__(session, model_cls=None)</code>","text":"<p>Initialize document repository.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>SQLAlchemy session for database operations.</p> required <code>model_cls</code> <code>type | None</code> <p>The Document model class. If None, uses default schema.</p> <code>None</code>"},{"location":"reference/#autorag_research.orm.repository.DocumentRepository.count_pages","title":"<code>count_pages(document_id)</code>","text":"<p>Count the number of pages in a document.</p> <p>Parameters:</p> Name Type Description Default <code>document_id</code> <code>int | str</code> <p>The document ID.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of pages in the document.</p>"},{"location":"reference/#autorag_research.orm.repository.DocumentRepository.get_all_with_pages","title":"<code>get_all_with_pages(limit=None, offset=None)</code>","text":"<p>Retrieve all documents with their pages eagerly loaded.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int | None</code> <p>Maximum number of results to return.</p> <code>None</code> <code>offset</code> <code>int | None</code> <p>Number of results to skip.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of documents with pages loaded.</p>"},{"location":"reference/#autorag_research.orm.repository.DocumentRepository.get_by_author","title":"<code>get_by_author(author)</code>","text":"<p>Retrieve all documents by a specific author.</p> <p>Parameters:</p> Name Type Description Default <code>author</code> <code>str</code> <p>The author name to search for.</p> required <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of documents by the author.</p>"},{"location":"reference/#autorag_research.orm.repository.DocumentRepository.get_by_filename","title":"<code>get_by_filename(filename)</code>","text":"<p>Retrieve a document by its filename.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename to search for.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The document if found, None otherwise.</p>"},{"location":"reference/#autorag_research.orm.repository.DocumentRepository.get_by_path_id","title":"<code>get_by_path_id(path_id)</code>","text":"<p>Retrieve a document by its file path ID.</p> <p>Parameters:</p> Name Type Description Default <code>path_id</code> <code>int | str</code> <p>The file ID to search for.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The document if found, None otherwise.</p>"},{"location":"reference/#autorag_research.orm.repository.DocumentRepository.get_by_title","title":"<code>get_by_title(title)</code>","text":"<p>Retrieve a document by its title.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>The title to search for.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The document if found, None otherwise.</p>"},{"location":"reference/#autorag_research.orm.repository.DocumentRepository.get_with_file","title":"<code>get_with_file(document_id)</code>","text":"<p>Retrieve a document with its file eagerly loaded.</p> <p>Parameters:</p> Name Type Description Default <code>document_id</code> <code>int | str</code> <p>The document ID.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The document with file loaded, None if not found.</p>"},{"location":"reference/#autorag_research.orm.repository.DocumentRepository.get_with_pages","title":"<code>get_with_pages(document_id)</code>","text":"<p>Retrieve a document with its pages eagerly loaded.</p> <p>Parameters:</p> Name Type Description Default <code>document_id</code> <code>int | str</code> <p>The document ID.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The document with pages loaded, None if not found.</p>"},{"location":"reference/#autorag_research.orm.repository.DocumentRepository.search_by_metadata","title":"<code>search_by_metadata(metadata_key, metadata_value)</code>","text":"<p>Search documents by metadata field.</p> <p>Parameters:</p> Name Type Description Default <code>metadata_key</code> <code>str</code> <p>The key in the JSONB metadata field.</p> required <code>metadata_value</code> <code>str</code> <p>The value to search for.</p> required <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of matching documents.</p>"},{"location":"reference/#autorag_research.orm.repository.ExecutorResultRepository","title":"<code>ExecutorResultRepository</code>","text":"<p>               Bases: <code>GenericRepository[Any]</code></p> <p>Repository for ExecutorResult entity with composite key support.</p>"},{"location":"reference/#autorag_research.orm.repository.ExecutorResultRepository.__init__","title":"<code>__init__(session, model_cls=None)</code>","text":"<p>Initialize executor result repository.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>SQLAlchemy session for database operations.</p> required <code>model_cls</code> <code>type | None</code> <p>The ExecutorResult model class to use. If None, uses default schema.</p> <code>None</code>"},{"location":"reference/#autorag_research.orm.repository.ExecutorResultRepository.delete_by_composite_key","title":"<code>delete_by_composite_key(query_id, pipeline_id)</code>","text":"<p>Delete an executor result by its composite primary key.</p> <p>Parameters:</p> Name Type Description Default <code>query_id</code> <code>int | str</code> <p>The query ID.</p> required <code>pipeline_id</code> <code>int | str</code> <p>The pipeline ID.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the result was deleted, False if not found.</p>"},{"location":"reference/#autorag_research.orm.repository.ExecutorResultRepository.delete_by_pipeline","title":"<code>delete_by_pipeline(pipeline_id)</code>","text":"<p>Delete all executor results for a specific pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_id</code> <code>int | str</code> <p>The pipeline ID.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of deleted records.</p>"},{"location":"reference/#autorag_research.orm.repository.ExecutorResultRepository.exists_by_composite_key","title":"<code>exists_by_composite_key(query_id, pipeline_id)</code>","text":"<p>Check if an executor result exists with the given composite key.</p> <p>Parameters:</p> Name Type Description Default <code>query_id</code> <code>int | str</code> <p>The query ID.</p> required <code>pipeline_id</code> <code>int | str</code> <p>The pipeline ID.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the result exists, False otherwise.</p>"},{"location":"reference/#autorag_research.orm.repository.ExecutorResultRepository.get_by_composite_key","title":"<code>get_by_composite_key(query_id, pipeline_id)</code>","text":"<p>Retrieve an executor result by its composite primary key.</p> <p>Parameters:</p> Name Type Description Default <code>query_id</code> <code>int | str</code> <p>The query ID.</p> required <code>pipeline_id</code> <code>int | str</code> <p>The pipeline ID.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The executor result if found, None otherwise.</p>"},{"location":"reference/#autorag_research.orm.repository.ExecutorResultRepository.get_by_execution_time_range","title":"<code>get_by_execution_time_range(pipeline_id, min_time, max_time)</code>","text":"<p>Retrieve executor results within an execution time range.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_id</code> <code>int | str</code> <p>The pipeline ID.</p> required <code>min_time</code> <code>int</code> <p>Minimum execution time (inclusive).</p> required <code>max_time</code> <code>int</code> <p>Maximum execution time (inclusive).</p> required <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of executor results within the specified range.</p>"},{"location":"reference/#autorag_research.orm.repository.ExecutorResultRepository.get_by_pipeline_id","title":"<code>get_by_pipeline_id(pipeline_id)</code>","text":"<p>Retrieve all executor results for a specific pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_id</code> <code>int | str</code> <p>The pipeline ID.</p> required <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of executor results for the pipeline.</p>"},{"location":"reference/#autorag_research.orm.repository.ExecutorResultRepository.get_by_queries_and_pipeline","title":"<code>get_by_queries_and_pipeline(query_ids, pipeline_id)</code>","text":"<p>Retrieve executor results for multiple queries under a specific pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>query_ids</code> <code>list[int | str]</code> <p>List of query IDs.</p> required <code>pipeline_id</code> <code>int | str</code> <p>The pipeline ID.</p> required <p>Returns:     List of executor results matching the criteria.</p>"},{"location":"reference/#autorag_research.orm.repository.ExecutorResultRepository.get_by_query_id","title":"<code>get_by_query_id(query_id)</code>","text":"<p>Retrieve all executor results for a specific query.</p> <p>Parameters:</p> Name Type Description Default <code>query_id</code> <code>int | str</code> <p>The query ID.</p> required <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of executor results for the query.</p>"},{"location":"reference/#autorag_research.orm.repository.ExecutorResultRepository.get_with_all_relations","title":"<code>get_with_all_relations(query_id, pipeline_id)</code>","text":"<p>Retrieve an executor result with all relations eagerly loaded.</p> <p>Parameters:</p> Name Type Description Default <code>query_id</code> <code>int | str</code> <p>The query ID.</p> required <code>pipeline_id</code> <code>int | str</code> <p>The pipeline ID.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The executor result with all relations loaded, None if not found.</p>"},{"location":"reference/#autorag_research.orm.repository.FileRepository","title":"<code>FileRepository</code>","text":"<p>               Bases: <code>GenericRepository</code></p> <p>Repository for File entity with specialized queries.</p>"},{"location":"reference/#autorag_research.orm.repository.FileRepository.__init__","title":"<code>__init__(session, model_cls=None)</code>","text":"<p>Initialize file repository.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>SQLAlchemy session for database operations.</p> required <code>model_cls</code> <code>type | None</code> <p>The File model class. If None, uses default schema.</p> <code>None</code>"},{"location":"reference/#autorag_research.orm.repository.FileRepository.count_by_type","title":"<code>count_by_type(file_type)</code>","text":"<p>Count the number of files of a specific type.</p> <p>Parameters:</p> Name Type Description Default <code>file_type</code> <code>str</code> <p>The file type to count.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of files of the specified type.</p>"},{"location":"reference/#autorag_research.orm.repository.FileRepository.get_all_by_type","title":"<code>get_all_by_type(file_type, limit=None, offset=None)</code>","text":"<p>Retrieve all files of a specific type with pagination.</p> <p>Parameters:</p> Name Type Description Default <code>file_type</code> <code>str</code> <p>The file type to filter by.</p> required <code>limit</code> <code>int | None</code> <p>Maximum number of results to return.</p> <code>None</code> <code>offset</code> <code>int | None</code> <p>Number of results to skip.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of files of the specified type.</p>"},{"location":"reference/#autorag_research.orm.repository.FileRepository.get_all_types","title":"<code>get_all_types()</code>","text":"<p>Get all unique file types in the database.</p> <p>Returns:</p> Type Description <code>list[ColumnElement[Any]]</code> <p>List of unique file types.</p>"},{"location":"reference/#autorag_research.orm.repository.FileRepository.get_by_path","title":"<code>get_by_path(path)</code>","text":"<p>Retrieve a file by its path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path to search for.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The file if found, None otherwise.</p>"},{"location":"reference/#autorag_research.orm.repository.FileRepository.get_by_type","title":"<code>get_by_type(file_type)</code>","text":"<p>Retrieve all files of a specific type.</p> <p>Parameters:</p> Name Type Description Default <code>file_type</code> <code>str</code> <p>The file type (raw, image, audio, video).</p> required <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of files of the specified type.</p>"},{"location":"reference/#autorag_research.orm.repository.FileRepository.get_with_documents","title":"<code>get_with_documents(file_id)</code>","text":"<p>Retrieve a file with its documents eagerly loaded.</p> <p>Parameters:</p> Name Type Description Default <code>file_id</code> <code>int | str</code> <p>The file ID.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The file with documents loaded, None if not found.</p>"},{"location":"reference/#autorag_research.orm.repository.FileRepository.search_by_path_pattern","title":"<code>search_by_path_pattern(pattern)</code>","text":"<p>Search files by path pattern using SQL LIKE.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>The pattern to search for (use % as wildcard).</p> required <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of matching files.</p>"},{"location":"reference/#autorag_research.orm.repository.GenericRepository","title":"<code>GenericRepository</code>","text":"<p>               Bases: <code>Generic[T]</code></p> <p>Generic repository implementing common CRUD operations.</p> <p>This base class provides reusable database operations that can be extended by specific repositories for custom business logic.</p>"},{"location":"reference/#autorag_research.orm.repository.GenericRepository.__init__","title":"<code>__init__(session, model_cls)</code>","text":"<p>Initialize repository with a session and model class.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>SQLAlchemy session for database operations.</p> required <code>model_cls</code> <code>type[T]</code> <p>The SQLAlchemy model class this repository manages.</p> required"},{"location":"reference/#autorag_research.orm.repository.GenericRepository.add","title":"<code>add(entity)</code>","text":"<p>Add a new entity to the session.</p> <p>Parameters:</p> Name Type Description Default <code>entity</code> <code>T</code> <p>The entity instance to add.</p> required <p>Returns:</p> Type Description <code>T</code> <p>The added entity.</p>"},{"location":"reference/#autorag_research.orm.repository.GenericRepository.add_all","title":"<code>add_all(entities)</code>","text":"<p>Add multiple entities to the session.</p> <p>Parameters:</p> Name Type Description Default <code>entities</code> <code>list[T]</code> <p>List of entity instances to add.</p> required <p>Returns:</p> Type Description <code>list[T]</code> <p>The added entities.</p>"},{"location":"reference/#autorag_research.orm.repository.GenericRepository.add_bulk","title":"<code>add_bulk(items)</code>","text":"<p>Memory-efficient bulk insert using SQLAlchemy Core.</p> <p>Unlike add_all(), this method does not create ORM objects in Python memory. Instead, it uses SQLAlchemy Core's insert() which generates a single multi-row INSERT statement, significantly reducing memory usage and improving performance for large batch inserts.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>list[dict]</code> <p>List of dictionaries representing records to insert.</p> required <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of inserted IDs.</p> Note <p>For 1000 records, this method uses ~3-5x less memory than add_all() because it bypasses ORM object creation and identity map tracking. String values are automatically sanitized to remove NUL bytes for PostgreSQL compatibility.</p>"},{"location":"reference/#autorag_research.orm.repository.GenericRepository.add_bulk_skip_duplicates","title":"<code>add_bulk_skip_duplicates(items)</code>","text":"<p>Memory-efficient bulk insert that skips rows with duplicate primary keys.</p> <p>Uses PostgreSQL's INSERT ... ON CONFLICT DO NOTHING to silently skip conflicting rows in a single SQL statement. This is useful when ingesting datasets that may contain duplicate primary keys (e.g., RAGBench).</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>list[dict]</code> <p>List of dictionaries representing records to insert.</p> required <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of inserted IDs (excludes skipped duplicates).</p> Note <p>String values are automatically sanitized to remove NUL bytes for PostgreSQL compatibility.</p>"},{"location":"reference/#autorag_research.orm.repository.GenericRepository.count","title":"<code>count()</code>","text":"<p>Count total number of entities.</p> <p>Returns:</p> Type Description <code>int</code> <p>Total count of entities.</p>"},{"location":"reference/#autorag_research.orm.repository.GenericRepository.delete","title":"<code>delete(entity)</code>","text":"<p>Delete an entity from the database.</p> <p>Parameters:</p> Name Type Description Default <code>entity</code> <code>T</code> <p>The entity instance to delete.</p> required"},{"location":"reference/#autorag_research.orm.repository.GenericRepository.delete_by_id","title":"<code>delete_by_id(_id)</code>","text":"<p>Delete an entity by its primary key.</p> <p>Parameters:</p> Name Type Description Default <code>_id</code> <code>Any</code> <p>The primary key value.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if entity was deleted, False if not found.</p>"},{"location":"reference/#autorag_research.orm.repository.GenericRepository.exists","title":"<code>exists(_id)</code>","text":"<p>Check if an entity exists by its primary key.</p> <p>Parameters:</p> Name Type Description Default <code>_id</code> <code>Any</code> <p>The primary key value.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if entity exists, False otherwise.</p>"},{"location":"reference/#autorag_research.orm.repository.GenericRepository.get_all","title":"<code>get_all(limit=None, offset=None)</code>","text":"<p>Retrieve all entities of this type.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int | None</code> <p>Maximum number of results to return.</p> <code>None</code> <code>offset</code> <code>int | None</code> <p>Number of results to skip.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[T]</code> <p>List of all entities.</p>"},{"location":"reference/#autorag_research.orm.repository.GenericRepository.get_by_id","title":"<code>get_by_id(_id)</code>","text":"<p>Retrieve an entity by its primary key.</p> <p>Parameters:</p> Name Type Description Default <code>_id</code> <code>Any</code> <p>The primary key value.</p> required <p>Returns:</p> Type Description <code>T | None</code> <p>The entity if found, None otherwise.</p>"},{"location":"reference/#autorag_research.orm.repository.GenericRepository.get_by_ids","title":"<code>get_by_ids(ids)</code>","text":"<p>Retrieve multiple entities by their primary keys.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>list[Any]</code> <p>List of primary key values.</p> required <p>Returns:</p> Type Description <code>list[T]</code> <p>List of entities found (may be fewer than requested if some don't exist).</p>"},{"location":"reference/#autorag_research.orm.repository.GenericRepository.update","title":"<code>update(entity)</code>","text":"<p>Update an existing entity.</p> <p>Parameters:</p> Name Type Description Default <code>entity</code> <code>T</code> <p>The entity instance to update.</p> required <p>Returns:</p> Type Description <code>T</code> <p>The updated entity.</p>"},{"location":"reference/#autorag_research.orm.repository.ImageChunkRepository","title":"<code>ImageChunkRepository</code>","text":"<p>               Bases: <code>BaseVectorRepository[Any]</code>, <code>BaseEmbeddingRepository[Any]</code></p> <p>Repository for ImageChunk entity with vector search capabilities.</p>"},{"location":"reference/#autorag_research.orm.repository.ImageChunkRepository.__init__","title":"<code>__init__(session, model_cls=None)</code>","text":"<p>Initialize image chunk repository.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>SQLAlchemy session for database operations.</p> required <code>model_cls</code> <code>type | None</code> <p>The ImageChunk model class to use. If None, uses default schema.</p> <code>None</code>"},{"location":"reference/#autorag_research.orm.repository.ImageChunkRepository.count_by_page","title":"<code>count_by_page(page_id)</code>","text":"<p>Count the number of image chunks for a specific page.</p> <p>Parameters:</p> Name Type Description Default <code>page_id</code> <code>int | str</code> <p>The page ID.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of image chunks for the page.</p>"},{"location":"reference/#autorag_research.orm.repository.ImageChunkRepository.get_by_page_id","title":"<code>get_by_page_id(page_id)</code>","text":"<p>Retrieve all image chunks for a specific page.</p> <p>Parameters:</p> Name Type Description Default <code>page_id</code> <code>int | str</code> <p>The page ID.</p> required <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of image chunks belonging to the page.</p>"},{"location":"reference/#autorag_research.orm.repository.ImageChunkRepository.get_with_all_relations","title":"<code>get_with_all_relations(image_chunk_id)</code>","text":"<p>Retrieve an image chunk with all relationships eagerly loaded.</p> <p>Parameters:</p> Name Type Description Default <code>image_chunk_id</code> <code>int | str</code> <p>The image chunk ID.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The image chunk with all relations loaded, None if not found.</p>"},{"location":"reference/#autorag_research.orm.repository.ImageChunkRepository.get_with_image_chunk_retrieved_results","title":"<code>get_with_image_chunk_retrieved_results(image_chunk_id)</code>","text":"<p>Retrieve an image chunk with its image chunk retrieved results eagerly loaded.</p> <p>Parameters:</p> Name Type Description Default <code>image_chunk_id</code> <code>int | str</code> <p>The image chunk ID.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The image chunk with image chunk retrieved results loaded, None if not found.</p>"},{"location":"reference/#autorag_research.orm.repository.ImageChunkRepository.get_with_page","title":"<code>get_with_page(image_chunk_id)</code>","text":"<p>Retrieve an image chunk with its page eagerly loaded.</p> <p>Parameters:</p> Name Type Description Default <code>image_chunk_id</code> <code>int | str</code> <p>The image chunk ID.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The image chunk with page loaded, None if not found.</p>"},{"location":"reference/#autorag_research.orm.repository.ImageChunkRepository.get_with_retrieval_relations","title":"<code>get_with_retrieval_relations(image_chunk_id)</code>","text":"<p>Retrieve an image chunk with its retrieval relations eagerly loaded.</p> <p>Parameters:</p> Name Type Description Default <code>image_chunk_id</code> <code>int | str</code> <p>The image chunk ID.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The image chunk with retrieval relations loaded, None if not found.</p>"},{"location":"reference/#autorag_research.orm.repository.MetricRepository","title":"<code>MetricRepository</code>","text":"<p>               Bases: <code>GenericRepository[Any]</code></p> <p>Repository for Metric entity with relationship loading capabilities.</p>"},{"location":"reference/#autorag_research.orm.repository.MetricRepository.__init__","title":"<code>__init__(session, model_cls=None)</code>","text":"<p>Initialize metric repository.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>SQLAlchemy session for database operations.</p> required <code>model_cls</code> <code>type | None</code> <p>The Metric model class to use. If None, uses default schema.</p> <code>None</code>"},{"location":"reference/#autorag_research.orm.repository.MetricRepository.exists_by_name","title":"<code>exists_by_name(name)</code>","text":"<p>Check if a metric exists with the given name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The metric name to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if a metric exists, False otherwise.</p>"},{"location":"reference/#autorag_research.orm.repository.MetricRepository.exists_by_name_and_type","title":"<code>exists_by_name_and_type(name, metric_type)</code>","text":"<p>Check if a metric exists with the given name and type.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The metric name to check.</p> required <code>metric_type</code> <code>str</code> <p>The metric type (retrieval or generation).</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if a metric exists, False otherwise.</p>"},{"location":"reference/#autorag_research.orm.repository.MetricRepository.get_all_generation_metrics","title":"<code>get_all_generation_metrics()</code>","text":"<p>Retrieve all generation metrics.</p> <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of all generation metrics ordered by name.</p>"},{"location":"reference/#autorag_research.orm.repository.MetricRepository.get_all_retrieval_metrics","title":"<code>get_all_retrieval_metrics()</code>","text":"<p>Retrieve all retrieval metrics.</p> <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of all retrieval metrics ordered by name.</p>"},{"location":"reference/#autorag_research.orm.repository.MetricRepository.get_by_name","title":"<code>get_by_name(name)</code>","text":"<p>Retrieve a metric by its name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The metric name to search for.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The metric if found, None otherwise.</p>"},{"location":"reference/#autorag_research.orm.repository.MetricRepository.get_by_name_and_type","title":"<code>get_by_name_and_type(name, metric_type)</code>","text":"<p>Retrieve a metric by its name and type.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The metric name to search for.</p> required <code>metric_type</code> <code>str</code> <p>The metric type (retrieval or generation).</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The metric if found, None otherwise.</p>"},{"location":"reference/#autorag_research.orm.repository.MetricRepository.get_by_type","title":"<code>get_by_type(metric_type)</code>","text":"<p>Retrieve all metrics of a specific type.</p> <p>Parameters:</p> Name Type Description Default <code>metric_type</code> <code>str</code> <p>The metric type (retrieval or generation).</p> required <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of metrics of the specified type.</p>"},{"location":"reference/#autorag_research.orm.repository.MetricRepository.get_with_all_relations","title":"<code>get_with_all_relations(metric_id)</code>","text":"<p>Retrieve a metric with all relations eagerly loaded.</p> <p>Parameters:</p> Name Type Description Default <code>metric_id</code> <code>int | str</code> <p>The metric ID.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The metric with all relations loaded, None if not found.</p>"},{"location":"reference/#autorag_research.orm.repository.MetricRepository.get_with_summaries","title":"<code>get_with_summaries(metric_id)</code>","text":"<p>Retrieve a metric with its summaries eagerly loaded.</p> <p>Parameters:</p> Name Type Description Default <code>metric_id</code> <code>int | str</code> <p>The metric ID.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The metric with summaries loaded, None if not found.</p>"},{"location":"reference/#autorag_research.orm.repository.MetricRepository.search_by_name","title":"<code>search_by_name(search_text, limit=10)</code>","text":"<p>Search metrics containing the specified text in their name.</p> <p>Parameters:</p> Name Type Description Default <code>search_text</code> <code>str</code> <p>Text to search for in metric names.</p> required <code>limit</code> <code>int</code> <p>Maximum number of results to return.</p> <code>10</code> <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of metrics containing the search text.</p>"},{"location":"reference/#autorag_research.orm.repository.PageRepository","title":"<code>PageRepository</code>","text":"<p>               Bases: <code>GenericRepository</code></p> <p>Repository for Page entity with specialized queries.</p>"},{"location":"reference/#autorag_research.orm.repository.PageRepository.__init__","title":"<code>__init__(session, model_cls=None)</code>","text":"<p>Initialize page repository.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>SQLAlchemy session for database operations.</p> required <code>model_cls</code> <code>type | None</code> <p>The Page model class. If None, uses default schema.</p> <code>None</code>"},{"location":"reference/#autorag_research.orm.repository.PageRepository.count_by_document","title":"<code>count_by_document(document_id)</code>","text":"<p>Count the number of pages in a document.</p> <p>Parameters:</p> Name Type Description Default <code>document_id</code> <code>int | str</code> <p>The document ID.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of pages in the document.</p>"},{"location":"reference/#autorag_research.orm.repository.PageRepository.get_all_with_document","title":"<code>get_all_with_document(limit=None, offset=None)</code>","text":"<p>Retrieve all pages with their documents eagerly loaded.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int | None</code> <p>Maximum number of results to return.</p> <code>None</code> <code>offset</code> <code>int | None</code> <p>Number of results to skip.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of pages with documents loaded.</p>"},{"location":"reference/#autorag_research.orm.repository.PageRepository.get_by_document_and_page_num","title":"<code>get_by_document_and_page_num(document_id, page_num)</code>","text":"<p>Retrieve a specific page by document ID and page number.</p> <p>Parameters:</p> Name Type Description Default <code>document_id</code> <code>int | str</code> <p>The document ID.</p> required <code>page_num</code> <code>int</code> <p>The page number.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The page if found, None otherwise.</p>"},{"location":"reference/#autorag_research.orm.repository.PageRepository.get_by_document_id","title":"<code>get_by_document_id(document_id)</code>","text":"<p>Retrieve all pages for a specific document.</p> <p>Parameters:</p> Name Type Description Default <code>document_id</code> <code>int | str</code> <p>The document ID.</p> required <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of pages belonging to the document.</p>"},{"location":"reference/#autorag_research.orm.repository.PageRepository.get_page_range","title":"<code>get_page_range(document_id, start_page, end_page)</code>","text":"<p>Retrieve a range of pages from a document.</p> <p>Parameters:</p> Name Type Description Default <code>document_id</code> <code>int | str</code> <p>The document ID.</p> required <code>start_page</code> <code>int</code> <p>The starting page number (inclusive).</p> required <code>end_page</code> <code>int</code> <p>The ending page number (inclusive).</p> required <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of pages in the specified range.</p>"},{"location":"reference/#autorag_research.orm.repository.PageRepository.get_with_chunks","title":"<code>get_with_chunks(page_id)</code>","text":"<p>Retrieve a page with its chunks eagerly loaded.</p> <p>Parameters:</p> Name Type Description Default <code>page_id</code> <code>int | str</code> <p>The page ID.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The page with chunks loaded, None if not found.</p>"},{"location":"reference/#autorag_research.orm.repository.PageRepository.get_with_document","title":"<code>get_with_document(page_id)</code>","text":"<p>Retrieve a page with its document eagerly loaded.</p> <p>Parameters:</p> Name Type Description Default <code>page_id</code> <code>int | str</code> <p>The page ID.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The page with document loaded, None if not found.</p>"},{"location":"reference/#autorag_research.orm.repository.PageRepository.get_with_image_chunks","title":"<code>get_with_image_chunks(page_id)</code>","text":"<p>Retrieve a page with its image chunks eagerly loaded.</p> <p>Parameters:</p> Name Type Description Default <code>page_id</code> <code>int | str</code> <p>The page ID.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The page with image chunks loaded, None if not found.</p>"},{"location":"reference/#autorag_research.orm.repository.PageRepository.get_with_page_chunk_relations","title":"<code>get_with_page_chunk_relations(page_id)</code>","text":"<p>Retrieve a page with its page-chunk relations eagerly loaded.</p> <p>Parameters:</p> Name Type Description Default <code>page_id</code> <code>int | str</code> <p>The page ID.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The page with page-chunk relations loaded, None if not found.</p>"},{"location":"reference/#autorag_research.orm.repository.PageRepository.search_by_metadata","title":"<code>search_by_metadata(metadata_key, metadata_value)</code>","text":"<p>Search pages by metadata field.</p> <p>Parameters:</p> Name Type Description Default <code>metadata_key</code> <code>str</code> <p>The key in the JSONB metadata field.</p> required <code>metadata_value</code> <code>str</code> <p>The value to search for.</p> required <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of matching pages.</p>"},{"location":"reference/#autorag_research.orm.repository.PipelineRepository","title":"<code>PipelineRepository</code>","text":"<p>               Bases: <code>GenericRepository[Any]</code></p> <p>Repository for Pipeline entity with relationship loading capabilities.</p>"},{"location":"reference/#autorag_research.orm.repository.PipelineRepository.__init__","title":"<code>__init__(session, model_cls=None)</code>","text":"<p>Initialize pipeline repository.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>SQLAlchemy session for database operations.</p> required <code>model_cls</code> <code>type | None</code> <p>The Pipeline model class to use. If None, uses default schema.</p> <code>None</code>"},{"location":"reference/#autorag_research.orm.repository.PipelineRepository.exists_by_name","title":"<code>exists_by_name(name)</code>","text":"<p>Check if a pipeline with the given name exists.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The pipeline name to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if pipeline exists, False otherwise.</p>"},{"location":"reference/#autorag_research.orm.repository.PipelineRepository.get_all_ordered_by_name","title":"<code>get_all_ordered_by_name()</code>","text":"<p>Retrieve all pipelines ordered by name.</p> <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of all pipelines ordered alphabetically by name.</p>"},{"location":"reference/#autorag_research.orm.repository.PipelineRepository.get_by_config_key","title":"<code>get_by_config_key(key, value)</code>","text":"<p>Retrieve pipelines with a specific config key-value pair.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The config key to search for.</p> required <code>value</code> <code>str | int | float | bool</code> <p>The config value to match.</p> required <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of pipelines with matching config.</p> Note <p>Uses JSONB containment operator (@&gt;) for efficient config searching.</p>"},{"location":"reference/#autorag_research.orm.repository.PipelineRepository.get_by_name","title":"<code>get_by_name(name)</code>","text":"<p>Retrieve a pipeline by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The pipeline name.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The pipeline if found, None otherwise.</p>"},{"location":"reference/#autorag_research.orm.repository.PipelineRepository.get_with_all_relations","title":"<code>get_with_all_relations(pipeline_id)</code>","text":"<p>Retrieve a pipeline with all relations eagerly loaded.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_id</code> <code>int | str</code> <p>The pipeline ID.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The pipeline with all relations loaded, None if not found.</p>"},{"location":"reference/#autorag_research.orm.repository.PipelineRepository.get_with_executor_results","title":"<code>get_with_executor_results(pipeline_id)</code>","text":"<p>Retrieve a pipeline with its executor results eagerly loaded.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_id</code> <code>int | str</code> <p>The pipeline ID.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The pipeline with executor results loaded, None if not found.</p>"},{"location":"reference/#autorag_research.orm.repository.PipelineRepository.get_with_retrieved_results","title":"<code>get_with_retrieved_results(pipeline_id)</code>","text":"<p>Retrieve a pipeline with its retrieved results eagerly loaded.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_id</code> <code>int | str</code> <p>The pipeline ID.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The pipeline with chunk and image chunk retrieved results loaded, None if not found.</p>"},{"location":"reference/#autorag_research.orm.repository.PipelineRepository.get_with_summaries","title":"<code>get_with_summaries(pipeline_id)</code>","text":"<p>Retrieve a pipeline with its summaries eagerly loaded.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_id</code> <code>int | str</code> <p>The pipeline ID.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The pipeline with summaries loaded, None if not found.</p>"},{"location":"reference/#autorag_research.orm.repository.PipelineRepository.search_by_name","title":"<code>search_by_name(name_pattern)</code>","text":"<p>Search pipelines by name pattern (case-insensitive).</p> <p>Parameters:</p> Name Type Description Default <code>name_pattern</code> <code>str</code> <p>The name pattern to search for (supports SQL LIKE wildcards).</p> required <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of pipelines matching the pattern.</p>"},{"location":"reference/#autorag_research.orm.repository.QueryRepository","title":"<code>QueryRepository</code>","text":"<p>               Bases: <code>BaseVectorRepository[Any]</code>, <code>BaseEmbeddingRepository[Any]</code></p> <p>Repository for Query entity with relationship loading and vector search capabilities.</p>"},{"location":"reference/#autorag_research.orm.repository.QueryRepository.__init__","title":"<code>__init__(session, model_cls=None)</code>","text":"<p>Initialize query repository.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>SQLAlchemy session for database operations.</p> required <code>model_cls</code> <code>type | None</code> <p>The Query model class to use. If None, uses default schema.</p> <code>None</code>"},{"location":"reference/#autorag_research.orm.repository.QueryRepository.count_all","title":"<code>count_all()</code>","text":"<p>Count total number of queries.</p> <p>Returns:</p> Type Description <code>int</code> <p>Total count of queries.</p>"},{"location":"reference/#autorag_research.orm.repository.QueryRepository.count_by_generation_gt_size","title":"<code>count_by_generation_gt_size(size)</code>","text":"<p>Count queries with a specific number of generation ground truths.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>The number of ground truths to match.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Count of queries with the specified number of ground truths.</p>"},{"location":"reference/#autorag_research.orm.repository.QueryRepository.find_by_contents","title":"<code>find_by_contents(contents)</code>","text":"<p>Find query by exact text content match.</p> <p>If multiple queries have the same content, returns the first one found.</p> <p>Parameters:</p> Name Type Description Default <code>contents</code> <code>str</code> <p>The exact query text content to find.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The first matching query if found, None otherwise.</p>"},{"location":"reference/#autorag_research.orm.repository.QueryRepository.get_all_ids","title":"<code>get_all_ids(limit, offset=0)</code>","text":"<p>Get all query IDs with pagination.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>Maximum number of query IDs to return.</p> required <code>offset</code> <code>int</code> <p>Number of query IDs to skip.</p> <code>0</code> <p>Returns:</p> Type Description <code>list[int | str]</code> <p>List of query IDs ordered by ID.</p>"},{"location":"reference/#autorag_research.orm.repository.QueryRepository.get_by_query_text","title":"<code>get_by_query_text(query_text)</code>","text":"<p>Retrieve a query by its text content.</p> <p>Parameters:</p> Name Type Description Default <code>query_text</code> <code>str</code> <p>The query text to search for.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The query if found, None otherwise.</p>"},{"location":"reference/#autorag_research.orm.repository.QueryRepository.get_queries_with_empty_content","title":"<code>get_queries_with_empty_content(limit=100)</code>","text":"<p>Retrieve queries with empty or whitespace-only content.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>Maximum number of queries to retrieve.</p> <code>100</code> <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of queries with empty content.</p>"},{"location":"reference/#autorag_research.orm.repository.QueryRepository.get_queries_with_generation_gt","title":"<code>get_queries_with_generation_gt()</code>","text":"<p>Retrieve all queries that have generation ground truth.</p> <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of queries with generation ground truth.</p>"},{"location":"reference/#autorag_research.orm.repository.QueryRepository.get_with_all_relations","title":"<code>get_with_all_relations(query_id)</code>","text":"<p>Retrieve a query with all relations eagerly loaded.</p> <p>Parameters:</p> Name Type Description Default <code>query_id</code> <code>int | str</code> <p>The query ID.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The query with all relations loaded, None if not found.</p>"},{"location":"reference/#autorag_research.orm.repository.QueryRepository.get_with_executor_results","title":"<code>get_with_executor_results(query_id)</code>","text":"<p>Retrieve a query with its executor results eagerly loaded.</p> <p>Parameters:</p> Name Type Description Default <code>query_id</code> <code>int | str</code> <p>The query ID.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The query with executor results loaded, None if not found.</p>"},{"location":"reference/#autorag_research.orm.repository.QueryRepository.get_with_retrieval_relations","title":"<code>get_with_retrieval_relations(query_id)</code>","text":"<p>Retrieve a query with its retrieval relations eagerly loaded.</p> <p>Parameters:</p> Name Type Description Default <code>query_id</code> <code>int | str</code> <p>The query ID.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The query with retrieval relations loaded, None if not found.</p>"},{"location":"reference/#autorag_research.orm.repository.QueryRepository.search_by_query_text","title":"<code>search_by_query_text(search_text, limit=10)</code>","text":"<p>Search queries containing the specified text.</p> <p>Parameters:</p> Name Type Description Default <code>search_text</code> <code>str</code> <p>Text to search for in query content.</p> required <code>limit</code> <code>int</code> <p>Maximum number of results to return.</p> <code>10</code> <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of queries containing the search text.</p>"},{"location":"reference/#autorag_research.orm.repository.SummaryRepository","title":"<code>SummaryRepository</code>","text":"<p>               Bases: <code>GenericRepository[Summary]</code></p> <p>Repository for Summary entity with composite key support.</p>"},{"location":"reference/#autorag_research.orm.repository.SummaryRepository.__init__","title":"<code>__init__(session)</code>","text":"<p>Initialize summary repository.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>SQLAlchemy session for database operations.</p> required"},{"location":"reference/#autorag_research.orm.repository.SummaryRepository.compare_pipelines_by_metric","title":"<code>compare_pipelines_by_metric(pipeline_ids, metric_id)</code>","text":"<p>Compare multiple pipelines on a specific metric.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_ids</code> <code>list[int]</code> <p>List of pipeline IDs to compare.</p> required <code>metric_id</code> <code>int</code> <p>The metric ID to compare on.</p> required <p>Returns:</p> Type Description <code>list[Summary]</code> <p>List of summaries for the specified pipelines and metric, ordered by metric result.</p>"},{"location":"reference/#autorag_research.orm.repository.SummaryRepository.delete_by_composite_key","title":"<code>delete_by_composite_key(pipeline_id, metric_id)</code>","text":"<p>Delete a summary by its composite primary key.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_id</code> <code>int</code> <p>The pipeline ID.</p> required <code>metric_id</code> <code>int</code> <p>The metric ID.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the summary was deleted, False if not found.</p>"},{"location":"reference/#autorag_research.orm.repository.SummaryRepository.exists_by_composite_key","title":"<code>exists_by_composite_key(pipeline_id, metric_id)</code>","text":"<p>Check if a summary exists with the given composite key.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_id</code> <code>int</code> <p>The pipeline ID.</p> required <code>metric_id</code> <code>int</code> <p>The metric ID.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the summary exists, False otherwise.</p>"},{"location":"reference/#autorag_research.orm.repository.SummaryRepository.get_by_composite_key","title":"<code>get_by_composite_key(pipeline_id, metric_id)</code>","text":"<p>Retrieve a summary by its composite primary key.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_id</code> <code>int</code> <p>The pipeline ID.</p> required <code>metric_id</code> <code>int</code> <p>The metric ID.</p> required <p>Returns:</p> Type Description <code>Summary | None</code> <p>The summary if found, None otherwise.</p>"},{"location":"reference/#autorag_research.orm.repository.SummaryRepository.get_by_execution_time_range","title":"<code>get_by_execution_time_range(pipeline_id, min_time, max_time)</code>","text":"<p>Retrieve summaries within an execution time range.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_id</code> <code>int</code> <p>The pipeline ID.</p> required <code>min_time</code> <code>int</code> <p>Minimum execution time (inclusive).</p> required <code>max_time</code> <code>int</code> <p>Maximum execution time (inclusive).</p> required <p>Returns:</p> Type Description <code>list[Summary]</code> <p>List of summaries within the specified time range.</p>"},{"location":"reference/#autorag_research.orm.repository.SummaryRepository.get_by_metric_id","title":"<code>get_by_metric_id(metric_id)</code>","text":"<p>Retrieve all summaries for a specific metric.</p> <p>Parameters:</p> Name Type Description Default <code>metric_id</code> <code>int</code> <p>The metric ID.</p> required <p>Returns:</p> Type Description <code>list[Summary]</code> <p>List of summaries for the metric.</p>"},{"location":"reference/#autorag_research.orm.repository.SummaryRepository.get_by_metric_result_range","title":"<code>get_by_metric_result_range(metric_id, min_result, max_result)</code>","text":"<p>Retrieve summaries within a metric result range.</p> <p>Parameters:</p> Name Type Description Default <code>metric_id</code> <code>int</code> <p>The metric ID.</p> required <code>min_result</code> <code>float</code> <p>Minimum metric result value (inclusive).</p> required <code>max_result</code> <code>float</code> <p>Maximum metric result value (inclusive).</p> required <p>Returns:</p> Type Description <code>list[Summary]</code> <p>List of summaries within the specified range.</p>"},{"location":"reference/#autorag_research.orm.repository.SummaryRepository.get_by_pipeline_id","title":"<code>get_by_pipeline_id(pipeline_id)</code>","text":"<p>Retrieve all summaries for a specific pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_id</code> <code>int</code> <p>The pipeline ID.</p> required <p>Returns:</p> Type Description <code>list[Summary]</code> <p>List of summaries for the pipeline.</p>"},{"location":"reference/#autorag_research.orm.repository.SummaryRepository.get_metric_summaries_with_relations","title":"<code>get_metric_summaries_with_relations(metric_id)</code>","text":"<p>Retrieve all summaries for a metric with relations eagerly loaded.</p> <p>Parameters:</p> Name Type Description Default <code>metric_id</code> <code>int</code> <p>The metric ID.</p> required <p>Returns:</p> Type Description <code>list[Summary]</code> <p>List of summaries with pipeline and metric loaded.</p>"},{"location":"reference/#autorag_research.orm.repository.SummaryRepository.get_pipeline_summaries_with_relations","title":"<code>get_pipeline_summaries_with_relations(pipeline_id)</code>","text":"<p>Retrieve all summaries for a pipeline with relations eagerly loaded.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_id</code> <code>int</code> <p>The pipeline ID.</p> required <p>Returns:</p> Type Description <code>list[Summary]</code> <p>List of summaries with pipeline and metric loaded.</p>"},{"location":"reference/#autorag_research.orm.repository.SummaryRepository.get_top_pipelines_by_metric","title":"<code>get_top_pipelines_by_metric(metric_id, limit=10, ascending=False)</code>","text":"<p>Retrieve top performing pipelines for a specific metric.</p> <p>Parameters:</p> Name Type Description Default <code>metric_id</code> <code>int</code> <p>The metric ID.</p> required <code>limit</code> <code>int</code> <p>Maximum number of results to return.</p> <code>10</code> <code>ascending</code> <code>bool</code> <p>If True, sort ascending (lower is better), otherwise descending (higher is better).</p> <code>False</code> <p>Returns:</p> Type Description <code>list[Summary]</code> <p>List of summaries ordered by metric result.</p>"},{"location":"reference/#autorag_research.orm.repository.SummaryRepository.get_with_all_relations","title":"<code>get_with_all_relations(pipeline_id, metric_id)</code>","text":"<p>Retrieve a summary with all relations eagerly loaded.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_id</code> <code>int</code> <p>The pipeline ID.</p> required <code>metric_id</code> <code>int</code> <p>The metric ID.</p> required <p>Returns:</p> Type Description <code>Summary | None</code> <p>The summary with all relations loaded, None if not found.</p>"},{"location":"reference/#autorag_research.orm.repository.TextOnlyUnitOfWork","title":"<code>TextOnlyUnitOfWork</code>","text":"<p>               Bases: <code>BaseUnitOfWork</code></p> <p>Text-only Unit of Work for managing text data ingestion transactions.</p> <p>This UoW focuses on text-based entities only (Query, Chunk, RetrievalRelation) and excludes image-related tables like ImageChunk.</p> <p>Provides lazy-initialized repositories for efficient resource usage.</p>"},{"location":"reference/#autorag_research.orm.repository.TextOnlyUnitOfWork.chunks","title":"<code>chunks</code>  <code>property</code>","text":"<p>Get the Chunk repository.</p> <p>Returns:</p> Type Description <code>ChunkRepository</code> <p>ChunkRepository instance.</p> <p>Raises:</p> Type Description <code>SessionNotSetError</code> <p>If session is not initialized.</p>"},{"location":"reference/#autorag_research.orm.repository.TextOnlyUnitOfWork.queries","title":"<code>queries</code>  <code>property</code>","text":"<p>Get the Query repository.</p> <p>Returns:</p> Type Description <code>QueryRepository</code> <p>QueryRepository instance.</p> <p>Raises:</p> Type Description <code>SessionNotSetError</code> <p>If session is not initialized.</p>"},{"location":"reference/#autorag_research.orm.repository.TextOnlyUnitOfWork.retrieval_relations","title":"<code>retrieval_relations</code>  <code>property</code>","text":"<p>Get the RetrievalRelation repository.</p> <p>Returns:</p> Type Description <code>RetrievalRelationRepository</code> <p>RetrievalRelationRepository instance.</p> <p>Raises:</p> Type Description <code>SessionNotSetError</code> <p>If session is not initialized.</p>"},{"location":"reference/#autorag_research.orm.repository.TextOnlyUnitOfWork.__init__","title":"<code>__init__(session_factory, schema=None)</code>","text":"<p>Initialize Text-only Unit of Work with a session factory.</p> <p>Parameters:</p> Name Type Description Default <code>session_factory</code> <code>sessionmaker</code> <p>SQLAlchemy sessionmaker instance.</p> required <code>schema</code> <code>Any | None</code> <p>Schema namespace from create_schema(). If None, uses default 768-dim schema.</p> <code>None</code>"},{"location":"reference/#autorag_research.orm.repository.UnitOfWork","title":"<code>UnitOfWork</code>","text":"<p>Unit of Work pattern for managing database transactions.</p> <p>Ensures data consistency by grouping multiple repository operations into a single atomic transaction.</p>"},{"location":"reference/#autorag_research.orm.repository.UnitOfWork.__enter__","title":"<code>__enter__()</code>","text":"<p>Enter the context manager and create a new session.</p> <p>Returns:</p> Type Description <code>UnitOfWork</code> <p>Self for method chaining.</p>"},{"location":"reference/#autorag_research.orm.repository.UnitOfWork.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Exit the context manager and clean up session.</p> <p>Automatically rolls back if an exception occurred.</p> <p>Parameters:</p> Name Type Description Default <code>exc_type</code> <code>Any</code> <p>Exception type if an error occurred.</p> required <code>exc_val</code> <code>Any</code> <p>Exception value if an error occurred.</p> required <code>exc_tb</code> <code>Any</code> <p>Exception traceback if an error occurred.</p> required"},{"location":"reference/#autorag_research.orm.repository.UnitOfWork.__init__","title":"<code>__init__(session_factory)</code>","text":"<p>Initialize Unit of Work with a session factory.</p> <p>Parameters:</p> Name Type Description Default <code>session_factory</code> <code>Any</code> <p>SQLAlchemy sessionmaker instance.</p> required"},{"location":"reference/#autorag_research.orm.repository.UnitOfWork.commit","title":"<code>commit()</code>","text":"<p>Commit the current transaction.</p>"},{"location":"reference/#autorag_research.orm.repository.UnitOfWork.flush","title":"<code>flush()</code>","text":"<p>Flush pending changes without committing.</p>"},{"location":"reference/#autorag_research.orm.repository.UnitOfWork.rollback","title":"<code>rollback()</code>","text":"<p>Rollback the current transaction.</p>"},{"location":"reference/#service","title":"Service","text":""},{"location":"reference/#autorag_research.orm.service","title":"<code>autorag_research.orm.service</code>","text":""},{"location":"reference/#autorag_research.orm.service.BaseEvaluationService","title":"<code>BaseEvaluationService</code>","text":"<p>               Bases: <code>BaseService</code>, <code>ABC</code></p> <p>Abstract base class for evaluation services.</p> <p>Provides common patterns for evaluation workflows: 1. Fetch execution results in batches using Generator (abstract) 2. Filter missing query IDs that need evaluation (abstract) 3. Compute metrics with batch processing (base) 4. Save evaluation results (abstract)</p> <p>The service supports: - Setting and changing metric functions dynamically - Batch processing with configurable batch size - Generator-based pagination to minimize memory usage and transaction issues</p> Example <pre><code>service = RetrievalEvaluationService(session_factory, schema)\n\n# Set metric and evaluate\nservice.set_metric(metric_id=1, metric_func=my_metric_func)\nservice.evaluate(pipeline_id=1, batch_size=100)\n\n# Change metric and evaluate again\nservice.set_metric(metric_id=2, metric_func=another_metric_func)\nservice.evaluate(pipeline_id=1, batch_size=100)\n</code></pre>"},{"location":"reference/#autorag_research.orm.service.BaseEvaluationService.metric_func","title":"<code>metric_func</code>  <code>property</code>","text":"<p>Get current metric function.</p>"},{"location":"reference/#autorag_research.orm.service.BaseEvaluationService.metric_id","title":"<code>metric_id</code>  <code>property</code>","text":"<p>Get current metric ID.</p>"},{"location":"reference/#autorag_research.orm.service.BaseEvaluationService.__init__","title":"<code>__init__(session_factory, schema=None)</code>","text":"<p>Initialize the evaluation service.</p> <p>Parameters:</p> Name Type Description Default <code>session_factory</code> <code>sessionmaker[Session]</code> <p>SQLAlchemy sessionmaker for database connections.</p> required <code>schema</code> <code>Any | None</code> <p>Schema namespace from create_schema(). If None, uses default 768-dim schema.</p> <code>None</code>"},{"location":"reference/#autorag_research.orm.service.BaseEvaluationService.evaluate","title":"<code>evaluate(pipeline_id, batch_size=100)</code>","text":"<p>Run the full evaluation pipeline for the current metric.</p> <p>This method uses Generator-based pagination to process query IDs: 1. Iterates through query ID batches using limit/offset 2. Filters to only those missing evaluation results 3. Fetches execution results for the batch 4. Computes metrics in batch 5. Saves results to database</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_id</code> <code>int | str</code> <p>The pipeline ID to evaluate.</p> required <code>batch_size</code> <code>int</code> <p>Number of queries to process per batch.</p> <code>100</code> <p>Returns:</p> Type Description <code>int</code> <p>Tuple of (queries_evaluated, average_score).</p> <code>float | None</code> <p>average_score is None if no queries were evaluated.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If metric is not set.</p>"},{"location":"reference/#autorag_research.orm.service.BaseEvaluationService.get_metric","title":"<code>get_metric(metric_name, metric_type=None)</code>","text":"<p>Get metric by name and optionally type.</p> <p>Parameters:</p> Name Type Description Default <code>metric_name</code> <code>str</code> <p>The name of the metric.</p> required <code>metric_type</code> <code>str | None</code> <p>Optional metric type filter ('retrieval' or 'generation').</p> <code>None</code> <p>Returns:</p> Type Description <code>Any | None</code> <p>The Metric entity if found, None otherwise.</p>"},{"location":"reference/#autorag_research.orm.service.BaseEvaluationService.get_or_create_metric","title":"<code>get_or_create_metric(name, metric_type)</code>","text":"<p>Get existing metric or create a new one.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The metric name.</p> required <code>metric_type</code> <code>str</code> <p>The metric type ('retrieval' or 'generation').</p> required <p>Returns:</p> Type Description <code>int | str</code> <p>The metric ID.</p>"},{"location":"reference/#autorag_research.orm.service.BaseEvaluationService.is_evaluation_complete","title":"<code>is_evaluation_complete(pipeline_id, metric_id, batch_size=100)</code>","text":"<p>Check if evaluation is complete for all queries.</p> <p>Iterates through all query IDs and checks: 1. Each query has execution results 2. Each query has evaluation results for the given pipeline and metric</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_id</code> <code>int | str</code> <p>The pipeline ID to check.</p> required <code>metric_id</code> <code>int | str</code> <p>The metric ID to check.</p> required <code>batch_size</code> <code>int</code> <p>Number of queries to check per batch.</p> <code>100</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if all queries have both execution and evaluation results,</p> <code>bool</code> <p>False otherwise (returns immediately on first missing).</p>"},{"location":"reference/#autorag_research.orm.service.BaseEvaluationService.set_metric","title":"<code>set_metric(metric_id, metric_func)</code>","text":"<p>Set the metric ID and function for evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>metric_id</code> <code>int | str</code> <p>The ID of the metric in the database.</p> required <code>metric_func</code> <code>MetricFunc</code> <p>Function that takes list[MetricInput] and returns list[float | None].</p> required"},{"location":"reference/#autorag_research.orm.service.BaseEvaluationService.verify_pipeline_completion","title":"<code>verify_pipeline_completion(pipeline_id, batch_size=100)</code>","text":"<p>Verify all queries have execution results for the pipeline.</p> <p>Iterates through query IDs in batches and checks each batch has results. Returns False immediately when any query is missing results.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_id</code> <code>int | str</code> <p>The pipeline ID to verify.</p> required <code>batch_size</code> <code>int</code> <p>Number of queries to check per batch.</p> <code>100</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if all queries have results, False otherwise.</p> <p>Raises:</p> Type Description <code>NoQueryInDBError</code> <p>If no queries exist in the database.</p>"},{"location":"reference/#autorag_research.orm.service.BasePipelineService","title":"<code>BasePipelineService</code>","text":"<p>               Bases: <code>BaseService</code>, <code>ABC</code></p> <p>Abstract base for pipeline services with shared pipeline management.</p> <p>Provides: - get_or_create_pipeline(): Idempotent pipeline creation with resume support - get_pipeline_config(): Pipeline config retrieval by ID</p> <p>Subclasses must implement _create_uow() and _get_schema_classes() as required by BaseService. The UoW returned by _create_uow() must expose a <code>pipelines</code> property (PipelineRepository).</p>"},{"location":"reference/#autorag_research.orm.service.BasePipelineService.get_or_create_pipeline","title":"<code>get_or_create_pipeline(name, config, *, strict=False)</code>","text":"<p>Get existing pipeline by name or create a new one.</p> <p>If a pipeline with the given name already exists, returns its ID. If the existing pipeline has a different config, behavior depends on <code>strict</code>: - strict=False (default): logs a warning and reuses the existing pipeline. - strict=True: raises <code>ValueError</code>. If no pipeline exists, creates a new one.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name for this pipeline (used as experiment identifier).</p> required <code>config</code> <code>dict</code> <p>Configuration dictionary for the pipeline.</p> required <code>strict</code> <code>bool</code> <p>When True, raise ValueError on config mismatch instead of warning.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[int | str, bool]</code> <p>Tuple of (pipeline_id, is_new) where is_new is True if a new pipeline was created.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If strict=True and an existing pipeline has a different config.</p>"},{"location":"reference/#autorag_research.orm.service.BasePipelineService.get_pipeline_config","title":"<code>get_pipeline_config(pipeline_id)</code>","text":"<p>Get pipeline configuration by ID.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_id</code> <code>int | str</code> <p>ID of the pipeline.</p> required <p>Returns:</p> Type Description <code>dict[Any, Any] | None</code> <p>Pipeline config dict if found, None otherwise.</p>"},{"location":"reference/#autorag_research.orm.service.GenerationEvaluationService","title":"<code>GenerationEvaluationService</code>","text":"<p>               Bases: <code>BaseEvaluationService</code></p> <p>Service for evaluating generation pipelines.</p> <p>This service handles the evaluation workflow for generation pipelines: 1. Fetch queries and ground truth (Query.generation_gt) 2. Fetch generation results (ExecutorResult.generation_result) 3. Compute evaluation metrics (e.g., BLEU, ROUGE, F1) 4. Store results in EvaluationResult table</p> <p>The service uses MetricInput to pass data to metric functions, which should accept list[MetricInput] and return list[float | None].</p> Example <pre><code>from autorag_research.orm.service import GenerationEvaluationService\n\n# Create service\nservice = GenerationEvaluationService(session_factory, schema)\n\n# Get or create metric\nmetric_id = service.get_or_create_metric(\"bleu\", \"generation\")\n\n# Set metric and evaluate\nservice.set_metric(metric_id=metric_id, metric_func=bleu_score)\ncount, avg = service.evaluate(pipeline_id=1, batch_size=100)\nprint(f\"Evaluated {count} queries, average={avg}\")\n</code></pre>"},{"location":"reference/#autorag_research.orm.service.GenerationPipelineService","title":"<code>GenerationPipelineService</code>","text":"<p>               Bases: <code>BasePipelineService</code></p> <p>Service for running generation pipelines.</p> <p>This service handles the common workflow for all generation pipelines: 1. Create a pipeline instance 2. Fetch queries from database 3. Run generation using the provided function (which handles retrieval internally) 4. Store results in ExecutorResult table</p> <p>The actual generation logic (including retrieval) is provided as a function parameter, making this service reusable for NaiveRAG, iterative RAG, etc.</p> Example <pre><code>from autorag_research.orm.service.generation_pipeline import GenerationPipelineService\n\n# Create service\nservice = GenerationPipelineService(session_factory, schema)\n\n# Create or resume pipeline\npipeline_id, is_new = service.get_or_create_pipeline(\n    name=\"naive_rag_v1\",\n    config={\"type\": \"naive_rag\", \"llm_model\": \"gpt-4\"},\n)\n\n# Run pipeline with async generation function\nresults = service.run_pipeline(\n    generate_func=my_async_generate_func,  # Async: handles retrieval + generation\n    pipeline_id=pipeline_id,\n    top_k=10,\n)\n</code></pre>"},{"location":"reference/#autorag_research.orm.service.GenerationPipelineService.delete_pipeline_results","title":"<code>delete_pipeline_results(pipeline_id)</code>","text":"<p>Delete all generation results for a specific pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_id</code> <code>int | str</code> <p>ID of the pipeline.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of deleted records.</p>"},{"location":"reference/#autorag_research.orm.service.GenerationPipelineService.get_chunk_contents","title":"<code>get_chunk_contents(chunk_ids)</code>","text":"<p>Get chunk contents by IDs.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_ids</code> <code>list[int | str]</code> <p>List of chunk IDs to fetch.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of chunk content strings in the same order as input IDs.</p>"},{"location":"reference/#autorag_research.orm.service.GenerationPipelineService.get_image_chunk_contents","title":"<code>get_image_chunk_contents(image_chunk_ids)</code>","text":"<p>Fetch image chunk contents (bytes, mimetype) by IDs.</p> <p>Parameters:</p> Name Type Description Default <code>image_chunk_ids</code> <code>list[int | str]</code> <p>List of image chunk IDs.</p> required <p>Returns:</p> Type Description <code>list[tuple[bytes, str]]</code> <p>List of (image_bytes, mimetype) tuples in same order as IDs.</p> <code>list[tuple[bytes, str]]</code> <p>Missing chunks return (b\"\", \"image/png\").</p>"},{"location":"reference/#autorag_research.orm.service.GenerationPipelineService.get_query_text","title":"<code>get_query_text(query_id)</code>","text":"<p>Get the text of a query by its ID.</p> <p>Parameters:</p> Name Type Description Default <code>query_id</code> <code>int | str</code> <p>The ID of the query.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The text of the query.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the query with the given ID is not found.</p>"},{"location":"reference/#autorag_research.orm.service.GenerationPipelineService.run_pipeline","title":"<code>run_pipeline(generate_func, pipeline_id, top_k=10, batch_size=128, max_concurrency=16, max_retries=3, retry_delay=1.0)</code>","text":"<p>Run generation pipeline for all queries with parallel execution and retry.</p> <p>Parameters:</p> Name Type Description Default <code>generate_func</code> <code>GenerateFunc</code> <p>Async function that performs retrieval + generation. Signature: async (query_id: int, top_k: int) -&gt; GenerationResult The function should handle retrieval internally.</p> required <code>pipeline_id</code> <code>int | str</code> <p>ID of the pipeline.</p> required <code>top_k</code> <code>int</code> <p>Number of top documents to retrieve per query.</p> <code>10</code> <code>batch_size</code> <code>int</code> <p>Number of queries to fetch from DB at once.</p> <code>128</code> <code>max_concurrency</code> <code>int</code> <p>Maximum number of concurrent async operations.</p> <code>16</code> <code>max_retries</code> <code>int</code> <p>Maximum number of retry attempts for failed queries.</p> <code>3</code> <code>retry_delay</code> <code>float</code> <p>Base delay in seconds for exponential backoff between retries.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with pipeline execution statistics:</p> <code>dict[str, Any]</code> <ul> <li>pipeline_id: The pipeline ID</li> </ul> <code>dict[str, Any]</code> <ul> <li>total_queries: Number of queries processed successfully</li> </ul> <code>dict[str, Any]</code> <ul> <li>token_usage: Aggregated token usage dict (prompt_tokens, completion_tokens, total_tokens, embedding_tokens)</li> </ul> <code>dict[str, Any]</code> <ul> <li>avg_execution_time_ms: Average execution time per query</li> </ul> <code>dict[str, Any]</code> <ul> <li>failed_queries: List of query IDs that failed after all retries</li> </ul>"},{"location":"reference/#autorag_research.orm.service.MultiModalIngestionService","title":"<code>MultiModalIngestionService</code>","text":"<p>               Bases: <code>BaseIngestionService</code></p> <p>Service for multi-modal data ingestion operations.</p> <p>This service provides batch-only methods for ingesting multi-modal RAG datasets. Users can access repositories directly via UoW for basic CRUD operations.</p> <p>Design Principles: - Batch-only methods (no single-add methods) - No simple wrappers around repository functions - Value-added operations with transaction management and validation - Mixed multi-hop support for retrieval ground truth</p> Example <pre><code>from autorag_research.orm.connection import DBConnection\nfrom autorag_research.orm.service import MultiModalIngestionService\n\n# Setup database connection\ndb = DBConnection.from_config()  # or DBConnection.from_env()\nsession_factory = db.get_session_factory()\n\n# Initialize service\nservice = MultiModalIngestionService(session_factory)\n\n# Read image file as bytes\nwith open(\"/path/to/image1.jpg\", \"rb\") as f:\n    image_bytes = f.read()\n\n# Batch add files\nfile_ids = service.add_files([\n    {\"path\": \"/path/to/image1.jpg\", \"file_type\": \"image\"},\n    {\"path\": \"/path/to/document1.pdf\", \"file_type\": \"raw\"},\n])\n</code></pre>"},{"location":"reference/#autorag_research.orm.service.MultiModalIngestionService.add_documents","title":"<code>add_documents(documents)</code>","text":"<p>Batch add documents to the database.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>list[dict]</code> <p>List of dicts with keys:       - filename (str | None)       - title (str | None)       - author (str | None)       - filepath_id (int | None) - FK to File       - metadata (dict | None) - JSONB metadata</p> required <p>Returns:</p> Type Description <code>list[int | str]</code> <p>List of created Document IDs.</p>"},{"location":"reference/#autorag_research.orm.service.MultiModalIngestionService.add_files","title":"<code>add_files(files)</code>","text":"<p>Batch add files to the database.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>list[dict[str, str]]</code> <p>List of dictionary (path, file_type).    file_type can be: \"raw\", \"image\", \"audio\", \"video\".</p> required <p>Returns:</p> Type Description <code>list[int | str]</code> <p>List of created File IDs.</p>"},{"location":"reference/#autorag_research.orm.service.MultiModalIngestionService.add_image_chunks","title":"<code>add_image_chunks(image_chunks)</code>","text":"<p>Batch add image chunks to the database.</p> <p>Uses memory-efficient bulk insert (SQLAlchemy Core) instead of ORM objects. This reduces memory usage by ~3-5x for large batches.</p> <p>Parameters:</p> Name Type Description Default <code>image_chunks</code> <code>list[dict[str, bytes | str | int | None]]</code> <p>List of dictionary (content, mimetype, parent_page_id).          content: Image binary data (required)          mimetype: Image MIME type e.g., \"image/png\" (required)          parent_page_id: FK to Page (optional)</p> required <p>Returns:</p> Type Description <code>list[int | str]</code> <p>List of created ImageChunk IDs.</p>"},{"location":"reference/#autorag_research.orm.service.MultiModalIngestionService.add_pages","title":"<code>add_pages(pages)</code>","text":"<p>Batch add pages to the database.</p> <p>Parameters:</p> Name Type Description Default <code>pages</code> <code>list[dict]</code> <p>List of dicts with keys:   - document_id (int) - FK to Document (required)   - page_num (int) - Page number (required)   - image_content (bytes | None) - Image binary data   - mimetype (str | None) - Image MIME type (e.g., \"image/png\")   - metadata (dict | None) - JSONB metadata</p> required <p>Returns:</p> Type Description <code>list[int | str]</code> <p>List of created Page IDs.</p>"},{"location":"reference/#autorag_research.orm.service.MultiModalIngestionService.embed_all_image_chunks","title":"<code>embed_all_image_chunks(embed_func, batch_size=100, max_concurrency=10)</code>","text":"<p>Embed all image chunks that don't have embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>embed_func</code> <code>ImageEmbeddingFunc</code> <p>Async function that takes image bytes and returns embedding vector.</p> required <code>batch_size</code> <code>int</code> <p>Number of image chunks to process per batch.</p> <code>100</code> <code>max_concurrency</code> <code>int</code> <p>Maximum concurrent embedding calls.</p> <code>10</code> <p>Returns:</p> Type Description <code>int</code> <p>Total number of image chunks successfully embedded.</p>"},{"location":"reference/#autorag_research.orm.service.MultiModalIngestionService.embed_all_image_chunks_multi_vector","title":"<code>embed_all_image_chunks_multi_vector(embed_func, batch_size=100, max_concurrency=10)</code>","text":"<p>Embed all image chunks that don't have multi-vector embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>embed_func</code> <code>ImageMultiVectorEmbeddingFunc</code> <p>Async function that takes image bytes and returns multi-vector embedding.</p> required <code>batch_size</code> <code>int</code> <p>Number of image chunks to process per batch.</p> <code>100</code> <code>max_concurrency</code> <code>int</code> <p>Maximum concurrent embedding calls.</p> <code>10</code> <p>Returns:</p> Type Description <code>int</code> <p>Total number of image chunks successfully embedded.</p>"},{"location":"reference/#autorag_research.orm.service.MultiModalIngestionService.get_statistics","title":"<code>get_statistics()</code>","text":"<p>Get statistics about the ingested data.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with counts for all entity types and embedding status.</p>"},{"location":"reference/#autorag_research.orm.service.MultiModalIngestionService.set_image_chunk_embeddings","title":"<code>set_image_chunk_embeddings(image_chunk_ids, embeddings)</code>","text":"<p>Batch set embeddings for image chunks.</p> <p>Parameters:</p> Name Type Description Default <code>image_chunk_ids</code> <code>list[int | str]</code> <p>List of image chunk IDs.</p> required <code>embeddings</code> <code>list[list[float]]</code> <p>List of embedding vectors (must match image_chunk_ids length).</p> required <p>Returns:</p> Type Description <code>int</code> <p>Total number of image chunks successfully updated.</p> <p>Raises:</p> Type Description <code>LengthMismatchError</code> <p>If image_chunk_ids and embeddings have different lengths.</p>"},{"location":"reference/#autorag_research.orm.service.MultiModalIngestionService.set_image_chunk_multi_embeddings","title":"<code>set_image_chunk_multi_embeddings(image_chunk_ids, embeddings)</code>","text":"<p>Batch set multi-vector embeddings for image chunks.</p> <p>Parameters:</p> Name Type Description Default <code>image_chunk_ids</code> <code>list[int | str]</code> <p>List of image chunk IDs.</p> required <code>embeddings</code> <code>list[list[list[float]]]</code> <p>List of multi-vector embeddings (list of list of floats per image chunk).</p> required <p>Returns:</p> Type Description <code>int</code> <p>Total number of image chunks successfully updated.</p> <p>Raises:</p> Type Description <code>LengthMismatchError</code> <p>If image_chunk_ids and embeddings have different lengths.</p>"},{"location":"reference/#autorag_research.orm.service.RetrievalEvaluationService","title":"<code>RetrievalEvaluationService</code>","text":"<p>               Bases: <code>BaseEvaluationService</code></p> <p>Service for evaluating retrieval pipelines.</p> <p>This service handles the evaluation workflow for retrieval pipelines: 1. Fetch queries and ground truth (RetrievalRelation) 2. Fetch retrieval results (ChunkRetrievedResult) 3. Compute evaluation metrics (e.g., Recall@K, Precision@K, MRR) 4. Store results in EvaluationResult table</p> <p>The service uses MetricInput to pass data to metric functions, which should accept list[MetricInput] and return list[float | None].</p> Example <pre><code>from autorag_research.orm.service import RetrievalEvaluationService\nfrom autorag_research.evaluation.metrics.retrieval import retrieval_recall\n\n# Create service\nservice = RetrievalEvaluationService(session_factory, schema)\n\n# Get or create metric\nmetric_id = service.get_or_create_metric(\"recall@10\", \"retrieval\")\n\n# Set metric and evaluate\nservice.set_metric(metric_id=metric_id, metric_func=retrieval_recall)\ncount, avg = service.evaluate(pipeline_id=1, batch_size=100)\nprint(f\"Evaluated {count} queries, average={avg}\")\n\n# Evaluate another metric\nmetric_id_2 = service.get_or_create_metric(\"precision@10\", \"retrieval\")\nservice.set_metric(metric_id=metric_id_2, metric_func=retrieval_precision)\nservice.evaluate(pipeline_id=1)\n</code></pre>"},{"location":"reference/#autorag_research.orm.service.RetrievalPipelineService","title":"<code>RetrievalPipelineService</code>","text":"<p>               Bases: <code>BasePipelineService</code></p> <p>Service for running retrieval pipelines.</p> <p>This service handles the common workflow for all retrieval pipelines: 1. Create a pipeline instance 2. Fetch queries from database 3. Run retrieval using the provided retrieval function 4. Store results in ChunkRetrievedResult table</p> <p>The actual retrieval logic is provided as a function parameter, making this service reusable for BM25, dense retrieval, hybrid, etc.</p> Example <pre><code>from autorag_research.orm.service import RetrievalPipelineService\n\n# Create service\nservice = RetrievalPipelineService(session_factory, schema)\n\n# Direct search (for single-query use cases)\nresults = service.bm25_search(query_ids=[1, 2, 3], top_k=10)\nresults = service.vector_search(query_ids=[1, 2, 3], top_k=10)\n\n# Or use run_pipeline for batch processing with result persistence\npipeline_id, is_new = service.get_or_create_pipeline(\n    name=\"bm25\",\n    config={\"type\": \"bm25\", \"tokenizer\": \"bert\"},\n)\nstats = service.run_pipeline(\n    retrieval_func=lambda ids, k: service.bm25_search(ids, k),\n    pipeline_id=pipeline_id,\n    top_k=10,\n)\n</code></pre>"},{"location":"reference/#autorag_research.orm.service.RetrievalPipelineService.bm25_search","title":"<code>bm25_search(query_ids, top_k=10, tokenizer='bert', index_name='idx_chunk_bm25')</code>","text":"<p>Execute BM25 retrieval for given query IDs.</p> <p>Uses VectorChord-BM25 full-text search on the chunks table.</p> <p>Parameters:</p> Name Type Description Default <code>query_ids</code> <code>list[int | str]</code> <p>List of query IDs to search for.</p> required <code>top_k</code> <code>int</code> <p>Number of top results to return per query.</p> <code>10</code> <code>tokenizer</code> <code>str</code> <p>Tokenizer to use for BM25 (default: \"bert\").</p> <code>'bert'</code> <code>index_name</code> <code>str</code> <p>Name of the BM25 index (default: \"idx_chunk_bm25\").</p> <code>'idx_chunk_bm25'</code> <p>Returns:</p> Type Description <code>list[list[dict[str, Any]]]</code> <p>List of result lists, one per query. Each result dict contains:</p> <code>list[list[dict[str, Any]]]</code> <ul> <li>doc_id: Chunk ID</li> </ul> <code>list[list[dict[str, Any]]]</code> <ul> <li>score: BM25 relevance score</li> </ul> <code>list[list[dict[str, Any]]]</code> <ul> <li>content: Chunk text content</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a query ID is not found in the database.</p>"},{"location":"reference/#autorag_research.orm.service.RetrievalPipelineService.bm25_search_by_text","title":"<code>bm25_search_by_text(query_text, top_k=10, tokenizer='bert', index_name='idx_chunk_bm25')</code>","text":"<p>Execute BM25 retrieval using raw query text (no Query entity needed).</p> <p>Parameters:</p> Name Type Description Default <code>query_text</code> <code>str</code> <p>The query text to search for.</p> required <code>top_k</code> <code>int</code> <p>Number of top results to return.</p> <code>10</code> <code>tokenizer</code> <code>str</code> <p>Tokenizer to use for BM25 (default: \"bert\").</p> <code>'bert'</code> <code>index_name</code> <code>str</code> <p>Name of the BM25 index (default: \"idx_chunk_bm25\").</p> <code>'idx_chunk_bm25'</code> <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>List of result dicts containing doc_id, score, and content.</p>"},{"location":"reference/#autorag_research.orm.service.RetrievalPipelineService.delete_pipeline_results","title":"<code>delete_pipeline_results(pipeline_id)</code>","text":"<p>Delete all retrieval results for a specific pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_id</code> <code>int | str</code> <p>ID of the pipeline.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of deleted records.</p>"},{"location":"reference/#autorag_research.orm.service.RetrievalPipelineService.fetch_query_texts","title":"<code>fetch_query_texts(query_ids)</code>","text":"<p>Batch fetch query texts from database.</p> <p>Parameters:</p> Name Type Description Default <code>query_ids</code> <code>list[int | str]</code> <p>List of query IDs to fetch.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of query text contents in the same order as query_ids.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a query ID is not found.</p>"},{"location":"reference/#autorag_research.orm.service.RetrievalPipelineService.find_query_by_text","title":"<code>find_query_by_text(query_text)</code>","text":"<p>Find existing query by text content.</p> <p>Parameters:</p> Name Type Description Default <code>query_text</code> <code>str</code> <p>The query text to search for.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The query if found, None otherwise.</p>"},{"location":"reference/#autorag_research.orm.service.RetrievalPipelineService.run_pipeline","title":"<code>run_pipeline(retrieval_func, pipeline_id, top_k=10, batch_size=128, max_concurrency=16, max_retries=3, retry_delay=1.0)</code>","text":"<p>Run retrieval pipeline for all queries with parallel execution and retry.</p> <p>Parameters:</p> Name Type Description Default <code>retrieval_func</code> <code>RetrievalFunc</code> <p>Async function that performs retrieval for a single query. Signature: async (query_id: int | str, top_k: int) -&gt; list[dict] Each result dict must have 'doc_id' (int) and 'score' keys.</p> required <code>pipeline_id</code> <code>int | str</code> <p>ID of the pipeline.</p> required <code>top_k</code> <code>int</code> <p>Number of top documents to retrieve per query.</p> <code>10</code> <code>batch_size</code> <code>int</code> <p>Number of queries to fetch from DB at once.</p> <code>128</code> <code>max_concurrency</code> <code>int</code> <p>Maximum number of concurrent async operations.</p> <code>16</code> <code>max_retries</code> <code>int</code> <p>Maximum number of retry attempts for failed queries.</p> <code>3</code> <code>retry_delay</code> <code>float</code> <p>Base delay in seconds for exponential backoff between retries.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with pipeline execution statistics:</p> <code>dict[str, Any]</code> <ul> <li>pipeline_id: The pipeline ID</li> </ul> <code>dict[str, Any]</code> <ul> <li>total_queries: Number of queries processed successfully</li> </ul> <code>dict[str, Any]</code> <ul> <li>total_results: Number of results stored</li> </ul> <code>dict[str, Any]</code> <ul> <li>failed_queries: List of query IDs that failed after all retries</li> </ul>"},{"location":"reference/#autorag_research.orm.service.RetrievalPipelineService.vector_search","title":"<code>vector_search(query_ids, top_k=10, search_mode='single')</code>","text":"<p>Execute vector search for given query IDs.</p> <p>Supports single-vector (cosine similarity) and multi-vector (MaxSim) search modes using VectorChord extension.</p> <p>Parameters:</p> Name Type Description Default <code>query_ids</code> <code>list[int | str]</code> <p>List of query IDs to search for.</p> required <code>top_k</code> <code>int</code> <p>Number of top results to return per query.</p> <code>10</code> <code>search_mode</code> <code>Literal['single', 'multi']</code> <p>\"single\" for dense retrieval, \"multi\" for late interaction.</p> <code>'single'</code> <p>Returns:</p> Type Description <code>list[list[dict[str, Any]]]</code> <p>List of result lists, one per query. Each result dict contains:</p> <code>list[list[dict[str, Any]]]</code> <ul> <li>doc_id: Chunk ID</li> </ul> <code>list[list[dict[str, Any]]]</code> <ul> <li>score: Relevance score in [-1, 1] range (higher = more relevant)</li> <li>single: 1 - cosine_distance (= cosine_similarity)</li> <li>multi: MaxSim / n_query_vectors (normalized late interaction)</li> </ul> <code>list[list[dict[str, Any]]]</code> <ul> <li>content: Chunk text content</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a query ID is not found or lacks required embeddings.</p>"},{"location":"reference/#autorag_research.orm.service.RetrievalPipelineService.vector_search_by_embedding","title":"<code>vector_search_by_embedding(embedding, top_k=10)</code>","text":"<p>Execute vector search using a provided embedding directly.</p> <p>This method enables retrieval pipelines that generate embeddings dynamically (like HyDE) rather than using pre-computed query embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>embedding</code> <code>list[float]</code> <p>The embedding vector to search with.</p> required <code>top_k</code> <code>int</code> <p>Number of top results to return.</p> <code>10</code> <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>List of result dicts containing doc_id, score, and content.</p> <code>list[dict[str, Any]]</code> <p>Score is cosine similarity in [-1, 1] range.</p>"},{"location":"reference/#autorag_research.orm.service.TextDataIngestionService","title":"<code>TextDataIngestionService</code>","text":"<p>               Bases: <code>BaseIngestionService</code></p> <p>Service for text-only data ingestion operations.</p> <p>Provides methods for:</p> <ul> <li>Adding queries (with optional generation_gt)</li> <li>Adding chunks (text-only, no parent page required)</li> <li>Creating retrieval ground truth relations (with multi-hop support)</li> <li>Setting embeddings for queries and chunks (accepts pre-computed vectors)</li> </ul> Example <p>Basic usage with queries, chunks, and retrieval ground truth:</p> <pre><code>from autorag_research.orm.connection import DBConnection\nfrom autorag_research.orm.service import TextDataIngestionService\n\n# Setup database connection\ndb = DBConnection.from_config()  # or DBConnection.from_env()\nsession_factory = db.get_session_factory()\n\n# Initialize service\nservice = TextDataIngestionService(session_factory)\n\n# Get statistics\nstats = service.get_statistics()\nprint(stats)\n</code></pre>"},{"location":"reference/#autorag_research.orm.service.TextDataIngestionService.clean","title":"<code>clean()</code>","text":"<p>Delete empty queries and chunks along with their associated retrieval relations.</p> <p>This method should be called after data ingestion and before embedding to remove any queries or chunks with empty or whitespace-only content. It also removes associated retrieval relations to maintain referential integrity.</p> <p>Returns:</p> Type Description <code>dict[str, int]</code> <p>Dictionary with counts of deleted queries and chunks.</p>"},{"location":"reference/#autorag_research.orm.service.TextDataIngestionService.get_retrieval_gt_by_query","title":"<code>get_retrieval_gt_by_query(query_id)</code>","text":"<p>Get all retrieval ground truth relations for a query.</p> <p>Parameters:</p> Name Type Description Default <code>query_id</code> <code>int | str</code> <p>The query ID.</p> required <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of RetrievalRelation entities ordered by group_index and group_order.</p>"},{"location":"reference/#autorag_research.orm.service.TextDataIngestionService.get_statistics","title":"<code>get_statistics()</code>","text":"<p>Get statistics about the ingested data.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with counts of queries, chunks, and embeddings status.</p>"},{"location":"reference/#utilities","title":"Utilities","text":""},{"location":"reference/#autorag_research.util","title":"<code>autorag_research.util</code>","text":""},{"location":"reference/#autorag_research.util.TokenUsageTracker","title":"<code>TokenUsageTracker</code>","text":"<p>Collects token usage from LangChain LLM responses.</p> <p>Usage::</p> <pre><code>tracker = TokenUsageTracker()\ntracker.record(response)      # extract + store from LangChain response\nresult = tracker.total        # aggregated total across all calls\nper_call = tracker.history    # per-call breakdown\n</code></pre>"},{"location":"reference/#autorag_research.util.TokenUsageTracker.history","title":"<code>history</code>  <code>property</code>","text":"<p>Per-call token usage breakdown (defensive copy).</p>"},{"location":"reference/#autorag_research.util.TokenUsageTracker.total","title":"<code>total</code>  <code>property</code>","text":"<p>Aggregated total token usage across all recorded calls.</p>"},{"location":"reference/#autorag_research.util.TokenUsageTracker.record","title":"<code>record(response)</code>","text":"<p>Extract and record token usage from a LangChain LLM response.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Any</code> <p>LangChain LLM response object (AIMessage, etc.)</p> required <p>Returns:</p> Type Description <code>dict[str, int] | None</code> <p>Extracted usage dict, or None if not available.</p>"},{"location":"reference/#autorag_research.util.aggregate_token_usage","title":"<code>aggregate_token_usage(current, new)</code>","text":"<p>Aggregate two token usage dicts (accumulator pattern).</p> <p>Parameters:</p> Name Type Description Default <code>current</code> <code>dict[str, int] | None</code> <p>Current aggregated token usage (or None).</p> required <code>new</code> <code>dict[str, int] | None</code> <p>New token usage to add (or None).</p> required <p>Returns:</p> Type Description <code>dict[str, int] | None</code> <p>Aggregated token usage dict, or None if both inputs are None.</p>"},{"location":"reference/#autorag_research.util.bytes_to_pil_image","title":"<code>bytes_to_pil_image(image_bytes)</code>","text":"<p>Convert image bytes to PIL Image.</p> <p>Parameters:</p> Name Type Description Default <code>image_bytes</code> <code>bytes</code> <p>Raw image bytes (PNG, JPEG, etc.)</p> required <p>Returns:</p> Type Description <code>Image</code> <p>PIL Image object.</p>"},{"location":"reference/#autorag_research.util.convert_inputs_to_list","title":"<code>convert_inputs_to_list(func)</code>","text":"<p>Decorator to convert all function inputs to Python lists.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>The function to be decorated.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>The wrapped function that converts all inputs to lists.</p>"},{"location":"reference/#autorag_research.util.extract_image_from_data_uri","title":"<code>extract_image_from_data_uri(data_uri)</code>","text":"<p>Extract image bytes and mimetype from a data URI.</p>"},{"location":"reference/#autorag_research.util.extract_token_logprobs","title":"<code>extract_token_logprobs(response, target_tokens=None)</code>","text":"<p>Extract log probabilities from LangChain LLM response.</p> <p>Works with any LangChain LLM that stores logprobs in <code>response_metadata[\"logprobs\"][\"content\"]</code>. Compatible providers include: - OpenAI (ChatOpenAI, AzureChatOpenAI) - Together AI, Fireworks AI, Anyscale - Local models via vLLM, text-generation-inference, Ollama (with logprobs enabled) - Any OpenAI-compatible API endpoint</p> <p>To enable logprobs, use provider-specific configuration: - OpenAI/vLLM: llm.bind(logprobs=True, top_logprobs=5) - Other providers: Check provider documentation for logprobs support</p> <p>LangChain stores logprobs in <code>response.response_metadata[\"logprobs\"][\"content\"]</code>. Each token entry has: token, logprob, bytes, top_logprobs.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Any</code> <p>LangChain AIMessage or similar response object.</p> required <code>target_tokens</code> <code>list[str] | None</code> <p>If provided, only return logprobs for these tokens. Case-insensitive matching. If None, returns all token logprobs.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, float] | None</code> <p>Dict mapping token strings to their log probability values.</p> <code>dict[str, float] | None</code> <p>Returns None if logprobs not available in response.</p> Example Note <ul> <li>log probability of 0.0 = 100% confidence</li> <li>More negative = less likely</li> <li>Convert to probability: exp(logprob)</li> </ul>"},{"location":"reference/#autorag_research.util.extract_token_logprobs--enable-logprobs-on-the-llm-openai-example","title":"Enable logprobs on the LLM (OpenAI example)","text":"<p>llm = ChatOpenAI(model=\"gpt-4o-mini\").bind(logprobs=True, top_logprobs=5) response = llm.invoke(\"Answer Yes or No: Is the sky blue?\") logprobs = extract_token_logprobs(response, target_tokens=[\"Yes\", \"No\"])</p>"},{"location":"reference/#autorag_research.util.extract_token_logprobs--returns-yes-00001-no-92-or-none-if-not-available","title":"Returns: {\"Yes\": -0.0001, \"No\": -9.2} or None if not available","text":""},{"location":"reference/#autorag_research.util.image_chunk_to_pil_images","title":"<code>image_chunk_to_pil_images(image_chunks)</code>","text":"<p>Convert raw image bytes to PIL Images, skipping invalid ones.</p> <p>Parameters:</p> Name Type Description Default <code>image_chunks</code> <code>list[tuple[bytes, str]]</code> <p>List of (bytes, mimetype) tuples. It can be a result of the GET operation from the ImageChunk repository.</p> required <p>Returns:</p> Type Description <code>list[Image]</code> <p>List of valid PIL Images.</p>"},{"location":"reference/#autorag_research.util.load_image","title":"<code>load_image(img)</code>","text":"<p>Convert any ImageType to a PIL Image in RGB mode.</p> <p>Accepts file paths (str/Path), raw bytes, or BytesIO objects and returns a PIL Image converted to RGB.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ImageType</code> <p>Image as file path (str/Path), raw bytes, or BytesIO.</p> required <p>Returns:</p> Type Description <code>Image</code> <p>PIL Image in RGB mode.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If img is not a supported type.</p>"},{"location":"reference/#autorag_research.util.normalize_dbsf","title":"<code>normalize_dbsf(scores)</code>","text":"<p>3-sigma distribution-based score fusion normalization.</p> <p>Normalizes using mean \u00b1 3*std as bounds, then clips to [0, 1]. This method is robust to outliers and works well when combining scores from different distributions. None values are preserved and excluded from statistics calculation.</p> <p>Reference: \"Score Normalization in Multi-Engine Text Retrieval\"</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>list[float | None]</code> <p>List of numeric scores to normalize. None values are preserved.</p> required <p>Returns:</p> Type Description <code>list[float | None]</code> <p>List of normalized scores clipped to [0, 1] range, with None preserved.</p> Example <p>normalize_dbsf([1.0, 2.0, 3.0, 4.0, 5.0]) [0.0, 0.25, 0.5, 0.75, 1.0]  # approximately normalize_dbsf([1.0, None, 3.0, 4.0, 5.0]) [0.0, None, 0.333..., 0.5, 0.666...]  # approximately</p>"},{"location":"reference/#autorag_research.util.normalize_minmax","title":"<code>normalize_minmax(scores)</code>","text":"<p>Min-max normalization to [0, 1] range.</p> <p>Scales scores linearly so that the minimum becomes 0 and maximum becomes 1. If all scores are equal, returns a list of 0.5 values. None values are preserved and excluded from statistics calculation.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>list[float | None]</code> <p>List of numeric scores to normalize. None values are preserved.</p> required <p>Returns:</p> Type Description <code>list[float | None]</code> <p>List of normalized scores in [0, 1] range, with None preserved.</p> Example <p>normalize_minmax([1.0, 2.0, 3.0]) [0.0, 0.5, 1.0] normalize_minmax([1.0, None, 3.0]) [0.0, None, 1.0]</p>"},{"location":"reference/#autorag_research.util.normalize_string","title":"<code>normalize_string(s)</code>","text":"<p>Taken from the official evaluation script for v1.1 of the SQuAD dataset. Lower text and remove punctuation, articles, and extra whitespace.</p>"},{"location":"reference/#autorag_research.util.normalize_tmm","title":"<code>normalize_tmm(scores, theoretical_min)</code>","text":"<p>Theoretical min-max normalization using theoretical min and actual max.</p> <p>Uses the theoretical minimum bound and actual maximum from the data. This is useful when the minimum is known (e.g., 0 for BM25) but the maximum varies per query. None values are preserved and excluded from statistics calculation.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>list[float | None]</code> <p>List of numeric scores to normalize. None values are preserved.</p> required <code>theoretical_min</code> <code>float</code> <p>Known minimum possible score (e.g., 0 for BM25, -1 for cosine).</p> required <p>Returns:</p> Type Description <code>list[float | None]</code> <p>List of normalized scores in [0, 1] range, with None preserved.</p> Example <p>normalize_tmm([0.0, 50.0, 100.0], theoretical_min=0.0) [0.0, 0.5, 1.0] normalize_tmm([0.0, None, 100.0], theoretical_min=0.0) [0.0, None, 1.0]</p>"},{"location":"reference/#autorag_research.util.normalize_zscore","title":"<code>normalize_zscore(scores)</code>","text":"<p>Z-score standardization (mean=0, std=1).</p> <p>Centers scores around mean and scales by standard deviation. If standard deviation is 0 (all scores equal), returns all zeros. None values are preserved and excluded from statistics calculation.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>list[float | None]</code> <p>List of numeric scores to normalize. None values are preserved.</p> required <p>Returns:</p> Type Description <code>list[float | None]</code> <p>List of z-score normalized values, with None preserved.</p> Example <p>normalize_zscore([1.0, 2.0, 3.0]) [-1.2247..., 0.0, 1.2247...] normalize_zscore([1.0, None, 3.0]) [-1.0, None, 1.0]</p>"},{"location":"reference/#autorag_research.util.pil_image_to_bytes","title":"<code>pil_image_to_bytes(image)</code>","text":"<p>Convert PIL image to bytes with mimetype.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>PIL Image object.</p> required <p>Returns:</p> Type Description <code>tuple[bytes, str]</code> <p>Tuple of (image_bytes, mimetype).</p>"},{"location":"reference/#autorag_research.util.pil_image_to_data_uri","title":"<code>pil_image_to_data_uri(image)</code>","text":"<p>Convert PIL Image to data URI for multi-modal LLMs.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>PIL Image object.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Data URI string (e.g., \"data:image/png;base64,iVBORw0...\").</p>"},{"location":"reference/#autorag_research.util.run_with_concurrency_limit","title":"<code>run_with_concurrency_limit(items, async_func, max_concurrency, error_message='Task failed')</code>  <code>async</code>","text":"<p>Run async function on items with concurrency limit using semaphore.</p> <p>A generic utility for running async operations with controlled concurrency. Each item is processed by the async function, with at most <code>max_concurrency</code> operations running simultaneously.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>Iterable[T]</code> <p>Iterable of items to process.</p> required <code>async_func</code> <code>Callable[[T], Awaitable[R]]</code> <p>Async function that takes an item and returns a result.</p> required <code>max_concurrency</code> <code>int</code> <p>Maximum number of concurrent operations.</p> required <code>error_message</code> <code>str</code> <p>Message to log when an operation fails.</p> <code>'Task failed'</code> <p>Returns:</p> Type Description <code>list[R | None]</code> <p>List of results (or None if failed) in same order as items.</p> Example <pre><code>async def embed_text(text: str) -&gt; list[float]:\n    return await some_api_call(text)\n\ntexts = [\"hello\", \"world\", \"test\"]\nembeddings = await run_with_concurrency_limit(\n    texts,\n    embed_text,\n    max_concurrency=5,\n    error_message=\"Failed to embed text\",\n)\n</code></pre>"},{"location":"reference/#autorag_research.util.to_async_func","title":"<code>to_async_func(func)</code>","text":"<p>Convert a synchronous function to an asynchronous function.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., R]</code> <p>The synchronous function to convert.</p> required <p>Returns:</p> Type Description <code>Callable[..., Awaitable[R]]</code> <p>An asynchronous function that runs the synchronous function in a thread.</p>"},{"location":"reference/#autorag_research.util.to_list","title":"<code>to_list(item)</code>","text":"<p>Recursively convert collections to Python lists.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Any</code> <p>The item to convert to a list. Can be numpy array, pandas Series, or any iterable collection.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The converted Python list.</p>"},{"location":"reference/#autorag_research.util.truncate_texts","title":"<code>truncate_texts(str_list, max_tokens)</code>","text":"<p>Truncate each string in the list to a maximum number of tokens using tiktoken.</p> <p>Parameters:</p> Name Type Description Default <code>str_list</code> <code>list[str]</code> <p>List of strings to be truncated.</p> required <code>max_tokens</code> <code>int</code> <p>Maximum number of tokens allowed per string.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of truncated strings.</p>"},{"location":"reference/#autorag_research.util.unpack_and_run","title":"<code>unpack_and_run(target_list, func, *args, **kwargs)</code>","text":"<p>Unpack each sublist in target_list and run func with the unpacked arguments.</p> <p>Parameters:</p> Name Type Description Default <code>target_list</code> <code>list[list[Any]]</code> <p>List of sublists to be unpacked and processed.</p> required <code>func</code> <code>Callable</code> <p>Callable function to run on the flattened list.</p> required <code>*args</code> <code>tuple</code> <p>Additional positional arguments to pass to func.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to func.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of results grouped by original sublist lengths.</p>"},{"location":"reference/#autorag_research.util.validate_plugin_name","title":"<code>validate_plugin_name(name)</code>","text":"<p>Check whether name is a valid plugin name.</p> <p>A valid plugin name starts with a lowercase letter and contains only lowercase letters, digits, and underscores.  This guards against path traversal and code-injection when the name is interpolated into filesystem paths and Python source templates.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The candidate plugin name.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if valid, <code>False</code> otherwise.</p>"},{"location":"reference/#autorag_research.exceptions","title":"<code>autorag_research.exceptions</code>","text":""},{"location":"reference/#autorag_research.exceptions.DuplicateRetrievalGTError","title":"<code>DuplicateRetrievalGTError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when retrieval GT already exists for a query and upsert is False.</p>"},{"location":"reference/#autorag_research.exceptions.EmbeddingError","title":"<code>EmbeddingError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the embedding model is not set.</p>"},{"location":"reference/#autorag_research.exceptions.EmptyIterableError","title":"<code>EmptyIterableError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when an iterable is empty but should contain items.</p>"},{"location":"reference/#autorag_research.exceptions.EnvNotFoundError","title":"<code>EnvNotFoundError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when a required environment variable is not found.</p>"},{"location":"reference/#autorag_research.exceptions.EvaluationError","title":"<code>EvaluationError</code>","text":"<p>               Bases: <code>ExecutorError</code></p> <p>Raised when evaluation fails.</p>"},{"location":"reference/#autorag_research.exceptions.ExecutorError","title":"<code>ExecutorError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for Executor errors.</p>"},{"location":"reference/#autorag_research.exceptions.InvalidDatasetNameError","title":"<code>InvalidDatasetNameError</code>","text":"<p>               Bases: <code>NameError</code></p> <p>Raised when an invalid dataset name is provided.</p>"},{"location":"reference/#autorag_research.exceptions.LLMError","title":"<code>LLMError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the LLM model is not set.</p>"},{"location":"reference/#autorag_research.exceptions.LengthMismatchError","title":"<code>LengthMismatchError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when there is a length mismatch between two related lists.</p>"},{"location":"reference/#autorag_research.exceptions.LogprobsNotSupportedError","title":"<code>LogprobsNotSupportedError</code>","text":"<p>               Bases: <code>ExecutorError</code></p> <p>Raised when a pipeline requires logprobs but the LLM doesn't support them.</p>"},{"location":"reference/#autorag_research.exceptions.MaxRetriesExceededError","title":"<code>MaxRetriesExceededError</code>","text":"<p>               Bases: <code>ExecutorError</code></p> <p>Raised when max retries are exceeded.</p>"},{"location":"reference/#autorag_research.exceptions.MissingDBNameError","title":"<code>MissingDBNameError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the database name is missing in the configuration.</p>"},{"location":"reference/#autorag_research.exceptions.MissingRequiredParameterError","title":"<code>MissingRequiredParameterError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when required parameters are missing.</p>"},{"location":"reference/#autorag_research.exceptions.NoQueryInDBError","title":"<code>NoQueryInDBError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when there are no queries in the database.</p>"},{"location":"reference/#autorag_research.exceptions.NoSessionError","title":"<code>NoSessionError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when there is no active database session.</p>"},{"location":"reference/#autorag_research.exceptions.PipelineExecutionError","title":"<code>PipelineExecutionError</code>","text":"<p>               Bases: <code>ExecutorError</code></p> <p>Raised when a pipeline fails to execute.</p>"},{"location":"reference/#autorag_research.exceptions.PipelineVerificationError","title":"<code>PipelineVerificationError</code>","text":"<p>               Bases: <code>ExecutorError</code></p> <p>Raised when pipeline results fail verification.</p>"},{"location":"reference/#autorag_research.exceptions.RepositoryNotSupportedError","title":"<code>RepositoryNotSupportedError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when a repository is not supported by the current UoW.</p>"},{"location":"reference/#autorag_research.exceptions.RerankerError","title":"<code>RerankerError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the reranker model fails.</p>"},{"location":"reference/#autorag_research.exceptions.SchemaNotFoundError","title":"<code>SchemaNotFoundError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when a schema is not found.</p>"},{"location":"reference/#autorag_research.exceptions.ServiceNotSetError","title":"<code>ServiceNotSetError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the service is not set.</p>"},{"location":"reference/#autorag_research.exceptions.SessionNotSetError","title":"<code>SessionNotSetError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the database session is not set.</p>"},{"location":"reference/#autorag_research.exceptions.UnsupportedDataSubsetError","title":"<code>UnsupportedDataSubsetError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when an unsupported data subset is requested.</p>"},{"location":"reference/#autorag_research.exceptions.UnsupportedLanguageError","title":"<code>UnsupportedLanguageError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when an unsupported language is specified.</p>"},{"location":"reference/#autorag_research.exceptions.UnsupportedMTEBTaskTypeError","title":"<code>UnsupportedMTEBTaskTypeError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when an unsupported MTEB task type is provided.</p>"},{"location":"rerankers/","title":"Rerankers","text":"<p>Re-score retrieved documents to improve ranking quality.</p>"},{"location":"rerankers/#available-rerankers","title":"Available Rerankers","text":""},{"location":"rerankers/#api-based","title":"API-based","text":"Reranker Type Provider Cohere API Cohere Jina API Jina AI VoyageAI API Voyage AI MixedbreadAI API Mixedbread AI RankGPT LLM Any LLM UPR LLM Any LLM FlagEmbedding LLM LLM-based BAAI"},{"location":"rerankers/#local-models","title":"Local Models","text":"Reranker Type Library SentenceTransformer CrossEncoder sentence-transformers FlagEmbedding CrossEncoder FlagEmbedding FlashRank ONNX flashrank ColBERT Token-level torch, transformers MonoT5 Seq2Seq torch, transformers TART Instruction-T5 torch, transformers KoReranker Korean torch, transformers OpenVINO HW-optimized optimum-intel"},{"location":"rerankers/#installation","title":"Installation","text":"<p>API rerankers require optional dependencies:</p> <pre><code>pip install autorag-research[reranker]\n</code></pre> <p>GPU/local rerankers require the <code>gpu</code> extra:</p> <pre><code>pip install autorag-research[gpu]\n</code></pre> <p>OpenVINO reranker requires:</p> <pre><code>pip install autorag-research[openvino]\n</code></pre>"},{"location":"rerankers/#base-class","title":"Base Class","text":"<p>All rerankers extend <code>BaseReranker</code>:</p> <pre><code>from autorag_research.rerankers import BaseReranker, RerankResult\n\n\nclass MyReranker(BaseReranker):\n    def rerank(self, query: str, documents: list[str], top_k: int | None = None) -&gt; list[RerankResult]:\n        # Return sorted results by relevance\n        pass\n\n    async def arerank(self, query: str, documents: list[str], top_k: int | None = None) -&gt; list[RerankResult]:\n        # Async version\n        pass\n</code></pre>"},{"location":"rerankers/#methods","title":"Methods","text":"Method Description <code>rerank(query, docs, top_k)</code> Single query reranking <code>arerank(query, docs, top_k)</code> Async single query <code>rerank_documents(queries, docs_list, top_k)</code> Batch reranking <code>arerank_documents(queries, docs_list, top_k)</code> Async batch"},{"location":"rerankers/#usage-with-injection","title":"Usage with Injection","text":"<pre><code>from autorag_research.injection import load_reranker, with_reranker\n\n# Load from config\nreranker = load_reranker(\"cohere\")\n\n# Or use decorator\n@with_reranker()\ndef my_func(reranker):\n    return reranker.rerank(\"query\", [\"doc1\", \"doc2\"])\n</code></pre>"},{"location":"rerankers/cohere/","title":"Cohere","text":"<p>Reranking via Cohere's API.</p>"},{"location":"rerankers/cohere/#overview","title":"Overview","text":"Field Value Type API Provider Cohere Default Model <code>rerank-v3.5</code> Env Variable <code>COHERE_API_KEY</code>"},{"location":"rerankers/cohere/#installation","title":"Installation","text":"<pre><code>pip install \"autorag-research[reranker]\"\n# or\nuv add \"autorag-research[reranker]\"\n</code></pre>"},{"location":"rerankers/cohere/#configuration","title":"Configuration","text":"<pre><code>_target_: autorag_research.rerankers.cohere.CohereReranker\nmodel_name: rerank-v3.5\n</code></pre>"},{"location":"rerankers/cohere/#options","title":"Options","text":"Option Type Default Description model_name str <code>rerank-v3.5</code> Cohere model name api_key str None API key (or use env var) batch_size int 64 Batch size for multiple queries"},{"location":"rerankers/cohere/#models","title":"Models","text":"Model Description rerank-v3.5 Latest, best quality rerank-english-v3.0 English optimized rerank-multilingual-v3.0 Multilingual"},{"location":"rerankers/cohere/#usage","title":"Usage","text":"<pre><code>from autorag_research.rerankers import CohereReranker\n\nreranker = CohereReranker(model_name=\"rerank-v3.5\")\nresults = reranker.rerank(\"What is RAG?\", [\"doc1\", \"doc2\", \"doc3\"], top_k=2)\n\nfor r in results:\n    print(f\"[{r.index}] {r.score:.3f}: {r.text[:50]}...\")\n</code></pre>"},{"location":"rerankers/colbert/","title":"ColBERT","text":"<p>Token-level MaxSim reranking via ColBERT.</p>"},{"location":"rerankers/colbert/#overview","title":"Overview","text":"Field Value Type Token-level Library torch, transformers Default Model <code>colbert-ir/colbertv2.0</code>"},{"location":"rerankers/colbert/#installation","title":"Installation","text":"<pre><code>pip install \"autorag-research[gpu]\"\n# or\nuv add \"autorag-research[gpu]\"\n</code></pre>"},{"location":"rerankers/colbert/#configuration","title":"Configuration","text":"<pre><code>_target_: autorag_research.rerankers.colbert.ColBERTReranker\nmodel_name: colbert-ir/colbertv2.0\n</code></pre>"},{"location":"rerankers/colbert/#options","title":"Options","text":"Option Type Default Description model_name str <code>colbert-ir/colbertv2.0</code> ColBERT model name max_length int 512 Maximum input sequence length device str None Device (auto-detected) batch_size int 64 Batch size for multiple queries"},{"location":"rerankers/colbert/#how-it-works","title":"How It Works","text":"<ol> <li>Encode query and document into token embeddings</li> <li>Compute cosine similarity matrix between all token pairs</li> <li>For each query token, take the maximum similarity (MaxSim)</li> <li>Average the MaxSim scores across query tokens</li> </ol>"},{"location":"rerankers/colbert/#usage","title":"Usage","text":"<pre><code>from autorag_research.rerankers import ColBERTReranker\n\nreranker = ColBERTReranker()\nresults = reranker.rerank(\"What is RAG?\", [\"doc1\", \"doc2\", \"doc3\"], top_k=2)\n</code></pre>"},{"location":"rerankers/flag_embedding/","title":"FlagEmbedding","text":"<p>Cross-encoder reranking via BAAI FlagEmbedding.</p>"},{"location":"rerankers/flag_embedding/#overview","title":"Overview","text":"Field Value Type CrossEncoder Library FlagEmbedding Default Model <code>BAAI/bge-reranker-large</code>"},{"location":"rerankers/flag_embedding/#installation","title":"Installation","text":"<pre><code>pip install \"autorag-research[gpu]\"\n# or\nuv add \"autorag-research[gpu]\"\n</code></pre>"},{"location":"rerankers/flag_embedding/#configuration","title":"Configuration","text":"<pre><code>_target_: autorag_research.rerankers.flag_embedding.FlagEmbeddingReranker\nmodel_name: BAAI/bge-reranker-large\n</code></pre>"},{"location":"rerankers/flag_embedding/#options","title":"Options","text":"Option Type Default Description model_name str <code>BAAI/bge-reranker-large</code> FlagEmbedding model name use_fp16 bool False Use FP16 for inference batch_size int 64 Batch size for multiple queries"},{"location":"rerankers/flag_embedding/#models","title":"Models","text":"Model Description BAAI/bge-reranker-large Best quality BAAI/bge-reranker-base Balanced BAAI/bge-reranker-v2-m3 Multilingual"},{"location":"rerankers/flag_embedding/#usage","title":"Usage","text":"<pre><code>from autorag_research.rerankers import FlagEmbeddingReranker\n\nreranker = FlagEmbeddingReranker()\nresults = reranker.rerank(\"What is RAG?\", [\"doc1\", \"doc2\", \"doc3\"], top_k=2)\n</code></pre>"},{"location":"rerankers/flag_embedding_llm/","title":"FlagEmbedding LLM","text":"<p>LLM-based reranking via BAAI FlagEmbedding.</p>"},{"location":"rerankers/flag_embedding_llm/#overview","title":"Overview","text":"Field Value Type LLM-based Library FlagEmbedding Default Model <code>BAAI/bge-reranker-v2-gemma</code>"},{"location":"rerankers/flag_embedding_llm/#installation","title":"Installation","text":"<pre><code>pip install \"autorag-research[gpu]\"\n# or\nuv add \"autorag-research[gpu]\"\n</code></pre>"},{"location":"rerankers/flag_embedding_llm/#configuration","title":"Configuration","text":"<pre><code>_target_: autorag_research.rerankers.flag_embedding_llm.FlagEmbeddingLLMReranker\nmodel_name: BAAI/bge-reranker-v2-gemma\n</code></pre>"},{"location":"rerankers/flag_embedding_llm/#options","title":"Options","text":"Option Type Default Description model_name str <code>BAAI/bge-reranker-v2-gemma</code> FlagEmbedding LLM model name use_fp16 bool False Use FP16 for inference batch_size int 64 Batch size for multiple queries"},{"location":"rerankers/flag_embedding_llm/#usage","title":"Usage","text":"<pre><code>from autorag_research.rerankers import FlagEmbeddingLLMReranker\n\nreranker = FlagEmbeddingLLMReranker()\nresults = reranker.rerank(\"What is RAG?\", [\"doc1\", \"doc2\", \"doc3\"], top_k=2)\n</code></pre>"},{"location":"rerankers/flag_embedding_llm/#when-to-use","title":"When to Use","text":"<p>Good for higher quality reranking when compute resources are available. Uses LLM-scale models (e.g., Gemma) for more nuanced relevance scoring.</p>"},{"location":"rerankers/flashrank/","title":"FlashRank","text":"<p>Lightweight ONNX-based reranking.</p>"},{"location":"rerankers/flashrank/#overview","title":"Overview","text":"Field Value Type ONNX Library flashrank Default Model <code>ms-marco-MiniLM-L-12-v2</code>"},{"location":"rerankers/flashrank/#installation","title":"Installation","text":"<pre><code>pip install \"autorag-research[gpu]\"\n# or\nuv add \"autorag-research[gpu]\"\n</code></pre>"},{"location":"rerankers/flashrank/#configuration","title":"Configuration","text":"<pre><code>_target_: autorag_research.rerankers.flashrank.FlashRankReranker\nmodel_name: ms-marco-MiniLM-L-12-v2\n</code></pre>"},{"location":"rerankers/flashrank/#options","title":"Options","text":"Option Type Default Description model_name str <code>ms-marco-MiniLM-L-12-v2</code> FlashRank model name max_length int 512 Maximum input sequence length batch_size int 64 Batch size for multiple queries"},{"location":"rerankers/flashrank/#usage","title":"Usage","text":"<pre><code>from autorag_research.rerankers import FlashRankReranker\n\nreranker = FlashRankReranker()\nresults = reranker.rerank(\"What is RAG?\", [\"doc1\", \"doc2\", \"doc3\"], top_k=2)\n</code></pre>"},{"location":"rerankers/flashrank/#when-to-use","title":"When to Use","text":"<p>Good for:</p> <ul> <li>CPU-only environments</li> <li>Low-latency requirements</li> <li>Production deployments without GPU</li> </ul>"},{"location":"rerankers/jina/","title":"Jina","text":"<p>Reranking via Jina AI's API.</p>"},{"location":"rerankers/jina/#overview","title":"Overview","text":"Field Value Type API Provider Jina AI Default Model <code>jina-reranker-v2-base-multilingual</code> Env Variable <code>JINA_API_KEY</code>"},{"location":"rerankers/jina/#configuration","title":"Configuration","text":"<pre><code>_target_: autorag_research.rerankers.jina.JinaReranker\nmodel_name: jina-reranker-v2-base-multilingual\n</code></pre>"},{"location":"rerankers/jina/#options","title":"Options","text":"Option Type Default Description model_name str <code>jina-reranker-v2-base-multilingual</code> Jina model name api_key str None API key (or use env var) batch_size int 64 Batch size"},{"location":"rerankers/jina/#models","title":"Models","text":"Model Description jina-reranker-v2-base-multilingual Multilingual, balanced jina-reranker-v1-base-en English only jina-reranker-v1-turbo-en Fast, English"},{"location":"rerankers/jina/#usage","title":"Usage","text":"<pre><code>from autorag_research.rerankers import JinaReranker\n\nreranker = JinaReranker()\nresults = reranker.rerank(\"query\", documents, top_k=5)\n</code></pre>"},{"location":"rerankers/koreranker/","title":"KoReranker","text":"<p>Korean-specific document reranking.</p>"},{"location":"rerankers/koreranker/#overview","title":"Overview","text":"Field Value Type CrossEncoder (Korean) Library torch, transformers Default Model <code>Dongjin-kr/ko-reranker</code>"},{"location":"rerankers/koreranker/#installation","title":"Installation","text":"<pre><code>pip install \"autorag-research[gpu]\"\n# or\nuv add \"autorag-research[gpu]\"\n</code></pre>"},{"location":"rerankers/koreranker/#configuration","title":"Configuration","text":"<pre><code>_target_: autorag_research.rerankers.koreranker.KoRerankerReranker\nmodel_name: Dongjin-kr/ko-reranker\n</code></pre>"},{"location":"rerankers/koreranker/#options","title":"Options","text":"Option Type Default Description model_name str <code>Dongjin-kr/ko-reranker</code> KoReranker model name max_length int 512 Maximum input sequence length device str None Device (auto-detected) batch_size int 64 Batch size for multiple queries"},{"location":"rerankers/koreranker/#usage","title":"Usage","text":"<pre><code>from autorag_research.rerankers import KoRerankerReranker\n\nreranker = KoRerankerReranker()\nresults = reranker.rerank(\"RAG\ub780 \ubb34\uc5c7\uc778\uac00?\", [\"\ubb38\uc11c1\", \"\ubb38\uc11c2\", \"\ubb38\uc11c3\"], top_k=2)\n</code></pre>"},{"location":"rerankers/koreranker/#when-to-use","title":"When to Use","text":"<p>Specifically designed for Korean language documents. Use this when:</p> <ul> <li>Your corpus is primarily Korean text</li> <li>You need Korean-aware relevance scoring</li> </ul>"},{"location":"rerankers/mixedbreadai/","title":"MixedbreadAI","text":"<p>Reranking via Mixedbread AI's API.</p>"},{"location":"rerankers/mixedbreadai/#overview","title":"Overview","text":"Field Value Type API Provider Mixedbread AI Default Model <code>mixedbread-ai/mxbai-rerank-large-v1</code> Env Variable <code>MIXEDBREAD_API_KEY</code>"},{"location":"rerankers/mixedbreadai/#configuration","title":"Configuration","text":"<pre><code>_target_: autorag_research.rerankers.mixedbreadai.MixedbreadAIReranker\nmodel_name: mixedbread-ai/mxbai-rerank-large-v1\n</code></pre>"},{"location":"rerankers/mixedbreadai/#options","title":"Options","text":"Option Type Default Description model_name str <code>mixedbread-ai/mxbai-rerank-large-v1</code> Model name api_key str None API key (or use env var) batch_size int 64 Batch size"},{"location":"rerankers/mixedbreadai/#models","title":"Models","text":"Model Description mxbai-rerank-large-v1 Large, best quality mxbai-rerank-base-v1 Base, balanced"},{"location":"rerankers/mixedbreadai/#usage","title":"Usage","text":"<pre><code>from autorag_research.rerankers import MixedbreadAIReranker\n\nreranker = MixedbreadAIReranker()\nresults = reranker.rerank(\"query\", documents, top_k=5)\n</code></pre>"},{"location":"rerankers/monot5/","title":"MonoT5","text":"<p>Sequence-to-sequence reranking via T5 models.</p>"},{"location":"rerankers/monot5/#overview","title":"Overview","text":"Field Value Type Seq2Seq Library torch, transformers Default Model <code>castorini/monot5-3b-msmarco-10k</code> Paper Nogueira et al., 2020"},{"location":"rerankers/monot5/#installation","title":"Installation","text":"<pre><code>pip install \"autorag-research[gpu]\"\n# or\nuv add \"autorag-research[gpu]\"\n</code></pre>"},{"location":"rerankers/monot5/#configuration","title":"Configuration","text":"<pre><code>_target_: autorag_research.rerankers.monot5.MonoT5Reranker\nmodel_name: castorini/monot5-3b-msmarco-10k\n</code></pre>"},{"location":"rerankers/monot5/#options","title":"Options","text":"Option Type Default Description model_name str <code>castorini/monot5-3b-msmarco-10k</code> MonoT5 model name max_length int 512 Maximum input sequence length device str None Device (auto-detected) batch_size int 64 Batch size for multiple queries"},{"location":"rerankers/monot5/#how-it-works","title":"How It Works","text":"<ol> <li>Format input as <code>\"Query: {query} Document: {passage} Relevant:\"</code></li> <li>Generate the first output token</li> <li>Compute softmax over \"true\" and \"false\" token logits</li> <li>Use probability of \"true\" as relevance score</li> </ol>"},{"location":"rerankers/monot5/#models","title":"Models","text":"Model Size Description castorini/monot5-base-msmarco-10k 220M Fast castorini/monot5-large-msmarco-10k 770M Balanced castorini/monot5-3b-msmarco-10k 3B Best quality"},{"location":"rerankers/monot5/#usage","title":"Usage","text":"<pre><code>from autorag_research.rerankers import MonoT5Reranker\n\nreranker = MonoT5Reranker(model_name=\"castorini/monot5-base-msmarco-10k\")\nresults = reranker.rerank(\"What is RAG?\", [\"doc1\", \"doc2\", \"doc3\"], top_k=2)\n</code></pre>"},{"location":"rerankers/openvino/","title":"OpenVINO","text":"<p>Intel hardware-optimized reranking via OpenVINO.</p>"},{"location":"rerankers/openvino/#overview","title":"Overview","text":"Field Value Type HW-optimized Library optimum-intel Default Model <code>BAAI/bge-reranker-large</code>"},{"location":"rerankers/openvino/#installation","title":"Installation","text":"<pre><code>pip install \"autorag-research[openvino]\"\n# or\nuv add \"autorag-research[openvino]\"\n</code></pre>"},{"location":"rerankers/openvino/#configuration","title":"Configuration","text":"<pre><code>_target_: autorag_research.rerankers.openvino.OpenVINOReranker\nmodel_name: BAAI/bge-reranker-large\n</code></pre>"},{"location":"rerankers/openvino/#options","title":"Options","text":"Option Type Default Description model_name str <code>BAAI/bge-reranker-large</code> HuggingFace model name max_length int 512 Maximum input sequence length batch_size int 64 Batch size for multiple queries"},{"location":"rerankers/openvino/#how-it-works","title":"How It Works","text":"<ol> <li>Auto-exports HuggingFace model to OpenVINO IR format</li> <li>Runs inference using Intel OpenVINO runtime</li> <li>Applies sigmoid activation to logits for scores</li> </ol>"},{"location":"rerankers/openvino/#usage","title":"Usage","text":"<pre><code>from autorag_research.rerankers import OpenVINOReranker\n\nreranker = OpenVINOReranker()\nresults = reranker.rerank(\"What is RAG?\", [\"doc1\", \"doc2\", \"doc3\"], top_k=2)\n</code></pre>"},{"location":"rerankers/openvino/#when-to-use","title":"When to Use","text":"<p>Good for:</p> <ul> <li>Intel CPU-based deployments</li> <li>Production environments without GPU</li> <li>Optimized inference on Intel hardware</li> </ul>"},{"location":"rerankers/rankgpt/","title":"RankGPT","text":"<p>LLM-based listwise reranking.</p>"},{"location":"rerankers/rankgpt/#overview","title":"Overview","text":"Field Value Type LLM Algorithm Listwise ranking Paper Sun et al., 2023"},{"location":"rerankers/rankgpt/#how-it-works","title":"How It Works","text":"<ol> <li>Presents all documents to LLM with query</li> <li>LLM outputs ranking: <code>[1] &gt; [3] &gt; [2]</code></li> <li>Uses sliding window for large document sets</li> </ol>"},{"location":"rerankers/rankgpt/#configuration","title":"Configuration","text":"<pre><code>_target_: autorag_research.rerankers.rankgpt.RankGPTReranker\nmodel_name: gpt-4o-mini\nmax_passages_per_call: 20\nwindow_size: 10\n</code></pre>"},{"location":"rerankers/rankgpt/#options","title":"Options","text":"Option Type Default Description llm BaseLanguageModel required LangChain LLM instance max_passages_per_call int 20 Max docs per LLM call window_size int 10 Sliding window step"},{"location":"rerankers/rankgpt/#usage","title":"Usage","text":"<pre><code>from langchain_openai import ChatOpenAI\nfrom autorag_research.rerankers import RankGPTReranker\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\nreranker = RankGPTReranker(llm=llm)\nresults = reranker.rerank(\"query\", documents, top_k=5)\n</code></pre>"},{"location":"rerankers/rankgpt/#when-to-use","title":"When to Use","text":"<p>Good for:</p> <ul> <li>High-quality ranking needed</li> <li>Small document sets (&lt;100)</li> <li>When API rerankers unavailable</li> </ul> <p>Consider API rerankers for:</p> <ul> <li>Large scale (cost)</li> <li>Low latency requirements</li> </ul>"},{"location":"rerankers/sentence_transformer/","title":"SentenceTransformer","text":"<p>Cross-encoder reranking via SentenceTransformers.</p>"},{"location":"rerankers/sentence_transformer/#overview","title":"Overview","text":"Field Value Type CrossEncoder Library sentence-transformers Default Model <code>cross-encoder/ms-marco-MiniLM-L-2-v2</code>"},{"location":"rerankers/sentence_transformer/#installation","title":"Installation","text":"<pre><code>pip install \"autorag-research[gpu]\"\n# or\nuv add \"autorag-research[gpu]\"\n</code></pre>"},{"location":"rerankers/sentence_transformer/#configuration","title":"Configuration","text":"<pre><code>_target_: autorag_research.rerankers.sentence_transformer.SentenceTransformerReranker\nmodel_name: cross-encoder/ms-marco-MiniLM-L-2-v2\n</code></pre>"},{"location":"rerankers/sentence_transformer/#options","title":"Options","text":"Option Type Default Description model_name str <code>cross-encoder/ms-marco-MiniLM-L-2-v2</code> HuggingFace model name max_length int 512 Maximum input sequence length device str None Device (auto-detected) batch_size int 64 Batch size for multiple queries"},{"location":"rerankers/sentence_transformer/#models","title":"Models","text":"Model Description cross-encoder/ms-marco-MiniLM-L-2-v2 Fast, lightweight cross-encoder/ms-marco-MiniLM-L-6-v2 Balanced cross-encoder/ms-marco-MiniLM-L-12-v2 Best quality"},{"location":"rerankers/sentence_transformer/#usage","title":"Usage","text":"<pre><code>from autorag_research.rerankers import SentenceTransformerReranker\n\nreranker = SentenceTransformerReranker()\nresults = reranker.rerank(\"What is RAG?\", [\"doc1\", \"doc2\", \"doc3\"], top_k=2)\n\nfor r in results:\n    print(f\"[{r.index}] {r.score:.3f}: {r.text[:50]}...\")\n</code></pre>"},{"location":"rerankers/tart/","title":"TART","text":"<p>Task-Aware Retrieval with Instructions.</p>"},{"location":"rerankers/tart/#overview","title":"Overview","text":"Field Value Type Instruction-T5 Library torch, transformers Default Model <code>facebook/tart-full-flan-t5-xl</code> Paper Asai et al., 2022"},{"location":"rerankers/tart/#installation","title":"Installation","text":"<pre><code>pip install \"autorag-research[gpu]\"\n# or\nuv add \"autorag-research[gpu]\"\n</code></pre>"},{"location":"rerankers/tart/#configuration","title":"Configuration","text":"<pre><code>_target_: autorag_research.rerankers.tart.TARTReranker\nmodel_name: facebook/tart-full-flan-t5-xl\n</code></pre>"},{"location":"rerankers/tart/#options","title":"Options","text":"Option Type Default Description model_name str <code>facebook/tart-full-flan-t5-xl</code> TART model name instruction str <code>Find passage to answer given question</code> Task instruction max_length int 512 Maximum input sequence length device str None Device (auto-detected) batch_size int 64 Batch size for multiple queries"},{"location":"rerankers/tart/#how-it-works","title":"How It Works","text":"<ol> <li>Prepend task instruction to query: <code>\"{instruction} [SEP] {query}\"</code></li> <li>Encode instruction-query with document as input pair</li> <li>Apply softmax to classification logits</li> <li>Use positive class probability as relevance score</li> </ol>"},{"location":"rerankers/tart/#usage","title":"Usage","text":"<pre><code>from autorag_research.rerankers import TARTReranker\n\nreranker = TARTReranker(instruction=\"Find passage to answer given question\")\nresults = reranker.rerank(\"What is RAG?\", [\"doc1\", \"doc2\", \"doc3\"], top_k=2)\n</code></pre>"},{"location":"rerankers/upr/","title":"UPR","text":"<p>Unsupervised Passage Reranker using question generation.</p>"},{"location":"rerankers/upr/#overview","title":"Overview","text":"Field Value Type LLM Algorithm Question generation Paper Sachan et al., 2022"},{"location":"rerankers/upr/#how-it-works","title":"How It Works","text":"<ol> <li>Generate a question from each passage</li> <li>Compare generated questions to original query</li> <li>Rank by similarity score</li> </ol>"},{"location":"rerankers/upr/#configuration","title":"Configuration","text":"<pre><code>_target_: autorag_research.rerankers.upr.UPRReranker\nmodel_name: gpt-4o-mini\n</code></pre>"},{"location":"rerankers/upr/#options","title":"Options","text":"Option Type Default Description llm BaseLanguageModel required LangChain LLM instance use_logprobs bool False Use log probabilities"},{"location":"rerankers/upr/#usage","title":"Usage","text":"<pre><code>from langchain_openai import ChatOpenAI\nfrom autorag_research.rerankers import UPRReranker\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\nreranker = UPRReranker(llm=llm)\nresults = reranker.rerank(\"query\", documents, top_k=5)\n</code></pre>"},{"location":"rerankers/upr/#when-to-use","title":"When to Use","text":"<p>Good for:</p> <ul> <li>Zero-shot reranking</li> <li>No training data available</li> <li>Research/experimentation</li> </ul> <p>Consider RankGPT for:</p> <ul> <li>Better accuracy</li> <li>Direct comparison</li> </ul>"},{"location":"rerankers/voyageai/","title":"VoyageAI","text":"<p>Reranking via Voyage AI's API.</p>"},{"location":"rerankers/voyageai/#overview","title":"Overview","text":"Field Value Type API Provider Voyage AI Default Model <code>rerank-2</code> Env Variable <code>VOYAGE_API_KEY</code>"},{"location":"rerankers/voyageai/#installation","title":"Installation","text":"<pre><code>pip install \"autorag-research[reranker]\"\n# or\nuv add \"autorag-research[reranker]\"\n</code></pre>"},{"location":"rerankers/voyageai/#configuration","title":"Configuration","text":"<pre><code>_target_: autorag_research.rerankers.voyageai.VoyageAIReranker\nmodel_name: rerank-2\n</code></pre>"},{"location":"rerankers/voyageai/#options","title":"Options","text":"Option Type Default Description model_name str <code>rerank-2</code> Voyage model name api_key str None API key (or use env var) batch_size int 64 Batch size"},{"location":"rerankers/voyageai/#models","title":"Models","text":"Model Description rerank-2 Latest version rerank-lite-1 Lightweight, faster"},{"location":"rerankers/voyageai/#usage","title":"Usage","text":"<pre><code>from autorag_research.rerankers import VoyageAIReranker\n\nreranker = VoyageAIReranker()\nresults = reranker.rerank(\"query\", documents, top_k=5)\n</code></pre>"},{"location":"tutorial/","title":"Tutorial","text":"<p>Step-by-step guides organized by what you want to accomplish.</p>"},{"location":"tutorial/#prerequisites","title":"Prerequisites","text":"<ul> <li>Installation completed</li> <li>PostgreSQL running</li> </ul>"},{"location":"tutorial/#tutorials","title":"Tutorials","text":"Goal Tutorial Setup environment First Steps Run text retrieval benchmark Text Retrieval Run full RAG pipeline Text RAG Work with visual documents Multimodal Use your own dataset Custom Dataset Build your own pipeline Custom Pipeline Create your own metric Custom Metric"},{"location":"tutorial/custom-dataset/","title":"Custom Dataset","text":"<p>Ingest your own dataset for benchmarking.</p>"},{"location":"tutorial/custom-dataset/#dataset-requirements","title":"Dataset Requirements","text":"<p>Your dataset needs:</p> <ol> <li>Documents: Text or images to search</li> <li>Queries: Questions to answer</li> <li>Ground Truth: Which documents answer each query</li> </ol>"},{"location":"tutorial/custom-dataset/#format","title":"Format","text":"<pre><code>corpus = {\n    \"doc_1\": \"Document text here...\",\n    \"doc_2\": \"Another document...\",\n}\n\nqueries = {\n    \"q_1\": \"What is the capital of France?\",\n    \"q_2\": \"How does photosynthesis work?\",\n}\n\nqrels = {\n    \"q_1\": [\"doc_1\"],  # doc_1 answers q_1\n    \"q_2\": [\"doc_2\", \"doc_5\"],  # multiple relevant docs\n}\n</code></pre>"},{"location":"tutorial/custom-dataset/#create-ingestor","title":"Create Ingestor","text":"<pre><code>from autorag_research.data.ingestor import TextEmbeddingDataIngestor, register_ingestor\nfrom typing import Literal\n\n@register_ingestor(name=\"my_dataset\", description=\"My custom dataset\")\nclass MyDatasetIngestor(TextEmbeddingDataIngestor):\n    def __init__(\n        self,\n        embedding_model,\n        split: Literal[\"train\", \"test\"] = \"test\",\n    ):\n        super().__init__(embedding_model)\n        self.split = split\n\n    def detect_primary_key_type(self):\n        return \"string\"  # or \"bigint\" for integer IDs\n\n    def ingest(self):\n        # Load your data\n        corpus = self._load_corpus()\n        queries = self._load_queries()\n        qrels = self._load_qrels()\n\n        # Add to database\n        for doc_id, text in corpus.items():\n            self._service.add_chunk(doc_id, text)\n\n        for query_id, query_text in queries.items():\n            self._service.add_query(query_id, query_text)\n\n        for query_id, doc_ids in qrels.items():\n            self._service.add_retrieval_gt(query_id, doc_ids)\n</code></pre>"},{"location":"tutorial/custom-dataset/#ingest","title":"Ingest","text":"<pre><code>autorag-research ingest \\\n  --name=my_dataset \\\n  --extra split=test \\\n  --embedding-model=openai-small\n</code></pre>"},{"location":"tutorial/custom-dataset/#run-benchmark","title":"Run Benchmark","text":"<pre><code># configs/experiment.yaml\ndb_name: my_dataset_test_openai_small\n\npipelines:\n  retrieval: [bm25]\nmetrics:\n  retrieval: [recall, ndcg]\n</code></pre> <pre><code>autorag-research run --config-name=experiment\n</code></pre>"},{"location":"tutorial/custom-dataset/#share-dataset","title":"Share Dataset","text":"<p>Export and upload to HuggingFace Hub:</p> <pre><code>autorag-research data dump --db-name=my_dataset_test_openai_small\nautorag-research data upload ./dump.file my_dataset openai-small\n</code></pre>"},{"location":"tutorial/custom-dataset/#next","title":"Next","text":"<ul> <li>Custom Pipeline - Test your algorithm</li> <li>Datasets - See existing formats</li> </ul>"},{"location":"tutorial/custom-metric/","title":"Custom Metric","text":"<p>Create your own evaluation metric.</p>"},{"location":"tutorial/custom-metric/#retrieval-metric","title":"Retrieval Metric","text":"<pre><code>from autorag_research.evaluation.metrics import (\n    BaseRetrievalMetricConfig,\n    metric,\n    MetricInput,\n)\nfrom dataclasses import dataclass\n\n\n@metric(fields_to_check=[\"retrieval_gt\", \"retrieved_ids\"])\ndef hit_rate(metric_input: MetricInput) -&gt; float:\n    \"\"\"Returns 1 if any ground truth doc was retrieved, 0 otherwise.\"\"\"\n    gt_ids = set()\n    for group in metric_input.retrieval_gt:\n        gt_ids.update(group)\n\n    retrieved = set(metric_input.retrieved_ids)\n    return 1.0 if gt_ids &amp; retrieved else 0.0\n\n\n@dataclass\nclass HitRateConfig(BaseRetrievalMetricConfig):\n    def get_metric_func(self):\n        return hit_rate\n</code></pre>"},{"location":"tutorial/custom-metric/#generation-metric","title":"Generation Metric","text":"<pre><code>from autorag_research.evaluation.metrics import (\n    BaseGenerationMetricConfig,\n    metric_loop,\n    MetricInput,\n)\nfrom dataclasses import dataclass\n\n\n@metric_loop(fields_to_check=[\"generation_gt\", \"generated_texts\"])\ndef exact_match(metric_inputs: list[MetricInput]) -&gt; list[float]:\n    \"\"\"Returns 1 if generated text exactly matches any ground truth.\"\"\"\n    scores = []\n    for inp in metric_inputs:\n        generated = inp.generated_texts.strip().lower()\n        matches = any(gt.strip().lower() == generated for gt in inp.generation_gt)\n        scores.append(1.0 if matches else 0.0)\n    return scores\n\n\n@dataclass\nclass ExactMatchConfig(BaseGenerationMetricConfig):\n    def get_metric_func(self):\n        return exact_match\n</code></pre>"},{"location":"tutorial/custom-metric/#add-configuration","title":"Add Configuration","text":"<pre><code># configs/metrics/retrieval/hit_rate.yaml\n_target_: my_module.HitRateConfig\n</code></pre>"},{"location":"tutorial/custom-metric/#use-in-experiment","title":"Use in Experiment","text":"<pre><code># configs/experiment.yaml\nmetrics:\n  retrieval:\n    - recall\n    - hit_rate  # your metric\n  generation:\n    - rouge\n    - exact_match  # your metric\n</code></pre>"},{"location":"tutorial/custom-metric/#next","title":"Next","text":"<ul> <li>Metrics - See existing implementations</li> <li>Custom Pipeline - Test algorithms</li> </ul>"},{"location":"tutorial/custom-pipeline/","title":"Custom Pipeline","text":"<p>Implement your own retrieval or generation algorithm.</p>"},{"location":"tutorial/custom-pipeline/#retrieval-pipeline","title":"Retrieval Pipeline","text":"<pre><code>from autorag_research.pipelines.retrieval import (\n    BaseRetrievalPipeline,\n    BaseRetrievalPipelineConfig,\n)\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass MyRetrievalConfig(BaseRetrievalPipelineConfig):\n    name: str = \"my_retrieval\"\n    custom_param: float = 0.5\n\n    def get_pipeline_class(self):\n        return MyRetrievalPipeline\n\n    def get_pipeline_kwargs(self):\n        return {\"custom_param\": self.custom_param}\n\n\nclass MyRetrievalPipeline(BaseRetrievalPipeline):\n    def __init__(self, session_factory, name, schema, custom_param):\n        super().__init__(session_factory, name, schema)\n        self.custom_param = custom_param\n\n    def _get_retrieval_func(self):\n        def retrieve(queries: list[str], top_k: int) -&gt; list[list[dict]]:\n            results = []\n            for query in queries:\n                # Your retrieval logic here\n                docs = self._search(query, top_k)\n                results.append([{\"doc_id\": d.id, \"score\": d.score} for d in docs])\n            return results\n\n        return retrieve\n\n    def _get_pipeline_config(self):\n        return {\"type\": \"my_retrieval\", \"custom_param\": self.custom_param}\n</code></pre>"},{"location":"tutorial/custom-pipeline/#generation-pipeline","title":"Generation Pipeline","text":"<pre><code>from autorag_research.pipelines.generation import (\n    BaseGenerationPipeline,\n    BaseGenerationPipelineConfig,\n    GenerationResult,\n)\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass MyRAGConfig(BaseGenerationPipelineConfig):\n    name: str = \"my_rag\"\n    retrieval_pipeline_name: str = \"bm25\"\n\n    def get_pipeline_class(self):\n        return MyRAGPipeline\n\n\nclass MyRAGPipeline(BaseGenerationPipeline):\n    def _generate(self, query: str, top_k: int) -&gt; GenerationResult:\n        # Step 1: Retrieve documents\n        retrieved = self._retrieval_pipeline.retrieve(query, top_k)\n\n        # Step 2: Build context\n        context = self._build_context(retrieved)\n\n        # Step 3: Generate answer\n        answer = self._llm.complete(f\"Context: {context}\\nQuestion: {query}\")\n\n        return GenerationResult(\n            text=answer.text,\n            token_usage={\"prompt\": 100, \"completion\": 50},\n            metadata={\"retrieved_ids\": [r[\"doc_id\"] for r in retrieved]},\n        )\n\n    def _get_pipeline_config(self):\n        return {\"type\": \"my_rag\"}\n</code></pre>"},{"location":"tutorial/custom-pipeline/#add-configuration","title":"Add Configuration","text":"<pre><code># configs/pipelines/retrieval/my_retrieval.yaml\n_target_: my_module.MyRetrievalConfig\nname: my_retrieval\ncustom_param: 0.7\n</code></pre>"},{"location":"tutorial/custom-pipeline/#benchmark-against-baselines","title":"Benchmark Against Baselines","text":"<pre><code># configs/experiment.yaml\npipelines:\n  retrieval:\n    - bm25           # baseline\n    - my_retrieval   # your algorithm\nmetrics:\n  retrieval: [recall, ndcg, mrr]\n</code></pre> <pre><code>autorag-research run --config-name=experiment\n</code></pre>"},{"location":"tutorial/custom-pipeline/#next","title":"Next","text":"<ul> <li>Custom Metric - Add evaluation</li> <li>Pipelines - See existing implementations</li> </ul>"},{"location":"tutorial/first-steps/","title":"First Steps","text":"<p>Install and verify AutoRAG-Research.</p>"},{"location":"tutorial/first-steps/#install","title":"Install","text":"<pre><code>pip install autorag-research\n</code></pre> <p>For development installation:</p> <pre><code>git clone https://github.com/NomaDamas/AutoRAG-Research.git\ncd AutoRAG-Research\nuv sync --all-extras\n</code></pre>"},{"location":"tutorial/first-steps/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10+</li> <li>PostgreSQL 18 client tools (<code>pg_restore-18</code> for database restore)</li> </ul> <p>Verify PostgreSQL tools:</p> <pre><code>pg_restore-18 --version\n</code></pre>"},{"location":"tutorial/first-steps/#start-postgresql","title":"Start PostgreSQL","text":"<pre><code>docker-compose up -d\n</code></pre>"},{"location":"tutorial/first-steps/#initialize-configuration","title":"Initialize Configuration","text":"<pre><code>autorag-research init\n</code></pre> <p>Creates <code>configs/</code> directory with templates.</p>"},{"location":"tutorial/first-steps/#verify","title":"Verify","text":"<pre><code>autorag-research show ingestors\n</code></pre> <p>You should see available dataset ingestors:</p> Name Description beir BEIR benchmark datasets mteb MTEB retrieval tasks ragbench RAGBench benchmark vidorev3 ViDoRe V3 visual documents"},{"location":"tutorial/first-steps/#next","title":"Next","text":"<ul> <li>Text Retrieval - Run your first benchmark</li> <li>Concepts - Understand the system</li> </ul>"},{"location":"tutorial/multimodal/","title":"Multimodal Retrieval","text":"<p>Benchmark visual document retrieval (PDF pages, screenshots).</p>"},{"location":"tutorial/multimodal/#prerequisites","title":"Prerequisites","text":"<ul> <li>GPU recommended for image embeddings</li> <li>Multimodal embedding model (ColPali)</li> </ul>"},{"location":"tutorial/multimodal/#download-dataset","title":"Download Dataset","text":"<pre><code>autorag-research data restore vidorev3 arxivqa_colpali\n</code></pre> <p>ViDoRe v3 contains document images with queries.</p>"},{"location":"tutorial/multimodal/#key-differences-from-text","title":"Key Differences from Text","text":"Aspect Text Multimodal Documents Plain text Images (PDF pages) Embeddings Text models Vision models (ColPali) BM25 Available Not available"},{"location":"tutorial/multimodal/#create-experiment-config","title":"Create Experiment Config","text":"<pre><code># configs/experiment.yaml\ndb_name: vidorev3_arxivqa_test_colpali\n\npipelines:\n  retrieval:\n    - colpali\n  generation: []\n\nmetrics:\n  retrieval:\n    - recall\n    - ndcg\n  generation: []\n</code></pre>"},{"location":"tutorial/multimodal/#run","title":"Run","text":"<pre><code>autorag-research run --config-name=experiment\n</code></pre>"},{"location":"tutorial/multimodal/#recommended-datasets","title":"Recommended Datasets","text":"Dataset Description ViDoRe v3 Document images, multiple domains VisRAG Visual RAG benchmark <p>See Multimodal Datasets for all options.</p>"},{"location":"tutorial/multimodal/#next","title":"Next","text":"<ul> <li>Custom Dataset - Add your own images</li> <li>Custom Pipeline - New retrieval method</li> </ul>"},{"location":"tutorial/text-rag/","title":"Text RAG Benchmark","text":"<p>Run full RAG: retrieval + generation with LLM.</p>"},{"location":"tutorial/text-rag/#prerequisites","title":"Prerequisites","text":"<ul> <li>LLM API key (OpenAI, Anthropic, or local model)</li> <li>Dataset with generation ground truth (RAGBench recommended)</li> </ul>"},{"location":"tutorial/text-rag/#download-dataset","title":"Download Dataset","text":"<pre><code>autorag-research data restore ragbench covidqa_openai-small\n</code></pre> <p>RAGBench includes generation ground truth (expected answers).</p>"},{"location":"tutorial/text-rag/#create-experiment-config","title":"Create Experiment Config","text":"<pre><code># configs/experiment.yaml\ndb_name: ragbench_covidqa_test_openai_small\n\npipelines:\n  retrieval:\n    - bm25\n  generation:\n    - basic_rag\n\nmetrics:\n  retrieval:\n    - recall\n    - ndcg\n  generation:\n    - rouge\n    - bleu\n    - bert_score\n</code></pre>"},{"location":"tutorial/text-rag/#configure-llm","title":"Configure LLM","text":"<pre><code># configs/pipelines/generation/basic_rag.yaml\n_target_: autorag_research.pipelines.generation.basic_rag.BasicRAGPipelineConfig\nname: basic_rag\nretrieval_pipeline_name: bm25\nllm: gpt-4o-mini\nprompt_template: |\n  Context:\n  {context}\n\n  Question: {query}\n\n  Answer:\ntop_k: 5\n</code></pre>"},{"location":"tutorial/text-rag/#run","title":"Run","text":"<pre><code>autorag-research run --config-name=experiment\n</code></pre>"},{"location":"tutorial/text-rag/#expected-output","title":"Expected Output","text":"<pre><code>Pipeline: bm25\n  Recall@10: 0.823\n  NDCG@10: 0.698\n\nPipeline: basic_rag\n  ROUGE-L: 0.412\n  BLEU: 0.287\n  BERTScore: 0.856\n</code></pre>"},{"location":"tutorial/text-rag/#recommended-datasets","title":"Recommended Datasets","text":"Dataset Has Generation GT RAGBench Yes BEIR No (retrieval only) MTEB No (retrieval only)"},{"location":"tutorial/text-rag/#next","title":"Next","text":"<ul> <li>Multimodal - Visual documents</li> <li>Custom Metric - Add evaluation</li> </ul>"},{"location":"tutorial/text-retrieval/","title":"Text Retrieval Benchmark","text":"<p>Run a text retrieval benchmark without generation (no LLM required).</p>"},{"location":"tutorial/text-retrieval/#download-dataset","title":"Download Dataset","text":"<pre><code>autorag-research data restore beir scifact_openai-small\n</code></pre> <p>Downloads BEIR SciFact (300 queries, 5,183 documents).</p>"},{"location":"tutorial/text-retrieval/#create-experiment-config","title":"Create Experiment Config","text":"<pre><code># configs/experiment.yaml\ndb_name: beir_scifact_test_openai_small\n\npipelines:\n  retrieval:\n    - bm25\n  generation: []\n\nmetrics:\n  retrieval:\n    - recall\n    - precision\n    - ndcg\n    - mrr\n  generation: []\n</code></pre>"},{"location":"tutorial/text-retrieval/#run","title":"Run","text":"<pre><code>autorag-research run --config-name=experiment\n</code></pre>"},{"location":"tutorial/text-retrieval/#expected-output","title":"Expected Output","text":"<pre><code>Pipeline: bm25\n  Recall@10: 0.847\n  Precision@10: 0.085\n  NDCG@10: 0.712\n  MRR@10: 0.634\n</code></pre>"},{"location":"tutorial/text-retrieval/#recommended-datasets","title":"Recommended Datasets","text":"Dataset Queries Documents Best For BEIR SciFact 300 5,183 Scientific claims BEIR NFCorpus 323 3,633 Biomedical MTEB varies varies General text <p>See Text Datasets for all options.</p>"},{"location":"tutorial/text-retrieval/#recommended-metrics","title":"Recommended Metrics","text":"Metric Measures Recall@k Coverage of ground truth NDCG@k Ranking quality MRR First relevant position <p>See Retrieval Metrics for details.</p>"},{"location":"tutorial/text-retrieval/#next","title":"Next","text":"<ul> <li>Text RAG - Add generation</li> <li>Custom Pipeline - Implement your algorithm</li> </ul>"}]}